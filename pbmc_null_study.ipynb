{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly as py\n",
    "import pandas as pd\n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from scvi.dataset import PbmcDataset, GeneExpressionDataset\n",
    "from scvi.models import VAE, IAVAE\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.utils import demultiply, make_dir_if_necessary, predict_de_genes, save_fig\n",
    "from scvi_utils import estimate_de_proba, estimate_lfc_density, estimate_lfc_mean\n",
    "from R_interop import all_predictions\n",
    "\n",
    "\n",
    "N_EPOCHS = 10\n",
    "DELTA = 0.5\n",
    "# SIZES = [5, 10, 20, 30, 50, 100]\n",
    "SIZE = 100\n",
    "SIZES = [SIZE]\n",
    "DO_CLOUD = True\n",
    "N_SIZES = len(SIZES)\n",
    "\n",
    "Q0 = 5e-2\n",
    "N_TRAININGS = 1\n",
    "N_PICKS = 2\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "PATH_TO_SCRIPTS = \"/home/ubuntu/conquer_comparison/scripts\"\n",
    "DIR_PATH = 'lfc_estimates/pbmc'\n",
    "make_dir_if_necessary(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Dataset and Training scVI-based models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR_PATH = \"lfc_estimates/null\"\n",
    "make_dir_if_necessary(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PBMC Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "all_dataset = PbmcDataset()\n",
    "all_dataset.subsample_genes(2000)\n",
    "\n",
    "unique_elements, counts_elements = np.unique(\n",
    "    all_dataset.labels.squeeze(), return_counts=True\n",
    ")\n",
    "\n",
    "df = pd.DataFrame(dict(counts=counts_elements, cell_types=all_dataset.cell_types))\n",
    "px.scatter(df, y=\"counts\", x=\"cell_types\")\n",
    "\n",
    "mask = all_dataset.labels.squeeze() == 2\n",
    "# By default all cells are labelled 2\n",
    "fake_labels = 2.0 * np.ones(len(all_dataset))\n",
    "# Except cluster 2 that is either 0 or 1\n",
    "fake_labels[mask] = np.random.random(mask.sum()) >= 0.5\n",
    "\n",
    "dataset = GeneExpressionDataset()\n",
    "dataset.populate_from_data(\n",
    "    X=all_dataset.X,\n",
    "    labels=fake_labels,\n",
    "    batch_indices=all_dataset.batch_indices,\n",
    ")\n",
    "\n",
    "n_genes = dataset.nb_genes\n",
    "is_significant_de = np.zeros(n_genes, dtype=bool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.unique(dataset.labels.squeeze()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = len(dataset)\n",
    "labels = dataset.labels.squeeze()\n",
    "interesting_indices = np.where((labels == 0) | (labels == 1))[0]\n",
    "TEST_INDICES = np.random.permutation(interesting_indices)[:1001]\n",
    "\n",
    "x_test, y_test = dataset.X[TEST_INDICES, :], dataset.labels[TEST_INDICES, :].squeeze()\n",
    "data_path = os.path.join(DIR_PATH, 'data.npy')\n",
    "labels_path = os.path.join(DIR_PATH, 'labels.npy')\n",
    "\n",
    "np.save(\n",
    "    data_path,\n",
    "    np.array(x_test.todense()).squeeze().astype(int)\n",
    ")\n",
    "np.savetxt(\n",
    "    labels_path,\n",
    "    y_test.squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_params = dict(\n",
    "    iaf=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4),\n",
    "    mf=dict(n_hidden=128, n_layers=1, n_latent=10),\n",
    "    iaf_k5=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4),\n",
    "    mf_k5=dict(n_hidden=128, n_layers=1, n_latent=10),\n",
    ")\n",
    "train_params = dict(\n",
    "    iaf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    mf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    iaf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5),\n",
    "    mf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5)\n",
    ")\n",
    "train_fn_params = dict(\n",
    "    iaf=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    mf=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    iaf_k5=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    mf_k5=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Â Competitors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions = all_predictions(\n",
    "    n_genes=n_genes, \n",
    "    n_picks=N_PICKS, \n",
    "    sizes=SIZES, \n",
    "    data_path=data_path, \n",
    "    labels_path=labels_path,\n",
    "    path_to_scripts=PATH_TO_SCRIPTS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LFC Error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfcs_scVI = estimate_lfc_mean(\n",
    "    VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    "    sizes=[SIZE],\n",
    "    n_picks=N_PICKS,\n",
    ")[SIZE]\n",
    "\n",
    "lfcs_scVI_ia = estimate_lfc_mean(\n",
    "    IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "    sizes=[SIZE],\n",
    "    n_picks=N_PICKS,\n",
    ")[SIZE]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(other_predictions[\"mast\"][\"lfc\"].shape)\n",
    "\n",
    "lfcs_deseq2 = other_predictions[\"deseq2\"][\"lfc\"]\n",
    "lfcs_edger = other_predictions[\"edger\"][\"lfc\"]\n",
    "lfcs_mast = other_predictions[\"mast\"][\"lfc\"]\n",
    "assert lfcs_mast.shape == (N_PICKS, n_genes), lfcs_mast.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lfcs_deseq2.shape)\n",
    "print(lfcs_edger.shape)\n",
    "print(lfcs_mast.shape)\n",
    "print(lfcs_scVI.shape)\n",
    "print(lfcs_scVI_ia.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def l2_err(vals):\n",
    "    res = 0.5 * (vals ** 2) ** (0.5)\n",
    "    res = np.nanmean(res, axis=-1)\n",
    "    return res\n",
    "\n",
    "\n",
    "scVI_errs = l2_err(lfcs_scVI)\n",
    "scVI_ia_errs = l2_err(lfcs_scVI_ia)\n",
    "deseq2_errs = l2_err(lfcs_deseq2)\n",
    "edger_errs = l2_err(lfcs_edger)\n",
    "mast_errs = l2_err(lfcs_mast)\n",
    "\n",
    "trace1 = go.Box(y=scVI_errs, name=\"scVI\")\n",
    "trace2 = go.Box(y=scVI_ia_errs, name=\"scVI IAF\")\n",
    "trace3 = go.Box(y=deseq2_errs, name=\"DeSeq2\")\n",
    "trace4 = go.Box(y=edger_errs, name=\"edgeR\")\n",
    "trace5 = go.Box(y=mast_errs, name=\"MAST\")\n",
    "traces = [trace1, trace2, trace3, trace4, trace5]\n",
    "\n",
    "layout = go.Layout(title=\"L2 Error on Null Real Data\")\n",
    "\n",
    "\n",
    "fig = go.Figure(traces, layout=layout)\n",
    "# save_fig(fig, filename=\"pbmc_null_lfc_err\", do_cloud=DO_CLOUD)\n",
    "iplot(fig, filename=\"pbmc_null_lfc_err\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu, ttest_1samp\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "def a_better_b_sign(a, b):\n",
    "    _, p = mannwhitneyu(a, b, alternative=\"less\")\n",
    "    return p\n",
    "\n",
    "\n",
    "vals = [scVI_errs, scVI_ia_errs, deseq2_errs, edger_errs, mast_errs]\n",
    "\n",
    "x = [\"scVI\", \"scVI IAF\", \"DeSeq2\", \"edgeR\", \"MAST\"]\n",
    "\n",
    "mat = [[a_better_b_sign(b, a) for a in vals] for b in vals]\n",
    "\n",
    "fig = ff.create_annotated_heatmap(z=mat, x=x, y=x)\n",
    "fig = fig.update_layout(\n",
    "    title_text=\"P values for error test on null data (pval that line a better than col b)\"\n",
    ")\n",
    "\n",
    "# save_fig(fig, filename=\"pbmc_null_lfc_err\", do_cloud=DO_CLOUD)\n",
    "iplot(fig, filename=\"pbmc_null_lfc_err_sign\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Significant DE genes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "de_probas_mf = estimate_de_proba(\n",
    "    VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    "    sizes=[SIZE],\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    ").squeeze()\n",
    "de_probas_ia = estimate_de_proba(\n",
    "    IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "    sizes=[SIZE],\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    ").squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nb_predicted(probas_arr):\n",
    "#     nb_predicted = np.zeros((N_TRAININGS, N_PICKS))\n",
    "#     for i in range(N_TRAININGS):\n",
    "#         for k in range(N_PICKS):\n",
    "#             probs_pred_de = probas_arr[i, k, :]\n",
    "#             is_pred_de = predict_de_genes(probs_pred_de, desired_fdr=Q0)\n",
    "#             nb_predicted[i, j, k] = is_pred_de.sum()\n",
    "    \n",
    "    nb_predicted = np.zeros((N_PICKS))\n",
    "    for k in range(N_PICKS):\n",
    "        probs_pred_de = probas_arr[k, :]\n",
    "        is_pred_de = predict_de_genes(probs_pred_de, desired_fdr=Q0)\n",
    "        nb_predicted[k] = is_pred_de.sum()\n",
    "    return nb_predicted\n",
    "\n",
    "def naive_is_de(probas_arr):\n",
    "    probas_arr[np.isnan(probas_arr)] = 0.0\n",
    "    return (probas_arr <= Q0).sum(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_predicted_scVI = compute_nb_predicted(de_probas_mf)\n",
    "nb_predicted_scVI_ia = compute_nb_predicted(de_probas_ia)\n",
    "\n",
    "nb_predicted_deseq2 = naive_is_de(other_predictions['deseq2']['pval'])\n",
    "nb_predicted_edger = naive_is_de(other_predictions['edger']['pval'])\n",
    "nb_predicted_mast = naive_is_de(other_predictions['mast']['pval'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bounds_scVI = [nb_predicted_scVI.min(), nb_predicted_scVI.max()]\n",
    "bounds_scVI_ia = [nb_predicted_scVI_ia.min(), nb_predicted_scVI_ia.max()]\n",
    "bounds_deseq2 = [nb_predicted_deseq2.min(), nb_predicted_deseq2.max()]\n",
    "bounds_edger = [nb_predicted_edger.min(), nb_predicted_edger.max()]\n",
    "bounds_mast = [nb_predicted_mast.min(), nb_predicted_mast.max()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(\n",
    "    dict(\n",
    "        FP=[\n",
    "            bounds_scVI[1] - bounds_scVI[0], \n",
    "            bounds_scVI_ia[1] - bounds_scVI_ia[0], \n",
    "            bounds_deseq2[1] - bounds_deseq2[0], \n",
    "            bounds_edger[1] - bounds_edger[0], \n",
    "            bounds_mast[1] - bounds_mast[0]\n",
    "        ],\n",
    "    ),\n",
    "    index=['MF', 'IAF', 'DESeq2', 'EdgeR', 'MAST']\n",
    ").T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scVI_robustness = get_robustness(lfcs_scVI)\n",
    "# scVI_ia_robustness = get_robustness(lfcs_scVI_ia)\n",
    "# deseq2_robustness = get_robustness(lfcs_deseq2)\n",
    "# edge_r_robustness = get_robustness(lfcs_edge_r)\n",
    "# mast_robustness = get_robustness(lfcs_mast)\n",
    "\n",
    "# trace1 = go.Box(y=scVI_robustness, name=\"scVI\")\n",
    "# trace2 = go.Box(y=scVI_ia_robustness, name=\"scVI IAF\")\n",
    "# trace3 = go.Box(y=deseq2_robustness, name=\"DeSeq2\")\n",
    "# trace4 = go.Box(y=edge_r_robustness, name=\"edgeR\")\n",
    "# trace5 = go.Box(y=mast_robustness, name=\"MAST\")\n",
    "# traces = [trace1, trace2, trace3, trace4, trace5]\n",
    "\n",
    "# layout = go.Layout(title=\"Robustness on Null Real Data\")\n",
    "\n",
    "# fig = go.Figure(traces, layout=layout)\n",
    "# # fig.show()\n",
    "# iplot(fig, filename='robustness_null_ercc_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from scipy.stats import mannwhitneyu, ttest_1samp\n",
    "# import plotly.figure_factory as ff\n",
    "\n",
    "\n",
    "# def a_better_b_sign(a, b):\n",
    "#     _, p = mannwhitneyu(a, b, alternative=\"less\")\n",
    "#     return p\n",
    "\n",
    "\n",
    "# vals = [\n",
    "#     scVI_robustness,\n",
    "#     scVI_ia_robustness,\n",
    "#     deseq2_robustness,\n",
    "#     edge_r_robustness,\n",
    "#     mast_robustness,\n",
    "# ]\n",
    "\n",
    "# x = [\"scVI\", \"scVI IAF\", \"DeSeq2\", \"edgeR\", \"MAST\"]\n",
    "\n",
    "# mat = [[a_better_b_sign(b, a) for a in vals] for b in vals]\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(z=mat, x=x, y=x)\n",
    "# fig.update_layout(\n",
    "#     title_text=\"P values for robustness test on null data (pval that line a better than col b)\"\n",
    "# )\n",
    "\n",
    "# iplot(fig, filename='significance_robustness_null_ercc_dataset')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
