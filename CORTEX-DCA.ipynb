{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "from dca.api import dca\n",
    "import anndata\n",
    "\n",
    "from benchmarking import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression data\n",
    "data_path = \"/home/ubuntu/single-cell-scVI/data/Zeisel/\"\n",
    "expression_train = np.loadtxt(data_path + \"data_train\")\n",
    "expression_test = np.loadtxt(data_path + \"data_test\")\n",
    "\n",
    "# zero masked matrix\n",
    "X_zero, i, j, ix = \\\n",
    "        np.load(data_path + \"imputation/X_zero.npy\"), np.load(data_path + \"imputation/i.npy\"),\\\n",
    "        np.load(data_path + \"imputation/j.npy\"), np.load(data_path + \"imputation/ix.npy\")\n",
    "#labels\n",
    "c_train = np.loadtxt(data_path + \"label_train\")\n",
    "c_test = np.loadtxt(data_path + \"label_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2253, 558)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agg_clusters_2(c):\n",
    "    new_c = np.zeros_like(c)\n",
    "    for t in np.arange(c.shape[0]):\n",
    "        if c[t] in [2, 6, 5]:\n",
    "            new_c[t] = 0\n",
    "        else:\n",
    "            new_c[t] = 1\n",
    "    return new_c\n",
    "\n",
    "def agg_clusters_3(c):\n",
    "    new_c = np.zeros_like(c)\n",
    "    for t in np.arange(c.shape[0]):\n",
    "        if c[t] in [2, 6, 5]:\n",
    "            new_c[t] = 0\n",
    "        elif c[t] in [1, 3]: \n",
    "            new_c[t] = 1\n",
    "        else:\n",
    "            new_c[t] = 2\n",
    "    return new_c\n",
    "\n",
    "def agg_clusters_4(c):\n",
    "    new_c = np.zeros_like(c)\n",
    "    for t in np.arange(c.shape[0]):\n",
    "        if c[t] in [2, 6, 5]:\n",
    "            new_c[t] = 0\n",
    "        elif c[t] in [1, 3]: \n",
    "            new_c[t] = 1\n",
    "        elif c[t] in [0]:\n",
    "            new_c[t] = 2\n",
    "        else:\n",
    "            new_c[t] = 3\n",
    "    return new_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCA: Successfully preprocessed 558 genes and 2253 cells.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 558)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           35776       count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (None, 558)          0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 558)          0           output[0][0]                     \n",
      "                                                                 dispersion[0][0]                 \n",
      "                                                                 pi[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 149,258\n",
      "Trainable params: 148,938\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2027 samples, validate on 226 samples\n",
      "Epoch 1/300\n",
      "2027/2027 [==============================] - 1s 725us/step - loss: 3.7074 - val_loss: 2.6846\n",
      "Epoch 2/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.5720 - val_loss: 2.3902\n",
      "Epoch 3/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.3734 - val_loss: 2.2751\n",
      "Epoch 4/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.3075 - val_loss: 2.2316\n",
      "Epoch 5/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.2642 - val_loss: 2.2013\n",
      "Epoch 6/300\n",
      "2027/2027 [==============================] - 0s 247us/step - loss: 2.2440 - val_loss: 2.1753\n",
      "Epoch 7/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.2268 - val_loss: 2.1643\n",
      "Epoch 8/300\n",
      "2027/2027 [==============================] - 1s 260us/step - loss: 2.2172 - val_loss: 2.1535\n",
      "Epoch 9/300\n",
      "2027/2027 [==============================] - 1s 249us/step - loss: 2.2079 - val_loss: 2.1415\n",
      "Epoch 10/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.2011 - val_loss: 2.1402\n",
      "Epoch 11/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1926 - val_loss: 2.1363\n",
      "Epoch 12/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1903 - val_loss: 2.1408\n",
      "Epoch 13/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1803 - val_loss: 2.1344\n",
      "Epoch 14/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1774 - val_loss: 2.1172\n",
      "Epoch 15/300\n",
      "2027/2027 [==============================] - 1s 249us/step - loss: 2.1755 - val_loss: 2.1100\n",
      "Epoch 16/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1702 - val_loss: 2.1129\n",
      "Epoch 17/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.1664 - val_loss: 2.1122\n",
      "Epoch 18/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1632 - val_loss: 2.1091\n",
      "Epoch 19/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1608 - val_loss: 2.1159\n",
      "Epoch 20/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1542 - val_loss: 2.1091\n",
      "Epoch 21/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1531 - val_loss: 2.0984\n",
      "Epoch 22/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1501 - val_loss: 2.0988\n",
      "Epoch 23/300\n",
      "2027/2027 [==============================] - 1s 252us/step - loss: 2.1507 - val_loss: 2.1042\n",
      "Epoch 24/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1419 - val_loss: 2.0955\n",
      "Epoch 25/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1482 - val_loss: 2.0861\n",
      "Epoch 26/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1423 - val_loss: 2.0958\n",
      "Epoch 27/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1387 - val_loss: 2.0881\n",
      "Epoch 28/300\n",
      "2027/2027 [==============================] - 0s 247us/step - loss: 2.1385 - val_loss: 2.0903\n",
      "Epoch 29/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1428 - val_loss: 2.0932\n",
      "Epoch 30/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1396 - val_loss: 2.0911\n",
      "Epoch 31/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1360 - val_loss: 2.1061\n",
      "Epoch 32/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1349 - val_loss: 2.0879\n",
      "Epoch 33/300\n",
      "2027/2027 [==============================] - 0s 240us/step - loss: 2.1320 - val_loss: 2.0876\n",
      "Epoch 34/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1275 - val_loss: 2.0870\n",
      "Epoch 35/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1264 - val_loss: 2.0815\n",
      "Epoch 36/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1300 - val_loss: 2.0798\n",
      "Epoch 37/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1225 - val_loss: 2.0811\n",
      "Epoch 38/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1264 - val_loss: 2.0914\n",
      "Epoch 39/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.1219 - val_loss: 2.0853\n",
      "Epoch 40/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1183 - val_loss: 2.0818\n",
      "Epoch 41/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1163 - val_loss: 2.0895\n",
      "Epoch 42/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1205 - val_loss: 2.0829\n",
      "Epoch 43/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1237 - val_loss: 2.1009\n",
      "Epoch 44/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1135 - val_loss: 2.0769\n",
      "Epoch 45/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1188 - val_loss: 2.0755\n",
      "Epoch 46/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1170 - val_loss: 2.0694\n",
      "Epoch 47/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.1139 - val_loss: 2.0847\n",
      "Epoch 48/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1149 - val_loss: 2.0811\n",
      "Epoch 49/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1145 - val_loss: 2.0756\n",
      "Epoch 50/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1074 - val_loss: 2.0696\n",
      "Epoch 51/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1089 - val_loss: 2.0704\n",
      "Epoch 52/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1117 - val_loss: 2.0722\n",
      "Epoch 53/300\n",
      "2027/2027 [==============================] - 0s 240us/step - loss: 2.1102 - val_loss: 2.0762\n",
      "Epoch 54/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1095 - val_loss: 2.0734\n",
      "Epoch 55/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1093 - val_loss: 2.0717\n",
      "Epoch 56/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1047 - val_loss: 2.0700\n",
      "\n",
      "Epoch 00056: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 57/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.1020 - val_loss: 2.0598\n",
      "Epoch 58/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.0950 - val_loss: 2.0597\n",
      "Epoch 59/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0995 - val_loss: 2.0601\n",
      "Epoch 60/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0943 - val_loss: 2.0602\n",
      "Epoch 61/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.0983 - val_loss: 2.0592\n",
      "Epoch 62/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0991 - val_loss: 2.0593\n",
      "Epoch 63/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0926 - val_loss: 2.0594\n",
      "Epoch 64/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0951 - val_loss: 2.0597\n",
      "Epoch 65/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0983 - val_loss: 2.0600\n",
      "Epoch 66/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0947 - val_loss: 2.0602\n",
      "Epoch 67/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.0960 - val_loss: 2.0594\n",
      "Epoch 68/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0919 - val_loss: 2.0600\n",
      "Epoch 69/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0949 - val_loss: 2.0599\n",
      "Epoch 70/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0972 - val_loss: 2.0594\n",
      "Epoch 71/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0959 - val_loss: 2.0584\n",
      "Epoch 72/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0950 - val_loss: 2.0591\n",
      "Epoch 73/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0922 - val_loss: 2.0584\n",
      "Epoch 74/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0944 - val_loss: 2.0599\n",
      "Epoch 75/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0996 - val_loss: 2.0593\n",
      "Epoch 76/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0962 - val_loss: 2.0601\n",
      "Epoch 77/300\n",
      "2027/2027 [==============================] - 0s 247us/step - loss: 2.0940 - val_loss: 2.0600\n",
      "Epoch 78/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0955 - val_loss: 2.0578\n",
      "Epoch 79/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0963 - val_loss: 2.0588\n",
      "Epoch 80/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.0974 - val_loss: 2.0581\n",
      "Epoch 81/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.0972 - val_loss: 2.0596\n",
      "Epoch 82/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.0922 - val_loss: 2.0591\n",
      "Epoch 83/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0976 - val_loss: 2.0582\n",
      "Epoch 84/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.0927 - val_loss: 2.0587\n",
      "Epoch 85/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0932 - val_loss: 2.0578\n",
      "Epoch 86/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0946 - val_loss: 2.0585\n",
      "Epoch 87/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.0920 - val_loss: 2.0579\n",
      "Epoch 88/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0901 - val_loss: 2.0590\n",
      "\n",
      "Epoch 00088: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 89/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.0926 - val_loss: 2.0581\n",
      "Epoch 90/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.0947 - val_loss: 2.0580\n",
      "Epoch 91/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0925 - val_loss: 2.0580\n",
      "Epoch 92/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0902 - val_loss: 2.0580\n",
      "Epoch 93/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0972 - val_loss: 2.0580\n",
      "Epoch 00093: early stopping\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "train = anndata.AnnData(expression_train)\n",
    "res = dca(train, verbose=True, mode=\"latent\", hidden_size=(64, 32, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.43730906, 0.7856623727581954, 0.8367413290660001]\n",
      "[0.3622666, 0.6974055612937855, 0.7671222348798609]\n",
      "[0.37938064, 0.6329615289828123, 0.5445525050067515]\n",
      "[0.21778725, 0.6648547814327295, 0.5989110659229909]\n"
     ]
    }
   ],
   "source": [
    "latent = train.obsm['X_dca']\n",
    "# these default results are not good. Let's try with another hidden size and report the best\n",
    "print(cluster_scores(latent, 2, agg_clusters_2(c_train)))\n",
    "print(cluster_scores(latent, 3, agg_clusters_3(c_train)))\n",
    "print(cluster_scores(latent, 4, agg_clusters_4(c_train)))\n",
    "print(cluster_scores(latent, 7, c_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCA: Successfully preprocessed 558 genes and 2253 cells.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 558)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           35776       count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 10)           650         enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 10)           30          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 10)           0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           704         center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (None, 558)          0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 558)          0           output[0][0]                     \n",
      "                                                                 dispersion[0][0]                 \n",
      "                                                                 pi[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 146,354\n",
      "Trainable params: 146,078\n",
      "Non-trainable params: 276\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2027 samples, validate on 226 samples\n",
      "Epoch 1/300\n",
      "2027/2027 [==============================] - 2s 776us/step - loss: 3.7176 - val_loss: 2.6899\n",
      "Epoch 2/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.5830 - val_loss: 2.3841\n",
      "Epoch 3/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.3926 - val_loss: 2.2853\n",
      "Epoch 4/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.3331 - val_loss: 2.2526\n",
      "Epoch 5/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.2905 - val_loss: 2.2065\n",
      "Epoch 6/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.2710 - val_loss: 2.1932\n",
      "Epoch 7/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.2541 - val_loss: 2.1863\n",
      "Epoch 8/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.2459 - val_loss: 2.1730\n",
      "Epoch 9/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.2357 - val_loss: 2.1726\n",
      "Epoch 10/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.2297 - val_loss: 2.1579\n",
      "Epoch 11/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.2233 - val_loss: 2.1574\n",
      "Epoch 12/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.2227 - val_loss: 2.1537\n",
      "Epoch 13/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.2119 - val_loss: 2.1649\n",
      "Epoch 14/300\n",
      "2027/2027 [==============================] - 1s 249us/step - loss: 2.2105 - val_loss: 2.1444\n",
      "Epoch 15/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.2111 - val_loss: 2.1425\n",
      "Epoch 16/300\n",
      "2027/2027 [==============================] - 0s 247us/step - loss: 2.2056 - val_loss: 2.1383\n",
      "Epoch 17/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.2030 - val_loss: 2.1406\n",
      "Epoch 18/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.2009 - val_loss: 2.1381\n",
      "Epoch 19/300\n",
      "2027/2027 [==============================] - 1s 250us/step - loss: 2.1979 - val_loss: 2.1364\n",
      "Epoch 20/300\n",
      "2027/2027 [==============================] - 1s 249us/step - loss: 2.1931 - val_loss: 2.1367\n",
      "Epoch 21/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1928 - val_loss: 2.1329\n",
      "Epoch 22/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1921 - val_loss: 2.1314\n",
      "Epoch 23/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1931 - val_loss: 2.1286\n",
      "Epoch 24/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1827 - val_loss: 2.1353\n",
      "Epoch 25/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1902 - val_loss: 2.1240\n",
      "Epoch 26/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1853 - val_loss: 2.1252\n",
      "Epoch 27/300\n",
      "2027/2027 [==============================] - 0s 247us/step - loss: 2.1836 - val_loss: 2.1217\n",
      "Epoch 28/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1811 - val_loss: 2.1266\n",
      "Epoch 29/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1869 - val_loss: 2.1240\n",
      "Epoch 30/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1845 - val_loss: 2.1226\n",
      "Epoch 31/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1794 - val_loss: 2.1390\n",
      "Epoch 32/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1804 - val_loss: 2.1305\n",
      "Epoch 33/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1772 - val_loss: 2.1215\n",
      "Epoch 34/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1717 - val_loss: 2.1208\n",
      "Epoch 35/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1716 - val_loss: 2.1175\n",
      "Epoch 36/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1748 - val_loss: 2.1153\n",
      "Epoch 37/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1693 - val_loss: 2.1198\n",
      "Epoch 38/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1731 - val_loss: 2.1236\n",
      "Epoch 39/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1686 - val_loss: 2.1204\n",
      "Epoch 40/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1652 - val_loss: 2.1205\n",
      "Epoch 41/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1634 - val_loss: 2.1243\n",
      "Epoch 42/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1687 - val_loss: 2.1137\n",
      "Epoch 43/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1730 - val_loss: 2.1360\n",
      "Epoch 44/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1622 - val_loss: 2.1100\n",
      "Epoch 45/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1687 - val_loss: 2.1194\n",
      "Epoch 46/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1642 - val_loss: 2.1095\n",
      "Epoch 47/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1631 - val_loss: 2.1209\n",
      "Epoch 48/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1645 - val_loss: 2.1155\n",
      "Epoch 49/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1641 - val_loss: 2.1123\n",
      "Epoch 50/300\n",
      "2027/2027 [==============================] - 1s 252us/step - loss: 2.1558 - val_loss: 2.1076\n",
      "Epoch 51/300\n",
      "2027/2027 [==============================] - 1s 253us/step - loss: 2.1583 - val_loss: 2.1081\n",
      "Epoch 52/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1622 - val_loss: 2.1090\n",
      "Epoch 53/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1613 - val_loss: 2.1172\n",
      "Epoch 54/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1608 - val_loss: 2.1097\n",
      "Epoch 55/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1593 - val_loss: 2.1111\n",
      "Epoch 56/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1558 - val_loss: 2.1098\n",
      "Epoch 57/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1595 - val_loss: 2.1066\n",
      "Epoch 58/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1530 - val_loss: 2.1103\n",
      "Epoch 59/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1587 - val_loss: 2.1096\n",
      "Epoch 60/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1526 - val_loss: 2.1073\n",
      "Epoch 61/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1555 - val_loss: 2.1046\n",
      "Epoch 62/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1580 - val_loss: 2.1143\n",
      "Epoch 63/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1493 - val_loss: 2.1037\n",
      "Epoch 64/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1503 - val_loss: 2.1066\n",
      "Epoch 65/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1546 - val_loss: 2.1054\n",
      "Epoch 66/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1502 - val_loss: 2.1037\n",
      "Epoch 67/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1515 - val_loss: 2.1059\n",
      "Epoch 68/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1465 - val_loss: 2.1047\n",
      "Epoch 69/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1495 - val_loss: 2.1029\n",
      "Epoch 70/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1520 - val_loss: 2.1008\n",
      "Epoch 71/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.1510 - val_loss: 2.1044\n",
      "Epoch 72/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1494 - val_loss: 2.0994\n",
      "Epoch 73/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.1467 - val_loss: 2.1000\n",
      "Epoch 74/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1451 - val_loss: 2.1117\n",
      "Epoch 75/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1524 - val_loss: 2.1001\n",
      "Epoch 76/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1488 - val_loss: 2.1062\n",
      "Epoch 77/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1462 - val_loss: 2.1078\n",
      "Epoch 78/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1474 - val_loss: 2.1030\n",
      "Epoch 79/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1479 - val_loss: 2.1065\n",
      "Epoch 80/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1486 - val_loss: 2.0986\n",
      "Epoch 81/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1489 - val_loss: 2.1033\n",
      "Epoch 82/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1422 - val_loss: 2.1020\n",
      "Epoch 83/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1482 - val_loss: 2.0997\n",
      "Epoch 84/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1417 - val_loss: 2.1143\n",
      "Epoch 85/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1430 - val_loss: 2.0998\n",
      "Epoch 86/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1453 - val_loss: 2.0979\n",
      "Epoch 87/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1414 - val_loss: 2.0975\n",
      "Epoch 88/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1379 - val_loss: 2.0985\n",
      "Epoch 89/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1409 - val_loss: 2.0998\n",
      "Epoch 90/300\n",
      "2027/2027 [==============================] - 1s 251us/step - loss: 2.1435 - val_loss: 2.0972\n",
      "Epoch 91/300\n",
      "2027/2027 [==============================] - 1s 258us/step - loss: 2.1409 - val_loss: 2.0941\n",
      "Epoch 92/300\n",
      "2027/2027 [==============================] - 1s 252us/step - loss: 2.1364 - val_loss: 2.1002\n",
      "Epoch 93/300\n",
      "2027/2027 [==============================] - 1s 252us/step - loss: 2.1444 - val_loss: 2.1271\n",
      "Epoch 94/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1427 - val_loss: 2.0945\n",
      "Epoch 95/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1415 - val_loss: 2.1074\n",
      "Epoch 96/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1450 - val_loss: 2.0985\n",
      "Epoch 97/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1407 - val_loss: 2.0973\n",
      "Epoch 98/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1338 - val_loss: 2.0988\n",
      "Epoch 99/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1396 - val_loss: 2.0930\n",
      "Epoch 100/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1377 - val_loss: 2.1036\n",
      "Epoch 101/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1372 - val_loss: 2.1098\n",
      "Epoch 102/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1406 - val_loss: 2.0946\n",
      "Epoch 103/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1375 - val_loss: 2.1010\n",
      "Epoch 104/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1362 - val_loss: 2.0981\n",
      "Epoch 105/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1324 - val_loss: 2.0939\n",
      "Epoch 106/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1323 - val_loss: 2.0937\n",
      "Epoch 107/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1343 - val_loss: 2.0962\n",
      "Epoch 108/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1394 - val_loss: 2.0914\n",
      "Epoch 109/300\n",
      "2027/2027 [==============================] - 0s 247us/step - loss: 2.1398 - val_loss: 2.0945\n",
      "Epoch 110/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1307 - val_loss: 2.0909\n",
      "Epoch 111/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1317 - val_loss: 2.0949\n",
      "Epoch 112/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1338 - val_loss: 2.0937\n",
      "Epoch 113/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1321 - val_loss: 2.1038\n",
      "Epoch 114/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1373 - val_loss: 2.0931\n",
      "Epoch 115/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1360 - val_loss: 2.0919\n",
      "Epoch 116/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1312 - val_loss: 2.0956\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1340 - val_loss: 2.1139\n",
      "Epoch 118/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1331 - val_loss: 2.0950\n",
      "Epoch 119/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1330 - val_loss: 2.0974\n",
      "Epoch 120/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1350 - val_loss: 2.1046\n",
      "\n",
      "Epoch 00120: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 121/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1283 - val_loss: 2.0868\n",
      "Epoch 122/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1232 - val_loss: 2.0864\n",
      "Epoch 123/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1226 - val_loss: 2.0858\n",
      "Epoch 124/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1249 - val_loss: 2.0863\n",
      "Epoch 125/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1262 - val_loss: 2.0862\n",
      "Epoch 126/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1298 - val_loss: 2.0852\n",
      "Epoch 127/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1199 - val_loss: 2.0864\n",
      "Epoch 128/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1212 - val_loss: 2.0867\n",
      "Epoch 129/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1250 - val_loss: 2.0852\n",
      "Epoch 130/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1198 - val_loss: 2.0862\n",
      "Epoch 131/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1238 - val_loss: 2.0850\n",
      "Epoch 132/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1235 - val_loss: 2.0855\n",
      "Epoch 133/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1212 - val_loss: 2.0854\n",
      "Epoch 134/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1185 - val_loss: 2.0852\n",
      "Epoch 135/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1208 - val_loss: 2.0846\n",
      "Epoch 136/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1262 - val_loss: 2.0852\n",
      "Epoch 137/300\n",
      "2027/2027 [==============================] - 0s 240us/step - loss: 2.1274 - val_loss: 2.0858\n",
      "Epoch 138/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1235 - val_loss: 2.0863\n",
      "Epoch 139/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1231 - val_loss: 2.0873\n",
      "Epoch 140/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1227 - val_loss: 2.0867\n",
      "Epoch 141/300\n",
      "2027/2027 [==============================] - 1s 251us/step - loss: 2.1206 - val_loss: 2.0876\n",
      "Epoch 142/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1225 - val_loss: 2.0855\n",
      "Epoch 143/300\n",
      "2027/2027 [==============================] - 0s 241us/step - loss: 2.1222 - val_loss: 2.0864\n",
      "Epoch 144/300\n",
      "2027/2027 [==============================] - 1s 251us/step - loss: 2.1222 - val_loss: 2.0863\n",
      "Epoch 145/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1268 - val_loss: 2.0852\n",
      "\n",
      "Epoch 00145: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 146/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1272 - val_loss: 2.0853\n",
      "Epoch 147/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1258 - val_loss: 2.0855\n",
      "Epoch 148/300\n",
      "2027/2027 [==============================] - 0s 240us/step - loss: 2.1198 - val_loss: 2.0856\n",
      "Epoch 149/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1228 - val_loss: 2.0856\n",
      "Epoch 150/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1183 - val_loss: 2.0854\n",
      "Epoch 00150: early stopping\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "train = anndata.AnnData(expression_train)\n",
    "res = dca(train, verbose=True, mode=\"latent\", hidden_size=(64, 10, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.42883036, 0.7577774656410581, 0.8077455329908643]\n",
      "[0.3955544, 0.6787002314106516, 0.7531831865665952]\n",
      "[0.38264117, 0.7026881105653309, 0.7931154496909288]\n",
      "[0.23001896, 0.582250654727492, 0.508371204260512]\n"
     ]
    }
   ],
   "source": [
    "latent = train.obsm['X_dca']\n",
    "# these default results are very poor. Let's try with latent space size 10 and report the best out of the two\n",
    "print(cluster_scores(latent, 2, agg_clusters_2(c_train)))\n",
    "print(cluster_scores(latent, 3, agg_clusters_3(c_train)))\n",
    "print(cluster_scores(latent, 4, agg_clusters_4(c_train)))\n",
    "print(cluster_scores(latent, 7, c_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCA: Successfully preprocessed 558 genes and 2253 cells.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 558)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           35776       count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (None, 558)          0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 558)          36270       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 558)          0           output[0][0]                     \n",
      "                                                                 dispersion[0][0]                 \n",
      "                                                                 pi[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 149,258\n",
      "Trainable params: 148,938\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 2027 samples, validate on 226 samples\n",
      "Epoch 1/300\n",
      "2027/2027 [==============================] - 1s 717us/step - loss: 3.4218 - val_loss: 2.5962\n",
      "Epoch 2/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.4633 - val_loss: 2.3659\n",
      "Epoch 3/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.3289 - val_loss: 2.2960\n",
      "Epoch 4/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.2641 - val_loss: 2.2322\n",
      "Epoch 5/300\n",
      "2027/2027 [==============================] - 0s 247us/step - loss: 2.2361 - val_loss: 2.2177\n",
      "Epoch 6/300\n",
      "2027/2027 [==============================] - 1s 250us/step - loss: 2.2153 - val_loss: 2.1827\n",
      "Epoch 7/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1996 - val_loss: 2.1711\n",
      "Epoch 8/300\n",
      "2027/2027 [==============================] - 1s 252us/step - loss: 2.1844 - val_loss: 2.1708\n",
      "Epoch 9/300\n",
      "2027/2027 [==============================] - 1s 250us/step - loss: 2.1794 - val_loss: 2.1661\n",
      "Epoch 10/300\n",
      "2027/2027 [==============================] - 1s 251us/step - loss: 2.1665 - val_loss: 2.1435\n",
      "Epoch 11/300\n",
      "2027/2027 [==============================] - 1s 254us/step - loss: 2.1633 - val_loss: 2.1527\n",
      "Epoch 12/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1615 - val_loss: 2.1567\n",
      "Epoch 13/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1606 - val_loss: 2.1433\n",
      "Epoch 14/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1522 - val_loss: 2.1302\n",
      "Epoch 15/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.1548 - val_loss: 2.1286\n",
      "Epoch 16/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1484 - val_loss: 2.1263\n",
      "Epoch 17/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1382 - val_loss: 2.1194\n",
      "Epoch 18/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1424 - val_loss: 2.1246\n",
      "Epoch 19/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1393 - val_loss: 2.1265\n",
      "Epoch 20/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1365 - val_loss: 2.1221\n",
      "Epoch 21/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1319 - val_loss: 2.1163\n",
      "Epoch 22/300\n",
      "2027/2027 [==============================] - 1s 248us/step - loss: 2.1286 - val_loss: 2.1237\n",
      "Epoch 23/300\n",
      "2027/2027 [==============================] - 1s 253us/step - loss: 2.1309 - val_loss: 2.1287\n",
      "Epoch 24/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1311 - val_loss: 2.1277\n",
      "Epoch 25/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.1255 - val_loss: 2.1081\n",
      "Epoch 26/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1249 - val_loss: 2.1121\n",
      "Epoch 27/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1209 - val_loss: 2.1112\n",
      "Epoch 28/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1179 - val_loss: 2.1088\n",
      "Epoch 29/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1150 - val_loss: 2.1158\n",
      "Epoch 30/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1144 - val_loss: 2.1212\n",
      "Epoch 31/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.1135 - val_loss: 2.1118\n",
      "Epoch 32/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1129 - val_loss: 2.1142\n",
      "Epoch 33/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.1145 - val_loss: 2.1253\n",
      "Epoch 34/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1111 - val_loss: 2.1106\n",
      "Epoch 35/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1084 - val_loss: 2.1214\n",
      "\n",
      "Epoch 00035: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 36/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.1014 - val_loss: 2.0983\n",
      "Epoch 37/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0986 - val_loss: 2.0966\n",
      "Epoch 38/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0953 - val_loss: 2.0964\n",
      "Epoch 39/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0966 - val_loss: 2.0980\n",
      "Epoch 40/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.0985 - val_loss: 2.0958\n",
      "Epoch 41/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.0931 - val_loss: 2.0970\n",
      "Epoch 42/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.0979 - val_loss: 2.0965\n",
      "Epoch 43/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0989 - val_loss: 2.0974\n",
      "Epoch 44/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0935 - val_loss: 2.0970\n",
      "Epoch 45/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.0971 - val_loss: 2.0963\n",
      "Epoch 46/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0960 - val_loss: 2.0967\n",
      "Epoch 47/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.0998 - val_loss: 2.0951\n",
      "Epoch 48/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.0947 - val_loss: 2.0965\n",
      "Epoch 49/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0933 - val_loss: 2.0960\n",
      "Epoch 50/300\n",
      "2027/2027 [==============================] - 1s 247us/step - loss: 2.0964 - val_loss: 2.0966\n",
      "Epoch 51/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.1010 - val_loss: 2.0966\n",
      "Epoch 52/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0965 - val_loss: 2.0961\n",
      "Epoch 53/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0990 - val_loss: 2.0963\n",
      "Epoch 54/300\n",
      "2027/2027 [==============================] - 0s 245us/step - loss: 2.0966 - val_loss: 2.0963\n",
      "Epoch 55/300\n",
      "2027/2027 [==============================] - 0s 242us/step - loss: 2.0933 - val_loss: 2.0953\n",
      "Epoch 56/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0923 - val_loss: 2.0966\n",
      "Epoch 57/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.0918 - val_loss: 2.0957\n",
      "\n",
      "Epoch 00057: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 58/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0950 - val_loss: 2.0956\n",
      "Epoch 59/300\n",
      "2027/2027 [==============================] - 0s 243us/step - loss: 2.0919 - val_loss: 2.0953\n",
      "Epoch 60/300\n",
      "2027/2027 [==============================] - 0s 246us/step - loss: 2.0914 - val_loss: 2.0953\n",
      "Epoch 61/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0942 - val_loss: 2.0952\n",
      "Epoch 62/300\n",
      "2027/2027 [==============================] - 0s 244us/step - loss: 2.0926 - val_loss: 2.0952\n",
      "Epoch 00062: early stopping\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "train = anndata.AnnData(X_zero)\n",
    "res = dca(train, verbose=True, mode=\"denoise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.819500923156738"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_index = i[ix], j[ix]\n",
    "x, y = train.X[all_index], expression_train[all_index]\n",
    "np.median(np.abs(x - y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
