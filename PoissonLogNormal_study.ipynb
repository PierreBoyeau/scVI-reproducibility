{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly as py\n",
    "import pandas as pd\n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from scvi.dataset import PowSimSynthetic, LatentLogPoissonDataset, SignedGamma, GeneExpressionDataset\n",
    "from scvi.models import VAE, IAVAE\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.utils import demultiply, make_dir_if_necessary, predict_de_genes, save_fig, load_pickle, save_pickle\n",
    "from scvi_utils import estimate_de_proba, estimate_lfc_density, estimate_lfc_mean, multi_train_estimates\n",
    "from R_interop import all_predictions, all_de_predictions\n",
    "\n",
    "\n",
    "N_EPOCHS = 100\n",
    "DELTA = 0.5\n",
    "SIZES = [5, 10, 20, 30, 50, 100]\n",
    "SIZE = 100\n",
    "N_SIZES = len(SIZES)\n",
    "DO_CLOUD = True\n",
    "Q0 = 5e-2\n",
    "N_TRAININGS = 5\n",
    "N_PICKS = 10\n",
    "n_genes = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "PATH_TO_SCRIPTS = \"/home/ubuntu/conquer_comparison/scripts\"\n",
    "DIR_PATH = 'lfc_estimates/lognormal'\n",
    "DF_PATH = \"/home/ubuntu/scVI/scvi/dataset/kolodziejczk_param.csv\"\n",
    "make_dir_if_necessary(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "py.sign_in(\"pierreboyeau\", \"2wvdnWZ2Qut1zD07ADVy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Constructing mu and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.read_csv(DF_PATH).sample(n_genes)\n",
    "means = selected[\"means\"].values\n",
    "\n",
    "means[means >= 1000] = 1000\n",
    "go.Figure([go.Histogram(x=means)]).show()\n",
    "\n",
    "lfc_sampler = SignedGamma(dim=2, proba_pos=0.5)\n",
    "lfcs = lfc_sampler.sample(n_genes).numpy()\n",
    "non_de_genes = np.random.choice(n_genes, size=300)\n",
    "lfcs[non_de_genes, :] = 0.0\n",
    "go.Figure([go.Histogram(x=lfcs[:, 0])]).show()\n",
    "\n",
    "log2_mu0 = lfcs[:, 0] + np.log2(means)\n",
    "log2_mu1 = lfcs[:, 1] + np.log2(means)\n",
    "\n",
    "loge_mu0 = log2_mu0 / np.log2(np.e)\n",
    "loge_mu1 = log2_mu1 / np.log2(np.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO\n",
    "a = (2.0 * np.random.random(size=(100, 1)) - 1).astype(float)\n",
    "sigma = 2.0*a.dot(a.T) + (1.0 + 0.5*(2.0*np.random.random(100)-1.0)) * np.eye(100)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(z=sigma))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2.0 * np.random.random(size=(n_genes, 1)) - 1).astype(float)\n",
    "sigma = 2.0*a.dot(a.T) + 0.5*(1.0 + 0.5*(2.0*np.random.random(n_genes)-1.0)) * np.eye(n_genes)\n",
    "sigma0 = 0.1*sigma\n",
    "\n",
    "a = (2.0 * np.random.random(size=(n_genes, 1)) - 1).astype(float)\n",
    "sigma = 2.0*a.dot(a.T) + 0.5*(1.0 + 0.5*(2.0*np.random.random(n_genes)-1.0)) * np.eye(n_genes)\n",
    "sigma1 = 0.1*sigma\n",
    "\n",
    "# sigma1 = sigma\n",
    "\n",
    "# u, s, vh = np.linalg.svd(sigma)\n",
    "# perturbations = s.min() + (s.max() - s.min()) * np.random.random(len(s))\n",
    "# sigma1 = u @ (np.diag(perturbations)) @ vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.tensor(loge_mu0), covariance_matrix=torch.tensor(sigma0)\n",
    ").sample((5000,))\n",
    "h1 = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.tensor(loge_mu1), covariance_matrix=torch.tensor(sigma1)\n",
    ").sample((5000,))\n",
    "\n",
    "h = torch.cat([h0, h1])\n",
    "\n",
    "x_obs = torch.distributions.Poisson(rate=h.exp()).sample()\n",
    "# is_zi = np.random.random(x_obs.shape) >= 0.9\n",
    "is_zi = np.random.random(x_obs.shape) <= np.exp(-1.4 * x_obs.numpy())\n",
    "x_obs[is_zi] = 0.0\n",
    "labels = torch.zeros((10000, 1))\n",
    "labels[5000:] = 1\n",
    "\n",
    "not_null_cell = (x_obs.sum(1) != 0)\n",
    "x_obs = x_obs[not_null_cell]\n",
    "labels = labels[not_null_cell]\n",
    "\n",
    "trace1 = go.Histogram(x=x_obs.mean(0))\n",
    "fig = go.Figure(data=[trace1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(x_obs[:, 500], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(DIR_PATH, \"dataset.pickle\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    dataset = GeneExpressionDataset()\n",
    "    dataset.populate_from_data(X=x_obs.numpy(), labels=labels.numpy())\n",
    "    dataset.lfc = lfcs\n",
    "    save_pickle(data=dataset, filename=dataset_path)\n",
    "else:\n",
    "    dataset = load_pickle(dataset_path)\n",
    "    lfcs = dataset.lfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_significant_de = np.abs(lfcs[:, 1] - lfcs[:, 0]) >= DELTA\n",
    "n_genes = dataset.nb_genes\n",
    "trace1 = go.Histogram(x=lfcs[:, 1] - lfcs[:, 0])\n",
    "fig = go.Figure(data=[trace1])\n",
    "# save_fig(fig, filename=\"powsimR_properties\", do_cloud=DO_CLOUD)\n",
    "# fig.show()\n",
    "iplot(fig, filename=\"lognormal_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = len(dataset)\n",
    "TEST_INDICES = np.random.permutation(n_examples)[:2000]\n",
    "\n",
    "x_test, y_test = dataset.X[TEST_INDICES, :], dataset.labels[TEST_INDICES, :].squeeze()\n",
    "data_path = os.path.join(DIR_PATH, 'data.npy')\n",
    "labels_path = os.path.join(DIR_PATH, 'labels.npy')\n",
    "\n",
    "np.save(\n",
    "    data_path,\n",
    "    x_test.squeeze().astype(int)\n",
    ")\n",
    "np.savetxt(\n",
    "    labels_path,\n",
    "    y_test.squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_params = dict(\n",
    "    iaf=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4),\n",
    "#     iaf=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=20, t=3),\n",
    "#     iaf_b=dict(n_hidden=128, n_layers=2, do_h=False, n_latent=10, t=3),\n",
    "#     mf=dict(n_hidden=128, n_layers=1, n_latent=20),\n",
    "    mf=dict(n_hidden=128, n_layers=1, n_latent=10),\n",
    "    iaf_k5=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4),\n",
    "    mf_k5=dict(n_hidden=128, n_layers=1, n_latent=10),\n",
    ")\n",
    "train_params = dict(\n",
    "    iaf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    iaf_b=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    mf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    iaf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5, single_backward=True),\n",
    "    mf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5, single_backward=True)\n",
    ")\n",
    "train_fn_params = dict(\n",
    "    iaf=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    iaf_b=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    mf=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    iaf_k5=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    mf_k5=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute competitors scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions = all_predictions(\n",
    "    filename=os.path.join(DIR_PATH, \"other_predictions1.pickle\"),\n",
    "    n_genes=n_genes, \n",
    "    n_picks=N_PICKS, \n",
    "    sizes=SIZES, \n",
    "    data_path=data_path, \n",
    "    labels_path=labels_path,\n",
    "    path_to_scripts=PATH_TO_SCRIPTS\n",
    ")\n",
    "\n",
    "# other_predictions = all_predictions(\n",
    "#     filename=os.path.join(DIR_PATH, \"other_predictions3.pickle\"),\n",
    "#     n_genes=n_genes, \n",
    "#     n_picks=1, \n",
    "#     sizes=[100], \n",
    "#     data_path=data_path, \n",
    "#     labels_path=labels_path,\n",
    "#     path_to_scripts=PATH_TO_SCRIPTS\n",
    "# )\n",
    "\n",
    "other_predictions = all_de_predictions(\n",
    "    other_predictions, significance_level=Q0, delta=DELTA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sign of LFC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.utils import plot_identity\n",
    "\n",
    "lfc_gt = -(lfcs[:, 1] - lfcs[:, 0])\n",
    "plt.scatter(lfc_gt, other_predictions[\"edger\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(lfc_gt, other_predictions[\"deseq2\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(lfc_gt, other_predictions[\"mast\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"] = other_predictions[\"edger\"][\"lfc\"]\n",
    "other_predictions[\"mast\"][\"lfc\"] = other_predictions[\"mast\"][\"lfc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_mf7.pickle\"),\n",
    "    mdl_class=VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    "    sizes=SIZES,\n",
    "#     sizes=[100],\n",
    "#     n_trainings=1,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    label_b=1\n",
    ").assign(algorithm=\"MF\")\n",
    "\n",
    "res_iaf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_iaf7.pickle\"),\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "    sizes=SIZES,\n",
    "#     sizes=[100],\n",
    "#     n_trainings=1,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    label_b=1\n",
    ").assign(algorithm=\"IAF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR / Power Control and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    mdl_class, dataset, mdl_params: dict, train_params: dict, train_fn_params: dict\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param mdl_class: Class of algorithm\n",
    "    :param dataset: Dataset\n",
    "    :param mdl_params:\n",
    "    :param train_params:\n",
    "    :param train_fn_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    my_vae = mdl_class(dataset.nb_genes, n_batch=dataset.n_batches, **mdl_params)\n",
    "    my_trainer = UnsupervisedTrainer(my_vae, dataset, **train_params)\n",
    "    print(my_trainer.test_set.data_loader.sampler.indices)\n",
    "    my_trainer.train(**train_fn_params)\n",
    "    print(my_trainer.train_losses)\n",
    "    return my_vae, my_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### FDR and TPR Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_fnr(my_df):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    assert len(my_df) == n_genes\n",
    "    is_pred_de = predict_de_genes(my_df.de_proba.values, desired_fdr=Q0)\n",
    "    alpha = my_df.de_proba.values[is_pred_de].min()\n",
    "    true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "    n_positives = is_significant_de.sum()\n",
    "    true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "    return pd.Series(dict(fdr=true_fdr, fnr=true_fnr, alpha=alpha))\n",
    "\n",
    "\n",
    "fdr_fnr_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "fdr_fnr_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "\n",
    "df = pd.concat([fdr_fnr_mf, fdr_fnr_iaf], ignore_index=True)\n",
    "\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fdr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Discovery Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_fdr_control\")\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fnr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Negative Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_power_control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_fnr_iaf.groupby(\"sample_size\").alpha.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_fnr_mf.groupby(\"sample_size\").alpha.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['deseq2', 'edger', 'mast']\n",
    "\n",
    "def get_fdr_fnr(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        y_pred: (n_sz, n_picks, n_genes) bool predictions\n",
    "        y_true: (n_genes) gt vals\n",
    "    \"\"\"\n",
    "    n_sz, n_picks, _ = y_pred.shape\n",
    "    fnrs = np.zeros((n_sz, n_picks))\n",
    "    fdrs = np.zeros((n_sz, n_picks))\n",
    "    for sz in range(n_sz):\n",
    "        for pick in range(n_picks):\n",
    "            y_pred_it = y_pred[sz, pick, :]\n",
    "            fnr = ((~y_true) * y_pred_it).sum() / y_pred_it.sum()\n",
    "            fdr = (y_true * (~y_pred_it)).sum() / y_true.sum()\n",
    "            fnrs[sz, pick] = fnr\n",
    "            fdrs[sz, pick] = fdr\n",
    "    fnrs[np.isnan(fnrs)] = 0.0\n",
    "    return dict(fnr=fnrs, fdr=fdrs)\n",
    "\n",
    "print(other_predictions[\"mast\"]['pval'].shape)\n",
    "print(other_predictions[\"deseq2\"]['pval'].shape)\n",
    "print(other_predictions[\"edger\"]['pval'].shape)\n",
    "\n",
    "is_de_mast = other_predictions[\"mast\"][\"is_de\"]\n",
    "is_de_deseq2 = other_predictions[\"deseq2\"][\"is_de\"]\n",
    "is_de_edger = other_predictions[\"edger\"][\"is_de\"]\n",
    "# is_de_edgerr = other_predictions[\"edger_robust\"][\"is_de\"]\n",
    "\n",
    "\n",
    "res_mast = get_fdr_fnr(is_de_mast, y_true=is_significant_de)\n",
    "res_deseq2 = get_fdr_fnr(is_de_deseq2, y_true=is_significant_de)\n",
    "res_edger = get_fdr_fnr(is_de_edger, y_true=is_significant_de)\n",
    "# res_edgerr = get_fdr_fnr(is_de_edgerr, y_true=is_significant_de)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Uderstand why different results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf = res_mf[(res_mf.experiment == 0) & (res_mf.training == 0) & (res_mf.sample_size == 100)]\n",
    "preds_iaf = res_iaf[(res_iaf.experiment == 0) & (res_iaf.training == 0) & (res_iaf.sample_size == 100)]\n",
    "\n",
    "# preds_mf = preds_mf.sort_values(\"de_proba\").set_index(\"gene\")\n",
    "# preds_iaf = preds_iaf.set_index(\"gene\").reindex(index=preds_mf.index)\n",
    "# preds_iaf[]\n",
    "\n",
    "preds = pd.concat([preds_mf, preds_iaf], ignore_index=True)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf = preds_mf.assign(\n",
    "    de_proba_iaf=preds_iaf.de_proba,\n",
    "    gene_mean=dataset.X.mean(0),\n",
    "    is_de=is_significant_de.astype(float),\n",
    ")\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [preds_mf[\"de_proba\"], preds_mf[\"de_proba_iaf\"]],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [preds_mf[\"de_proba\"], preds_mf[\"de_proba_iaf\"]],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains_res = all_fdrs.mean(axis=1)\n",
    "# print(trains_res.mean(), trains_res.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_preds_1d = y_preds.reshape((-1, dataset.nb_genes))\n",
    "# n_exps = len(y_preds_1d)\n",
    "# confs = np.zeros((n_exps, 2, 2))\n",
    "# for i in range(n_exps):\n",
    "#     confs[i, :, :] = confusion_matrix(is_significant_de, y_preds_1d[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(is_significant_de, y_preds_1d[0, :])\n",
    "\n",
    "# confs_mean = confs.mean(0)\n",
    "# confs_mean\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(\n",
    "#     z=confs_mean, x=[\"Pred Negative\", \"Pred Positive\"], y=[\"GT Negative\", \"GT Positive\"]\n",
    "# )\n",
    "# fig.update({\"layout\": dict(title=\"Confusion Matrix\")})\n",
    "\n",
    "# py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 0\n",
    "\n",
    "preds_md = res_mf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "preds_iaf = res_iaf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# preds_deseq2 = 1.0 - other_predictions['deseq2']['pval'][-1, 0, :]\n",
    "# preds_edger = 1.0 - other_predictions['edger']['pval'][-1, 0, :]\n",
    "# preds_mast = 1.0 - other_predictions['mast']['pval'][-1, 0, :]\n",
    "\n",
    "preds_deseq2 = 1.0 - other_predictions['deseq2']['pval'][:]\n",
    "preds_edger = 1.0 - other_predictions['edger']['pval'][:]\n",
    "preds_mast = 1.0 - other_predictions['mast']['pval'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(preds_md).mean())\n",
    "print(np.isnan(preds_iaf).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_edger).mean())\n",
    "print(np.isnan(preds_mast).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_pr(fig, preds, y_true, name):\n",
    "    average_precision = average_precision_score(y_true, preds)\n",
    "    preds[np.isnan(preds)] = np.min(preds[~np.isnan(preds)])\n",
    "    precs, recs, _ = precision_recall_curve(y_true=y_true, probas_pred=preds)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=recs,\n",
    "            y=precs,\n",
    "            name=name+'@AP: {0:0.2f}'.format(average_precision)\n",
    "        )\n",
    "    )\n",
    "    return\n",
    "layout = go.Layout(\n",
    "    title='Precision Recall Curves',\n",
    "    xaxis=dict(title='Recall'),\n",
    "    yaxis=dict(title='Precision'),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "plot_pr(fig=fig, preds=preds_md, y_true=is_significant_de, name='MF')\n",
    "plot_pr(fig=fig, preds=preds_iaf, y_true=is_significant_de, name='IAF')\n",
    "plot_pr(fig=fig, preds=preds_deseq2, y_true=is_significant_de, name='DESeq2')\n",
    "plot_pr(fig=fig, preds=preds_edger, y_true=is_significant_de, name='EdgeR')\n",
    "# plot_pr(fig=fig, preds=preds_edgerr, y_true=is_significant_de, name='EdgeR Robust')\n",
    "plot_pr(fig=fig, preds=preds_mast, y_true=is_significant_de, name='MAST')\n",
    "\n",
    "fig.show()\n",
    "iplot(fig, filename=\"lognormal_pr_curves2\", sharing=\"private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ap(my_df):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    average_precision = average_precision_score(is_significant_de, my_df.de_proba)\n",
    "    return pd.Series(dict(AP=average_precision))\n",
    "\n",
    "\n",
    "ap_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "ap_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "\n",
    "all_ap = pd.concat([ap_mf, ap_iaf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(all_ap, x=\"sample_size\", y=\"AP\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ap.groupby([\"algorithm\", \"sample_size\"]).agg(dict(AP=[\"mean\", \"std\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc_gt = -(lfcs[:, 1] - lfcs[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_genes = np.sort(np.random.permutation(n_genes)[:150])\n",
    "\n",
    "lfcs_mf = (\n",
    "    res_mf\n",
    "    .loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")\n",
    "    [[\"lfc_mean\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes]\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_ia = (\n",
    "    res_iaf\n",
    "    .loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")\n",
    "    [[\"lfc_mean\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "all_lfcs = pd.concat([lfcs_mf, lfcs_ia], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    lfcs_mf,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    lfcs_ia,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    all_lfcs,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    color=\"algorithm\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_inside(my_df, confidence=64):\n",
    "    is_in_hdi = (lfc_gt <= my_df[\"hdi{}_high\".format(confidence)]) & (\n",
    "        lfc_gt >= my_df[\"hdi{}_low\".format(confidence)]\n",
    "    )\n",
    "    return pd.Series(dict(prop=is_in_hdi.mean()))\n",
    "\n",
    "\n",
    "mean_inside_hdi_mf = (\n",
    "    res_mf.groupby(by=[\"experiment\", \"sample_size\", \"training\"])\n",
    "    .apply(frac_inside, confidence=64)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "mean_inside_hdi_iaf = (\n",
    "    res_iaf.groupby(by=[\"experiment\", \"sample_size\", \"training\"])\n",
    "    .apply(frac_inside, confidence=64)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_mf = mean_inside_hdi_mf.groupby([\"sample_size\"]).prop.agg([\"mean\", \"std\"])\n",
    "summary_iaf = mean_inside_hdi_iaf.groupby([\"sample_size\"]).prop.agg([\"mean\", \"std\"])\n",
    "\n",
    "display(summary_mf)\n",
    "display(summary_iaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Bar(x=summary_mf.index, y=summary_mf[\"mean\"], error_y_array=summary_mf[\"std\"]),\n",
    "        go.Bar(x=summary_iaf.index, y=summary_iaf[\"mean\"], error_y_array=summary_iaf[\"std\"]),\n",
    "    ]\n",
    ")\n",
    "fig.update_yaxes(range=[0.95, 1.0])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of LFC errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_err(diff):\n",
    "    res = 0.5 * (diff ** 2) ** (0.5)\n",
    "    res = np.nanmean(res, axis=-1)\n",
    "    return res\n",
    "\n",
    "def l2_err_competitor(vals: np.ndarray, other: np.ndarray = None):\n",
    "    vals[np.isnan(vals)] = 0.0\n",
    "    if other is None:\n",
    "        diff = vals\n",
    "    else:\n",
    "        diff = vals - other\n",
    "    res = compute_l2_err(diff)\n",
    "    assert res.shape == (N_SIZES, N_PICKS)\n",
    "    data = []\n",
    "    for (size_ix, size) in enumerate(SIZES):\n",
    "        for pick in range(N_PICKS):\n",
    "            data.append(dict(experiment=pick, training=0, sample_size=size, error=res[size_ix, pick]))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "lfcs_errs_deseq2 = l2_err_competitor(other_predictions[\"deseq2\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"DESeq2\")\n",
    "lfcs_errs_edger = l2_err_competitor(other_predictions[\"edger\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"EdgeR\")\n",
    "lfcs_errs_mast = l2_err_competitor(other_predictions[\"mast\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"MAST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_l2_err(my_df):\n",
    "    diff = my_df.sort_values(\"gene\")[\"lfc_mean\"] - lfc_gt\n",
    "    error = 0.5 * (diff ** 2) ** (0.5)\n",
    "    error = np.nanmean(error)\n",
    "    return pd.Series(dict(error=error))\n",
    "\n",
    "lfcs_errs_mf = (\n",
    "    res_mf\n",
    "    .groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "lfcs_errs_iaf = (\n",
    "    res_iaf\n",
    "    .groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errs = pd.concat([\n",
    "    lfcs_errs_mf,\n",
    "    lfcs_errs_iaf,\n",
    "    lfcs_errs_deseq2,\n",
    "    lfcs_errs_edger,\n",
    "    lfcs_errs_mast,\n",
    "], ignore_index=True)\n",
    "\n",
    "px.box(all_errs, x=\"sample_size\", y=\"error\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((res_mf.hdi64_high - res_mf.hdi64_low).mean())\n",
    "print((res_iaf.hdi64_high - res_iaf.hdi64_low).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage(my_df, low_key=\"hdi64_low\", high_key=\"hdi64_high\"):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    assert len(my_df) == n_genes\n",
    "    gene_is_covered = (lfc_gt >= my_df[low_key]) & (lfc_gt <= my_df[high_key])\n",
    "#     mean_cov = (gene_is_covered / (my_df[high_key] - my_df[low_key])).mean()\n",
    "    mean_cov = (gene_is_covered).mean()\n",
    "    return pd.Series(dict(mean_cov=mean_cov))\n",
    "    \n",
    "\n",
    "coverage_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\", \"algorithm\"])\n",
    "    .apply(get_coverage, low_key=\"hdi64_low\", high_key=\"hdi64_high\")\n",
    "    .reset_index()\n",
    "#     .groupby(\"sample_size\")\n",
    "#     .agg(dict(mean_cov=[\"mean\", \"std\"]))\n",
    ")\n",
    "coverage_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\", \"algorithm\"])\n",
    "    .apply(get_coverage, low_key=\"hdi64_low\", high_key=\"hdi64_high\")\n",
    "    .reset_index()\n",
    "#     .groupby(\"sample_size\")\n",
    "#     .agg(dict(mean_cov=[\"mean\", \"std\"]))\n",
    ")\n",
    "\n",
    "all_coverages = pd.concat([coverage_mf, coverage_iaf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(all_coverages, x=\"sample_size\", y=\"mean_cov\", color=\"algorithm\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
