{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly as py\n",
    "import pandas as pd\n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from scvi.dataset import PowSimSynthetic, LatentLogPoissonDataset, SignedGamma, GeneExpressionDataset\n",
    "from scvi.models import VAE, IAVAE\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.utils import demultiply, make_dir_if_necessary, predict_de_genes, save_fig, load_pickle, save_pickle, has_lower_mean\n",
    "from scvi_utils import estimate_de_proba, estimate_lfc_density, estimate_lfc_mean, multi_train_estimates\n",
    "from R_interop import all_predictions, all_de_predictions\n",
    "\n",
    "\n",
    "N_EPOCHS = 200\n",
    "DELTA = 0.5\n",
    "SIZES = [5, 10, 20, 30, 50, 100]\n",
    "SIZE = 100\n",
    "N_SIZES = len(SIZES)\n",
    "DO_CLOUD = True\n",
    "Q0 = 5e-2\n",
    "N_TRAININGS = 5\n",
    "N_PICKS = 10\n",
    "n_genes = 1000\n",
    "\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "PATH_TO_SCRIPTS = \"/home/ubuntu/conquer_comparison/scripts\"\n",
    "DIR_PATH = 'lfc_estimates/lognormal'\n",
    "DF_PATH = \"/home/ubuntu/scVI/scvi/dataset/kolodziejczk_param.csv\"\n",
    "make_dir_if_necessary(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "py.sign_in(\"pierreboyeau\", \"2wvdnWZ2Qut1zD07ADVy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Constructing mu and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.read_csv(DF_PATH).sample(n_genes)\n",
    "means = selected[\"means\"].values\n",
    "\n",
    "means[means >= 1000] = 1000\n",
    "go.Figure([go.Histogram(x=means)]).show()\n",
    "\n",
    "lfc_sampler = SignedGamma(dim=2, proba_pos=0.5)\n",
    "lfcs = lfc_sampler.sample(n_genes).numpy()\n",
    "non_de_genes = np.random.choice(n_genes, size=300)\n",
    "lfcs[non_de_genes, :] = 0.0\n",
    "go.Figure([go.Histogram(x=lfcs[:, 0])]).show()\n",
    "\n",
    "log2_mu0 = lfcs[:, 0] + np.log2(means)\n",
    "log2_mu1 = lfcs[:, 1] + np.log2(means)\n",
    "\n",
    "loge_mu0 = log2_mu0 / np.log2(np.e)\n",
    "loge_mu1 = log2_mu1 / np.log2(np.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO\n",
    "a = (2.0 * np.random.random(size=(100, 1)) - 1).astype(float)\n",
    "sigma = 2.0*a.dot(a.T) + (1.0 + 0.5*(2.0*np.random.random(100)-1.0)) * np.eye(100)\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(z=sigma))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2.0 * np.random.random(size=(n_genes, 1)) - 1).astype(float)\n",
    "sigma = 2.0*a.dot(a.T) + 0.5*(1.0 + 0.5*(2.0*np.random.random(n_genes)-1.0)) * np.eye(n_genes)\n",
    "sigma0 = 0.1*sigma\n",
    "\n",
    "a = (2.0 * np.random.random(size=(n_genes, 1)) - 1).astype(float)\n",
    "sigma = 2.0*a.dot(a.T) + 0.5*(1.0 + 0.5*(2.0*np.random.random(n_genes)-1.0)) * np.eye(n_genes)\n",
    "sigma1 = 0.1*sigma\n",
    "\n",
    "# sigma1 = sigma\n",
    "\n",
    "# u, s, vh = np.linalg.svd(sigma)\n",
    "# perturbations = s.min() + (s.max() - s.min()) * np.random.random(len(s))\n",
    "# sigma1 = u @ (np.diag(perturbations)) @ vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.tensor(loge_mu0), covariance_matrix=torch.tensor(sigma0)\n",
    ").sample((5000,))\n",
    "h1 = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.tensor(loge_mu1), covariance_matrix=torch.tensor(sigma1)\n",
    ").sample((5000,))\n",
    "\n",
    "h = torch.cat([h0, h1])\n",
    "\n",
    "x_obs = torch.distributions.Poisson(rate=h.exp()).sample()\n",
    "# is_zi = np.random.random(x_obs.shape) >= 0.9\n",
    "is_zi = np.random.random(x_obs.shape) <= np.exp(-1.4 * x_obs.numpy())\n",
    "x_obs[is_zi] = 0.0\n",
    "labels = torch.zeros((10000, 1))\n",
    "labels[5000:] = 1\n",
    "\n",
    "not_null_cell = (x_obs.sum(1) != 0)\n",
    "x_obs = x_obs[not_null_cell]\n",
    "labels = labels[not_null_cell]\n",
    "\n",
    "trace1 = go.Histogram(x=x_obs.mean(0))\n",
    "fig = go.Figure(data=[trace1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(x_obs[:, 500], bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(DIR_PATH, \"dataset.pickle\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    dataset = GeneExpressionDataset()\n",
    "    dataset.populate_from_data(X=x_obs.numpy(), labels=labels.numpy())\n",
    "    dataset.lfc = lfcs\n",
    "    save_pickle(data=dataset, filename=dataset_path)\n",
    "else:\n",
    "    dataset = load_pickle(dataset_path)\n",
    "    lfcs = dataset.lfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_significant_de = np.abs(lfcs[:, 1] - lfcs[:, 0]) >= DELTA\n",
    "n_genes = dataset.nb_genes\n",
    "trace1 = go.Histogram(x=lfcs[:, 1] - lfcs[:, 0])\n",
    "fig = go.Figure(data=[trace1])\n",
    "# save_fig(fig, filename=\"powsimR_properties\", do_cloud=DO_CLOUD)\n",
    "# fig.show()\n",
    "iplot(fig, filename=\"lognormal_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = len(dataset)\n",
    "TEST_INDICES = np.random.permutation(n_examples)[:2000]\n",
    "\n",
    "x_test, y_test = dataset.X[TEST_INDICES, :], dataset.labels[TEST_INDICES, :].squeeze()\n",
    "data_path = os.path.join(DIR_PATH, 'data.npy')\n",
    "labels_path = os.path.join(DIR_PATH, 'labels.npy')\n",
    "\n",
    "np.save(\n",
    "    data_path,\n",
    "    x_test.squeeze().astype(int)\n",
    ")\n",
    "np.savetxt(\n",
    "    labels_path,\n",
    "    y_test.squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_params = dict(\n",
    "    iaf=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4, dropout_rate=0.2),\n",
    "    mf=dict(n_hidden=128, n_layers=1, n_latent=10, dropout_rate=0.2),\n",
    "    iaf_at=dict(n_hidden=128, n_layers=2, do_h=False, n_latent=12, t=3, dropout_rate=0.18),\n",
    "    mf_at=dict(n_hidden=128, n_layers=1, n_latent=5, dropout_rate=0.1),\n",
    "    iaf_k5=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4),\n",
    "    mf_k5=dict(n_hidden=128, n_layers=1, n_latent=10),\n",
    ")\n",
    "train_params = dict(\n",
    "    iaf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    iaf_b=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    mf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    iaf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5, single_backward=False),\n",
    "    mf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5, single_backward=False)\n",
    ")\n",
    "train_fn_params = dict(\n",
    "    iaf=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    iaf_b=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    mf=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    iaf_k5=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    mf_k5=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute competitors scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions = all_predictions(\n",
    "    filename=os.path.join(DIR_PATH, \"other_predictions1.pickle\"),\n",
    "    n_genes=n_genes, \n",
    "    n_picks=N_PICKS, \n",
    "    sizes=SIZES, \n",
    "    data_path=data_path, \n",
    "    labels_path=labels_path,\n",
    "    path_to_scripts=PATH_TO_SCRIPTS\n",
    ")\n",
    "\n",
    "other_predictions = all_de_predictions(\n",
    "    other_predictions, significance_level=Q0, delta=DELTA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sign of LFC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.utils import plot_identity\n",
    "\n",
    "lfc_gt = -(lfcs[:, 1] - lfcs[:, 0])\n",
    "plt.scatter(lfc_gt, other_predictions[\"edger\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(lfc_gt, other_predictions[\"deseq2\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(lfc_gt, other_predictions[\"mast\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"] = -other_predictions[\"edger\"][\"lfc\"]\n",
    "other_predictions[\"mast\"][\"lfc\"] = -other_predictions[\"mast\"][\"lfc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_mf_final1_high_lr_epochs.pickle\"),\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf.pickle\"),\n",
    "    mdl_class=VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    "    sizes=SIZES,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    label_b=1\n",
    ").assign(algorithm=\"MF\")\n",
    "\n",
    "res_iaf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_iaf_final1_high_lr_epochs.pickle\"),\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf.pickle\"),\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "    sizes=SIZES,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    label_b=1\n",
    ").assign(algorithm=\"IAF\")\n",
    "\n",
    "res_iafk5 = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_iafk5_final1_high_lr_epochs.pickle\"),\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "    sizes=SIZES,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    label_b=1\n",
    ").assign(algorithm=\"IAF K5\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# res_mf = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf_at.pickle\"),\n",
    "#     mdl_class=VAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"mf_at\"],\n",
    "#     train_params=train_params[\"mf\"],\n",
    "#     train_fn_params=train_fn_params[\"mf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1\n",
    "# ).assign(algorithm=\"MF\")\n",
    "\n",
    "# res_iaf = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf_at.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf_at\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1\n",
    "# ).assign(algorithm=\"IAF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR / Power Control and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    mdl_class, dataset, mdl_params: dict, train_params: dict, train_fn_params: dict\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param mdl_class: Class of algorithm\n",
    "    :param dataset: Dataset\n",
    "    :param mdl_params:\n",
    "    :param train_params:\n",
    "    :param train_fn_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    my_vae = mdl_class(dataset.nb_genes, n_batch=dataset.n_batches, **mdl_params)\n",
    "    my_trainer = UnsupervisedTrainer(my_vae, dataset, **train_params)\n",
    "    print(my_trainer.test_set.data_loader.sampler.indices)\n",
    "    my_trainer.train(**train_fn_params)\n",
    "    print(my_trainer.train_losses)\n",
    "    return my_vae, my_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### FDR and TPR Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Comparer flows avec MF pour les mÃªmes 5 cells et comparer PE FDR a FDR\n",
    "Montrer que FDR mieux estimer avec flows est super cool\n",
    "\n",
    "Dans papier, Ok d'utiliser deux decision rules. Dire que PE FDR overconservative ok\n",
    "Dire investigation futur papier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_5 = res_mf.loc[lambda x: (x.experiment == 0) & (x.training == 0) & (x.sample_size==5), \"de_proba\"].values\n",
    "probas_100 = res_mf.loc[lambda x: (x.experiment == 0) & (x.training == 0) & (x.sample_size==100), \"de_proba\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_genes = np.argsort(-probas_5)\n",
    "sorted_pgs = probas_5[sorted_genes]\n",
    "cumulative_fdr_5 = (1.0 - sorted_pgs).cumsum() / (1.0 + np.arange(len(sorted_pgs)))\n",
    "\n",
    "d = (cumulative_fdr_5 <= 5e-2).sum() - 1\n",
    "print(d, cumulative_fdr_5[d])\n",
    "\n",
    "fdr_k = []\n",
    "for k in range(n_genes):\n",
    "    predictions_5 = np.zeros(n_genes)\n",
    "    predictions_5[sorted_genes[:(k+1)]] = 1\n",
    "    fdr = ((~is_significant_de) * predictions_5).sum() / (k+1)\n",
    "    fdr_k.append(fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_pgs[:0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_genes = np.argsort(-probas_100)\n",
    "sorted_genes = np.random.permutation(n_genes)\n",
    "sorted_pgs = probas_100[sorted_genes]\n",
    "cumulative_fdr_100 = (1.0 - sorted_pgs).cumsum() / (1.0 + np.arange(len(sorted_pgs)))\n",
    "\n",
    "d = (cumulative_fdr_100 <= 5e-2).sum() - 1\n",
    "print(d, cumulative_fdr_100[d])\n",
    "\n",
    "fdr_k_100 = []\n",
    "for k in range(n_genes):\n",
    "    predictions_100 = np.zeros(n_genes)\n",
    "    predictions_100[sorted_genes[:(k+1)]] = 1\n",
    "    fdr = ((~is_significant_de) * predictions_100).sum() / (k+1)\n",
    "    fdr_k_100.append(fdr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_fdr_5, label=\"PE FDR\")\n",
    "plt.plot(fdr_k, label=\"True 5\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_fdr_100, label=\"PE FDR\")\n",
    "plt.plot(fdr_k_100, label=\"True 100\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fdr_k, label=\"True 5\")\n",
    "plt.plot(fdr_k_100, label=\"True 100\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cumulative_fdr_5, label=\"5\")\n",
    "plt.plot(cumulative_fdr_100, label=\"100\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.loc[lambda x: x.experiment == 0 & x.training == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_fnr(my_df):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    assert len(my_df) == n_genes\n",
    "    is_pred_de = predict_de_genes(my_df.de_proba.values, desired_fdr=Q0)\n",
    "    \n",
    "    alpha = my_df.de_proba.values[is_pred_de].min()\n",
    "#     alpha = 0.8\n",
    "#     is_pred_de = my_df.de_proba.values >= 0.4\n",
    "    true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "    n_positives = is_significant_de.sum()\n",
    "    true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "    return pd.Series(dict(fdr=true_fdr, fnr=true_fnr, alpha=alpha))\n",
    "\n",
    "\n",
    "fdr_fnr_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "fdr_fnr_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "\n",
    "df = pd.concat([fdr_fnr_mf, fdr_fnr_iaf], ignore_index=True)\n",
    "\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fdr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Discovery Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_fdr_control\")\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fnr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Negative Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_power_control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_fnr_iaf.groupby(\"sample_size\").alpha.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_fnr_mf.groupby(\"sample_size\").alpha.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['deseq2', 'edger', 'mast']\n",
    "\n",
    "def get_fdr_fnr(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        y_pred: (n_sz, n_picks, n_genes) bool predictions\n",
    "        y_true: (n_genes) gt vals\n",
    "    \"\"\"\n",
    "    n_sz, n_picks, _ = y_pred.shape\n",
    "    fnrs = np.zeros((n_sz, n_picks))\n",
    "    fdrs = np.zeros((n_sz, n_picks))\n",
    "    for sz in range(n_sz):\n",
    "        for pick in range(n_picks):\n",
    "            y_pred_it = y_pred[sz, pick, :]\n",
    "            fnr = ((~y_true) * y_pred_it).sum() / y_pred_it.sum()\n",
    "            fdr = (y_true * (~y_pred_it)).sum() / y_true.sum()\n",
    "            fnrs[sz, pick] = fnr\n",
    "            fdrs[sz, pick] = fdr\n",
    "    fnrs[np.isnan(fnrs)] = 0.0\n",
    "    return dict(fnr=fnrs, fdr=fdrs)\n",
    "\n",
    "print(other_predictions[\"mast\"]['pval'].shape)\n",
    "print(other_predictions[\"deseq2\"]['pval'].shape)\n",
    "print(other_predictions[\"edger\"]['pval'].shape)\n",
    "\n",
    "is_de_mast = other_predictions[\"mast\"][\"is_de\"]\n",
    "is_de_deseq2 = other_predictions[\"deseq2\"][\"is_de\"]\n",
    "is_de_edger = other_predictions[\"edger\"][\"is_de\"]\n",
    "# is_de_edgerr = other_predictions[\"edger_robust\"][\"is_de\"]\n",
    "\n",
    "\n",
    "res_mast = get_fdr_fnr(is_de_mast, y_true=is_significant_de)\n",
    "res_deseq2 = get_fdr_fnr(is_de_deseq2, y_true=is_significant_de)\n",
    "res_edger = get_fdr_fnr(is_de_edger, y_true=is_significant_de)\n",
    "# res_edgerr = get_fdr_fnr(is_de_edgerr, y_true=is_significant_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf = res_mf[(res_mf.experiment == 0) & (res_mf.training == 0) & (res_mf.sample_size == 100)]\n",
    "preds_iaf = res_iaf[(res_iaf.experiment == 0) & (res_iaf.training == 0) & (res_iaf.sample_size == 100)]\n",
    "\n",
    "# preds_mf = preds_mf.sort_values(\"de_proba\").set_index(\"gene\")\n",
    "# preds_iaf = preds_iaf.set_index(\"gene\").reindex(index=preds_mf.index)\n",
    "# preds_iaf[]\n",
    "\n",
    "preds = pd.concat([preds_mf, preds_iaf], ignore_index=True)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf = preds_mf.assign(\n",
    "    de_proba_iaf=preds_iaf.de_proba,\n",
    "    gene_mean=dataset.X.mean(0),\n",
    "    is_de=is_significant_de.astype(float),\n",
    ")\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [preds_mf[\"de_proba\"], preds_mf[\"de_proba_iaf\"]],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [preds_mf[\"de_proba\"], preds_mf[\"de_proba_iaf\"]],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algos_comparison(my_df, key1, other_keys, key_values=\"error\"):\n",
    "    vals_key1 = my_df.loc[my_df[\"algorithm\"]==key1, key_values].values\n",
    "    algo1_is_better = True\n",
    "    for key2 in other_keys:\n",
    "        vals_other = my_df.loc[my_df[\"algorithm\"] == key2, key_values].values\n",
    "        try:\n",
    "            key1_better = has_lower_mean(vals_key1, vals_other)\n",
    "        except ValueError:\n",
    "            key1_better = False\n",
    "            break\n",
    "        if not key1_better:\n",
    "            algo1_is_better = False\n",
    "            break\n",
    "    return key1_better\n",
    "\n",
    "\n",
    "gped = df.groupby(\"sample_size\")\n",
    "fdr_mf_better = gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"IAF\"], key_values=\"fdr\")\n",
    "fdr_iaf_better = gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"MF\"], key_values=\"fdr\")\n",
    "\n",
    "fnr_mf_better = gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"IAF\"], key_values=\"fnr\")\n",
    "fnr_iaf_better = gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"MF\"], key_values=\"fnr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table = df.groupby([\"sample_size\", \"algorithm\"])[\"fdr\", \"fnr\"].mean().round(3).reset_index()\n",
    "\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"fdr_better\"] = fdr_mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"fdr_better\"] = fdr_iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"fnr_better\"] = fnr_mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"fnr_better\"] = fnr_iaf_better.values\n",
    "\n",
    "res_table.loc[res_table[\"fdr_better\"], \"fdr\"] = res_table.loc[\n",
    "    res_table[\"fdr_better\"], \"fdr\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[res_table[\"fnr_better\"], \"fnr\"] = res_table.loc[\n",
    "    res_table[\"fnr_better\"], \"fnr\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[:, \"fdr\"] = res_table.loc[:, \"fdr\"].apply(lambda x: \"$ {} $\".format(x))\n",
    "res_table.loc[:, \"fnr\"] = res_table.loc[:, \"fnr\"].apply(lambda x: \"$ {} $\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.pivot(\n",
    "    index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.loc[lambda x: x[\"sample_size\"].isin([5, 20, 100])].pivot(\n",
    "    index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    res_table.loc[lambda x: x[\"sample_size\"].isin([5, 20, 100])]\n",
    "    .pivot(index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"])\n",
    "    .T\n",
    "    .to_latex(escape=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"fdr\").loc[\n",
    "    :, [5, 20, 100]\n",
    "].to_latex(escape=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"fnr\").loc[:, [5, 20, 100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Understand why issue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to understand why FDR not properly estimated with many cells"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf.sample_size.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [\n",
    "        res_mf.loc[lambda x: (x.sample_size==5) & (x.training==0),\"de_proba\"], \n",
    "        res_iaf.loc[lambda x: (x.sample_size==5) & (x.training==0),\"de_proba\"]\n",
    "    ],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [\n",
    "        preds_mf.loc[lambda x: x.sample_size==100,\"de_proba\"], \n",
    "        preds_mf.loc[lambda x: x.sample_size==100,\"de_proba_iaf\"]\n",
    "    ],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trains_res = all_fdrs.mean(axis=1)\n",
    "# print(trains_res.mean(), trains_res.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_preds_1d = y_preds.reshape((-1, dataset.nb_genes))\n",
    "# n_exps = len(y_preds_1d)\n",
    "# confs = np.zeros((n_exps, 2, 2))\n",
    "# for i in range(n_exps):\n",
    "#     confs[i, :, :] = confusion_matrix(is_significant_de, y_preds_1d[i, :])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# confusion_matrix(is_significant_de, y_preds_1d[0, :])\n",
    "\n",
    "# confs_mean = confs.mean(0)\n",
    "# confs_mean\n",
    "\n",
    "# fig = ff.create_annotated_heatmap(\n",
    "#     z=confs_mean, x=[\"Pred Negative\", \"Pred Positive\"], y=[\"GT Negative\", \"GT Positive\"]\n",
    "# )\n",
    "# fig.update({\"layout\": dict(title=\"Confusion Matrix\")})\n",
    "\n",
    "# py.iplot(fig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 2\n",
    "\n",
    "preds_md = res_mf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "preds_iaf = res_iaf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "preds_iafk5 = res_iafk5.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "preds_deseq2 = 1.0 - other_predictions['deseq2']['pval'][-1, 0, :]\n",
    "preds_edger = 1.0 - other_predictions['edger']['pval'][-1, 0, :]\n",
    "preds_mast = 1.0 - other_predictions['mast']['pval'][-1, 0, :]\n",
    "\n",
    "# preds_deseq2 = 1.0 - other_predictions['deseq2']['pval'][:]\n",
    "# preds_edger = 1.0 - other_predictions['edger']['pval'][:]\n",
    "# preds_mast = 1.0 - other_predictions['mast']['pval'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(preds_md).mean())\n",
    "print(np.isnan(preds_iaf).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_edger).mean())\n",
    "print(np.isnan(preds_mast).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_pr(fig, preds, y_true, name):\n",
    "    average_precision = average_precision_score(y_true, preds)\n",
    "    preds[np.isnan(preds)] = np.min(preds[~np.isnan(preds)])\n",
    "    precs, recs, _ = precision_recall_curve(y_true=y_true, probas_pred=preds)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=recs,\n",
    "            y=precs,\n",
    "            name=name+'@AP: {0:0.2f}'.format(average_precision)\n",
    "        )\n",
    "    )\n",
    "    return\n",
    "layout = go.Layout(\n",
    "    title='Precision Recall Curves',\n",
    "    xaxis=dict(title='Recall'),\n",
    "    yaxis=dict(title='Precision'),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "plot_pr(fig=fig, preds=preds_md, y_true=is_significant_de, name='MF')\n",
    "plot_pr(fig=fig, preds=preds_iaf, y_true=is_significant_de, name='IAF')\n",
    "plot_pr(fig=fig, preds=preds_iafk5, y_true=is_significant_de, name='IAFK5')\n",
    "\n",
    "plot_pr(fig=fig, preds=preds_deseq2, y_true=is_significant_de, name='DESeq2')\n",
    "plot_pr(fig=fig, preds=preds_edger, y_true=is_significant_de, name='EdgeR')\n",
    "# plot_pr(fig=fig, preds=preds_edgerr, y_true=is_significant_de, name='EdgeR Robust')\n",
    "plot_pr(fig=fig, preds=preds_mast, y_true=is_significant_de, name='MAST')\n",
    "\n",
    "# fig.show()\n",
    "iplot(fig, filename=\"lognormal_pr_curves2\", sharing=\"private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ap(my_df):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    average_precision = average_precision_score(is_significant_de, my_df.de_proba)\n",
    "    return pd.Series(dict(AP=average_precision))\n",
    "\n",
    "\n",
    "ap_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "ap_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "\n",
    "ap_iafk5 = (\n",
    "    res_iafk5.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF K5\")\n",
    ")\n",
    "\n",
    "all_ap = pd.concat([ap_mf, ap_iaf, ap_iafk5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(all_ap, x=\"sample_size\", y=\"AP\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ap.groupby([\"algorithm\", \"sample_size\"]).agg(dict(AP=[\"mean\", \"std\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc_gt = -(lfcs[:, 1] - lfcs[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 0\n",
    "subsample_genes = np.sort(np.random.permutation(n_genes)[:150])\n",
    "\n",
    "lfcs_mf = (\n",
    "    res_mf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        #         err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        #         err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_ia = (\n",
    "    res_iaf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        #         err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        #         err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_iak5 = (\n",
    "    res_iafk5.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "all_lfcs = pd.concat([lfcs_mf, lfcs_ia, lfcs_iak5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    all_lfcs,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    color=\"algorithm\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "# fig.show()\n",
    "iplot(fig, sharing=\"private\", filename=\"logpoisson_diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 against 100\n",
    "\n",
    "lfcs_a = (\n",
    "    res_iaf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 5)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "        legend=\"5\"\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_b= (\n",
    "    res_iaf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "        legend=\"100\"\n",
    "    )\n",
    ")\n",
    "\n",
    "all_lfcs = pd.concat([lfcs_a, lfcs_b], ignore_index=True)\n",
    "\n",
    "fig = px.scatter(\n",
    "    all_lfcs,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    color=\"legend\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "        showlegend=False\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIDENCES = [50, 75, 95, 64, 99]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_inside(my_df):\n",
    "    index = []\n",
    "    values = []\n",
    "    for confidence in [50, 75, 95, 64, 99]:\n",
    "        is_in_hdi = (lfc_gt <= my_df[\"hdi{}_high\".format(confidence)]) & (\n",
    "            lfc_gt >= my_df[\"hdi{}_low\".format(confidence)]\n",
    "        )\n",
    "        values.append((confidence / 100.0 - is_in_hdi.mean()) ** 2)\n",
    "        index.append(\"error{}\".format(confidence))\n",
    "    return pd.Series(values, index=index)\n",
    "\n",
    "\n",
    "errs_mf = (\n",
    "    res_mf.groupby(by=[\"experiment\", \"sample_size\", \"training\"])\n",
    "    .apply(frac_inside)\n",
    "    .reset_index()\n",
    ").assign(algorithm=\"Mean Field\")\n",
    "\n",
    "errs_iaf = (\n",
    "    res_iaf.groupby(by=[\"experiment\", \"sample_size\", \"training\"])\n",
    "    .apply(frac_inside)\n",
    "    .reset_index()\n",
    ").assign(algorithm=\"IAF\")\n",
    "\n",
    "display(errs_mf)\n",
    "display(errs_iaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(my_df):\n",
    "    errors = [\"error{}\".format(err) for err in CONFIDENCES]\n",
    "    err_scores = my_df.loc[:, errors].sum(1)\n",
    "    err_scores_mf = err_scores[my_df[\"algorithm\"] == \"Mean Field\"].values\n",
    "    err_scores_iaf = err_scores[my_df[\"algorithm\"] == \"IAF\"].values\n",
    "    \n",
    "    disp_mf = str(err_scores_mf.mean().round(3))\n",
    "    disp_iaf = str(err_scores_iaf.mean().round(3))\n",
    "#     print(err_scores_mf)\n",
    "    if has_lower_mean(err_scores_mf, err_scores_iaf):\n",
    "        disp_mf = \"\\mathbf{{ {} }}\".format(disp_mf)\n",
    "    if has_lower_mean(err_scores_iaf, err_scores_mf):\n",
    "        disp_iaf = \"\\mathbf{{ {} }}\".format(disp_iaf)\n",
    "    \n",
    "    return pd.Series(\n",
    "        dict(\n",
    "            IAF=disp_mf, \n",
    "            MF=disp_iaf\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "errs = pd.concat([errs_mf, errs_iaf], ignore_index=True)\n",
    "errs.groupby(\"sample_size\").apply(scoring)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of LFC errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_err(diff):\n",
    "    res = 0.5 * (diff ** 2) ** (0.5)\n",
    "    res = np.nanmean(res, axis=-1)\n",
    "    return res\n",
    "\n",
    "def l2_err_competitor(vals: np.ndarray, other: np.ndarray = None):\n",
    "    vals[np.isnan(vals)] = 0.0\n",
    "    if other is None:\n",
    "        diff = vals\n",
    "    else:\n",
    "        diff = vals - other\n",
    "    res = compute_l2_err(diff)\n",
    "    assert res.shape == (N_SIZES, N_PICKS)\n",
    "    data = []\n",
    "    for (size_ix, size) in enumerate(SIZES):\n",
    "        for pick in range(N_PICKS):\n",
    "            data.append(dict(experiment=pick, training=0, sample_size=size, error=res[size_ix, pick]))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "lfcs_errs_deseq2 = l2_err_competitor(other_predictions[\"deseq2\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"DESeq2\")\n",
    "lfcs_errs_edger = l2_err_competitor(other_predictions[\"edger\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"EdgeR\")\n",
    "lfcs_errs_mast = l2_err_competitor(other_predictions[\"mast\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"MAST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfcs_errs_mast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_l2_err(my_df):\n",
    "    diff = my_df.sort_values(\"gene\")[\"lfc_mean\"] - lfc_gt\n",
    "    error = 0.5 * (diff ** 2) ** (0.5)\n",
    "    error = np.nanmean(error)\n",
    "    return pd.Series(dict(error=error))\n",
    "\n",
    "lfcs_errs_mf = (\n",
    "    res_mf\n",
    "    .groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "lfcs_errs_iaf = (\n",
    "    res_iaf\n",
    "    .groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errs = pd.concat([\n",
    "    lfcs_errs_mf,\n",
    "    lfcs_errs_iaf,\n",
    "    lfcs_errs_deseq2,\n",
    "    lfcs_errs_edger,\n",
    "    lfcs_errs_mast,\n",
    "], ignore_index=True)\n",
    "\n",
    "px.box(all_errs, x=\"sample_size\", y=\"error\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algos_comparison(my_df, key1, other_keys):\n",
    "    vals_key1 = my_df.loc[my_df[\"algorithm\"]==key1, \"error\"].values\n",
    "    algo1_is_better = True\n",
    "    for key2 in other_keys:\n",
    "        vals_other = my_df.loc[my_df[\"algorithm\"] == key2, \"error\"].values\n",
    "        key1_better = has_lower_mean(vals_key1, vals_other)\n",
    "        if not key1_better:\n",
    "            algo1_is_better = False\n",
    "            break\n",
    "    return key1_better\n",
    "\n",
    "gped = all_errs.groupby(\"sample_size\")\n",
    "mf_or_iaf_better = (\n",
    "    gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"DESeq2\", \"EdgeR\", \"MAST\"]) &\n",
    "    gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"DESeq2\", \"EdgeR\", \"MAST\"])\n",
    ")\n",
    "mf_better = gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"IAF\", \"DESeq2\", \"EdgeR\", \"MAST\"])\n",
    "iaf_better = gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"MF\", \"DESeq2\", \"EdgeR\", \"MAST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table = (\n",
    "    all_errs.groupby([\"sample_size\", \"algorithm\"])\n",
    "    .error.agg(dict(err_mean=\"mean\", err_std=\"std\"))\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        displayed=lambda x: x.apply(\n",
    "#             lambda y: \"{:.3f} \\pm {:.3f}\".format(y.err_mean, y.err_std), axis=1\n",
    "            lambda y: \"{:.3f}\".format(y.err_mean), axis=1\n",
    "\n",
    "        ),\n",
    "        is_better=False,\n",
    "        one_of_best=False,\n",
    "    )\n",
    ")\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"is_better\"] = mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"is_better\"] = iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"one_of_best\"] = mf_or_iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"one_of_best\"] = mf_or_iaf_better.values\n",
    "\n",
    "\n",
    "res_table.loc[lambda x: x.one_of_best, \"displayed\"] = (\n",
    "    res_table.loc[lambda x: x.one_of_best, \"displayed\"] + \"^*\"\n",
    ")\n",
    "res_table.loc[lambda x: x.is_better, \"displayed\"] = res_table.loc[\n",
    "    lambda x: x.is_better, \"displayed\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[:, \"displayed\"] = res_table.loc[:, \"displayed\"].apply(lambda x: \"$ {} $\".format(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"displayed\").loc[\n",
    "    [\"DESeq2\", \"EdgeR\", \"MAST\", \"MF\", \"IAF\"],\n",
    "#     :\n",
    "    [5, 20, 100]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"displayed\")\n",
    "    .loc[\n",
    "        [\"DESeq2\", \"EdgeR\", \"MAST\", \"MF\", \"IAF\"],\n",
    "        #     :\n",
    "        [5, 20, 100],\n",
    "    ]\n",
    "    .to_latex(escape=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if poorer performance of models for important number of cells is linked to mixing factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((res_mf.hdi64_high - res_mf.hdi64_low).mean())\n",
    "print((res_iaf.hdi64_high - res_iaf.hdi64_low).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage(my_df, low_key=\"hdi64_low\", high_key=\"hdi64_high\"):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    assert len(my_df) == n_genes\n",
    "    gene_is_covered = (lfc_gt >= my_df[low_key]) & (lfc_gt <= my_df[high_key])\n",
    "#     mean_cov = (gene_is_covered / (my_df[high_key] - my_df[low_key])).mean()\n",
    "    mean_cov = (gene_is_covered).mean()\n",
    "    return pd.Series(dict(mean_cov=mean_cov))\n",
    "    \n",
    "\n",
    "coverage_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\", \"algorithm\"])\n",
    "    .apply(get_coverage, low_key=\"hdi64_low\", high_key=\"hdi64_high\")\n",
    "    .reset_index()\n",
    "#     .groupby(\"sample_size\")\n",
    "#     .agg(dict(mean_cov=[\"mean\", \"std\"]))\n",
    ")\n",
    "coverage_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\", \"algorithm\"])\n",
    "    .apply(get_coverage, low_key=\"hdi64_low\", high_key=\"hdi64_high\")\n",
    "    .reset_index()\n",
    "#     .groupby(\"sample_size\")\n",
    "#     .agg(dict(mean_cov=[\"mean\", \"std\"]))\n",
    ")\n",
    "\n",
    "all_coverages = pd.concat([coverage_mf, coverage_iaf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_coverages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(all_coverages, x=\"sample_size\", y=\"mean_cov\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FNR inconsistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "\n",
    "Problem linked to the fact that when you condition on less samples, the posterior LFC is sharper\n",
    "\n",
    "I see two solutions:\n",
    "- Voting stategy when you have many samples\n",
    "- Modification of the decision rule\n",
    "- Use posterior predicted\n",
    "- dataset is too easy ==> Add complexity\n",
    "- base decision making on credible intervals as previously\n",
    "- 3 ways classification: Upregulated, downregulated, non DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi_utils import train_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl, trainer = train_model(\n",
    "#     mdl_class=VAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"mf\"],\n",
    "#     train_params=train_params[\"mf\"],\n",
    "#     train_fn_params=train_fn_params[\"mf\"],\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_idx = 200\n",
    "true_lfc = lfc_gt[gene_idx]\n",
    "print(true_lfc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "# z, labels, scales = trainer.test_set.get_latents(\n",
    "#     n_samples=50, other=True, device=\"cpu\"\n",
    "# )\n",
    "\n",
    "# labels = labels.squeeze()\n",
    "# where_a = np.where(labels == 0)[0]\n",
    "# where_b = np.where(labels == 1)[0]\n",
    "# where_a = where_a[np.random.choice(len(where_a), size=100)]\n",
    "# where_b = where_b[np.random.choice(len(where_b), size=100)]\n",
    "# scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "# lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "\n",
    "# de_probas = (np.abs(lfc) >= 0.5).mean(0)\n",
    "# is_pred_de = predict_de_genes(de_probas, desired_fdr=Q0)\n",
    "# alpha = is_pred_de[is_pred_de].min()\n",
    "\n",
    "# true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "# n_positives = is_significant_de.sum()\n",
    "# true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "# print(true_fdr, true_fnr)\n",
    "\n",
    "# plt.hist(lfc[:, gene_idx])\n",
    "# plt.axvline(x=true_lfc, color=\"black\")\n",
    "# plt.title(de_probas[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, labels, scales = trainer.test_set.get_latents(\n",
    "    n_samples=500, other=True, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=100)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=100)]\n",
    "scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "\n",
    "de_probas = (np.abs(lfc) >= 0.5).mean(0)\n",
    "de_probas_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "\n",
    "# is_pred_de = predict_de_genes(de_probas, desired_fdr=Q0)\n",
    "is_pred_de = de_probas >= 0.5\n",
    "# probas_thresh = -np.sort(-de_probas)[215]\n",
    "# is_pred_de = de_probas >= probas_thresh\n",
    "\n",
    "alpha = is_pred_de[is_pred_de].min()\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)\n",
    "\n",
    "plt.hist(lfc[:, gene_idx])\n",
    "plt.axvline(x=true_lfc, color=\"black\")\n",
    "plt.title(de_probas[gene_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RÃ©sultats pour decision >= 0.5\n",
    "\n",
    "**100 cells**\n",
    "0.23706896551724138 0.019390581717451522\n",
    "\n",
    "0.2886178861788618 0.030470914127423823\n",
    "\n",
    "\n",
    "\n",
    "**5 cellules**\n",
    "0.42448979591836733 0.2188365650969529\n",
    "\n",
    "0.12435233160621761 0.06371191135734072\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision making based on credible intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = np.percentile(lfc, q=[2.5, 97.5], axis=0)\n",
    "\n",
    "is_pred_de = (np.abs(low) >= 0.5) & (np.abs(high) >= 0.5) & (low * high >= 0.0)\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "# n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Option 1\n",
    "# z, labels, scales = trainer.test_set.get_latents(\n",
    "#     n_samples=50, other=True, device=\"cpu\"\n",
    "# )\n",
    "\n",
    "# labels = labels.squeeze()\n",
    "# where_a = np.where(labels == 0)[0]\n",
    "# where_b = np.where(labels == 1)[0]\n",
    "# where_a = where_a[np.random.choice(len(where_a), size=5)]\n",
    "# where_b = where_b[np.random.choice(len(where_b), size=5)]\n",
    "# scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "# lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "# de_probas_small = (np.abs(lfc) >= 0.5).mean(0)\n",
    "\n",
    "\n",
    "# is_pred_de = predict_de_genes(de_probas_small, desired_fdr=Q0)\n",
    "# alpha = is_pred_de[is_pred_de].min()\n",
    "# true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "# n_positives = is_significant_de.sum()\n",
    "# true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "# print(true_fdr, true_fnr)\n",
    "\n",
    "# plt.hist(lfc[:, gene_idx])\n",
    "# plt.axvline(x=true_lfc, color=\"black\")\n",
    "# plt.title(de_probas_small[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "# z, labels, scales = trainer.test_set.get_latents(\n",
    "#     n_samples=1000, other=True, device=\"cpu\"\n",
    "# )\n",
    "\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=5)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=5)]\n",
    "scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "de_probas_small = (np.abs(lfc) >= 0.5).mean(0)\n",
    "de_probas_small_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "# is_pred_de_small = predict_de_genes(de_probas_small, desired_fdr=Q0)\n",
    "is_pred_de_small = de_probas_small >= 0.5\n",
    "alpha = is_pred_de_small[is_pred_de_small].min()\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de_small).sum() / is_pred_de_small.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de_small)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)\n",
    "\n",
    "plt.hist(lfc[:, gene_idx])\n",
    "plt.axvline(x=true_lfc, color=\"black\")\n",
    "plt.title(de_probas_small[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = np.percentile(lfc, q=[2.5, 97.5], axis=0)\n",
    "\n",
    "is_pred_de = (np.abs(low) >= 0.5) & (np.abs(high) >= 0.5) & (low * high >= 0.0)\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "z, labels, scales = trainer.test_set.get_latents(\n",
    "    n_samples=2000, other=True, device=\"cpu\"\n",
    ")\n",
    "\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=1)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=1)]\n",
    "scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=10)\n",
    "lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "de_probas_small = (np.abs(lfc) >= 0.5).mean(0)\n",
    "de_probas_small_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "is_pred_de_small = predict_de_genes(de_probas_small, desired_fdr=Q0)\n",
    "# is_pred_de_small = de_probas_small >= 0.5\n",
    "alpha = is_pred_de_small[is_pred_de_small].min()\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de_small).sum() / is_pred_de_small.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de_small)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)\n",
    "\n",
    "plt.hist(lfc[:, gene_idx])\n",
    "plt.axvline(x=true_lfc, color=\"black\")\n",
    "plt.title(de_probas_small[gene_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_pred_de.sum())\n",
    "print(is_pred_de_small.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Empirical distribution of predicted probabilities of being DE\")\n",
    "\n",
    "plt.hist(de_probas_small, alpha=0.25, label=\"5 cells\")\n",
    "plt.hist(de_probas, alpha=0.25, label=\"100 cells\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(de_probas_std, alpha=0.25)\n",
    "plt.hist(de_probas_small_std, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_votes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, labels, scales = trainer.test_set.get_latents(\n",
    "    n_samples=2000, other=True, device=\"cpu\"\n",
    ")\n",
    "\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=100)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=100)]\n",
    "scales_a_all = scales[:, where_a, :].numpy()\n",
    "scales_b_all = scales[:, where_b, :].numpy()\n",
    "\n",
    "all_votes = np.zeros((n_votes, n_genes))\n",
    "for vote in tqdm(range(n_votes)):\n",
    "    where_a = np.random.choice(100, size=1)\n",
    "    where_b = np.random.choice(100, size=1)\n",
    "    scales_a = scales_a_all[:, where_a, :].reshape((-1, n_genes))\n",
    "    scales_b = scales_b_all[:, where_b, :].reshape((-1, n_genes))\n",
    "\n",
    "    scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "    lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "\n",
    "    de_probas = (np.abs(lfc) >= 0.5).mean(0)\n",
    "    de_probas_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "\n",
    "    is_pred_de = predict_de_genes(de_probas, desired_fdr=Q0)\n",
    "    \n",
    "    all_votes[vote, :] = is_pred_de\n",
    "    \n",
    "#     de_probas_small_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "#     alpha = is_pred_de[is_pred_de].min()\n",
    "\n",
    "#     true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "#     n_positives = is_significant_de.sum()\n",
    "#     true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "#     print(true_fdr, true_fnr)\n",
    "\n",
    "# plt.hist(lfc[:, gene_idx])\n",
    "# plt.axvline(x=true_lfc, color=\"black\")\n",
    "# plt.title(de_probas[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pred_de_vote = all_votes.mean(0) >= 0.5\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de_vote).sum() / is_pred_de_vote.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de_vote)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_votes.mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credible intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_iaf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnr_fdr(my_df):\n",
    "    my_is_pred_de = my_df.is_pred_de\n",
    "    true_fdr = ((1.0 - is_significant_de) * my_is_pred_de).sum() / my_is_pred_de.sum()\n",
    "    n_positives = is_significant_de.sum()\n",
    "    true_fnr = (is_significant_de * (1.0 - my_is_pred_de)).sum() / n_positives\n",
    "    return pd.Series(dict(fdr=true_fdr, fnr=true_fnr))\n",
    "\n",
    "(\n",
    "    res_iaf\n",
    "    .assign(is_pred_de=lambda x: (x.hdi64_low.abs() >= 0.5) \n",
    "            &  (x.hdi64_high.abs() >= 0.5) \n",
    "            & (x.hdi64_low * x.hdi64_high >= 0.0))\n",
    "    .groupby([\"training\", \"algorithm\", \"sample_size\", \"experiment\"])\n",
    "    .apply(fnr_fdr)\n",
    "    .reset_index()\n",
    "    .groupby([\"sample_size\"])\n",
    "    .agg(dict(fdr=[\"mean\", \"std\"], fnr=[\"mean\", \"std\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "759px",
    "left": "77px",
    "top": "110px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
