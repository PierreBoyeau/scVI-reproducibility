{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import torch\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly as py\n",
    "import pandas as pd\n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from scvi.dataset import (\n",
    "    PowSimSynthetic,\n",
    "    LatentLogPoissonDataset,\n",
    "    SignedGamma,\n",
    "    GeneExpressionDataset,\n",
    ")\n",
    "from scvi.models import VAE, IAVAE\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.utils import (\n",
    "    demultiply,\n",
    "    make_dir_if_necessary,\n",
    "    predict_de_genes,\n",
    "    save_fig,\n",
    "    load_pickle,\n",
    "    save_pickle,\n",
    "    has_lower_mean,\n",
    "    softmax,\n",
    "    compute_hdi\n",
    ")\n",
    "from scvi_utils import (\n",
    "    estimate_de_proba,\n",
    "    estimate_lfc_density,\n",
    "    estimate_lfc_mean,\n",
    "    multi_train_estimates,\n",
    "    train_model\n",
    ")\n",
    "from R_interop import all_predictions, all_de_predictions\n",
    "\n",
    "\n",
    "N_EPOCHS = 200\n",
    "DELTA = 0.5\n",
    "SIZES = [5, 10, 20, 30, 50, 100]\n",
    "SIZE = 100\n",
    "N_SIZES = len(SIZES)\n",
    "DO_CLOUD = True\n",
    "Q0 = 5e-2\n",
    "N_TRAININGS = 5\n",
    "N_PICKS = 10\n",
    "n_genes = 1000\n",
    "FREQUENCY = 1\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "PATH_TO_SCRIPTS = \"/home/ubuntu/conquer_comparison/scripts\"\n",
    "DIR_PATH = \"lfc_estimates/lognormal2\"\n",
    "DF_PATH = \"/home/ubuntu/scVI/scvi/dataset/kolodziejczk_param.csv\"\n",
    "make_dir_if_necessary(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    import chart_studio.plotly as py\n",
    "\n",
    "py.sign_in(\"pierreboyeau\", \"2wvdnWZ2Qut1zD07ADVy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##Â Constructing mu and sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = pd.read_csv(DF_PATH).sample(n_genes)\n",
    "means = selected[\"means\"].values\n",
    "\n",
    "means[means >= 1000] = 1000\n",
    "go.Figure([go.Histogram(x=means)]).show()\n",
    "\n",
    "lfc_sampler = SignedGamma(dim=2, proba_pos=0.5)\n",
    "lfcs = lfc_sampler.sample(n_genes).numpy()\n",
    "non_de_genes = np.random.choice(n_genes, size=300)\n",
    "lfcs[non_de_genes, :] = 0.0\n",
    "go.Figure([go.Histogram(x=lfcs[:, 0])]).show()\n",
    "\n",
    "log2_mu0 = lfcs[:, 0] + np.log2(means)\n",
    "log2_mu1 = lfcs[:, 1] + np.log2(means)\n",
    "\n",
    "loge_mu0 = log2_mu0 / np.log2(np.e)\n",
    "loge_mu1 = log2_mu1 / np.log2(np.e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEMO\n",
    "a = (2.0 * np.random.random(size=(100, 1)) - 1).astype(float)\n",
    "# sigma = 2.0*a.dot(a.T) + (1.0 + 0.5*(2.0*np.random.random(100)-1.0)) * np.eye(100)\n",
    "sigma = 0.5 * a.dot(a.T) + (1.0 + 0.5 * (2.0 * np.random.random(100) - 1.0)) * np.eye(\n",
    "    100\n",
    ")\n",
    "\n",
    "fig = go.Figure(data=go.Heatmap(z=sigma))\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (2.0 * np.random.random(size=(n_genes, 1)) - 1).astype(float)\n",
    "sigma = 2.0 * a.dot(a.T) + 0.5 * (\n",
    "    1.0 + 0.5 * (2.0 * np.random.random(n_genes) - 1.0)\n",
    ") * np.eye(n_genes)\n",
    "sigma0 = 0.1 * sigma\n",
    "\n",
    "# sigma = 0.5 *a.dot(a.T) + (1.0 + 0.5*(2.0*np.random.random(n_genes)-1.0)) * np.eye(n_genes)\n",
    "# sigma0 = 0.05*sigma\n",
    "\n",
    "a = (2.0 * np.random.random(size=(n_genes, 1)) - 1).astype(float)\n",
    "sigma = 2.0 * a.dot(a.T) + 0.5 * (\n",
    "    1.0 + 0.5 * (2.0 * np.random.random(n_genes) - 1.0)\n",
    ") * np.eye(n_genes)\n",
    "sigma1 = 0.1 * sigma\n",
    "sigma1 = sigma0\n",
    "\n",
    "# sigma1 = sigma\n",
    "\n",
    "# u, s, vh = np.linalg.svd(sigma)\n",
    "# perturbations = s.min() + (s.max() - s.min()) * np.random.random(len(s))\n",
    "# sigma1 = u @ (np.diag(perturbations)) @ vh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0 = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.tensor(loge_mu0), covariance_matrix=torch.tensor(sigma0)\n",
    ").sample((5000,))\n",
    "h1 = torch.distributions.MultivariateNormal(\n",
    "    loc=torch.tensor(loge_mu1), covariance_matrix=torch.tensor(sigma1)\n",
    ").sample((5000,))\n",
    "\n",
    "h = torch.cat([h0, h1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_obs = torch.distributions.Poisson(rate=h.exp()).sample()\n",
    "# is_zi = np.random.random(x_obs.shape) >= 0.9\n",
    "is_zi = (np.random.random(x_obs.shape) <= np.exp(-0.5 * x_obs.numpy())) | (\n",
    "    np.random.random(x_obs.shape) <= 0.4\n",
    ")\n",
    "print(is_zi.mean())\n",
    "x_obs[is_zi] = 0.0\n",
    "labels = torch.zeros((10000, 1))\n",
    "labels[5000:] = 1\n",
    "\n",
    "not_null_cell = x_obs.sum(1) != 0\n",
    "x_obs = x_obs[not_null_cell]\n",
    "labels = labels[not_null_cell]\n",
    "\n",
    "trace1 = go.Histogram(x=x_obs.mean(0))\n",
    "fig = go.Figure(data=[trace1])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plt.hist(x_obs[:, 500], bins=100)\n",
    "plt.show()\n",
    "_ = plt.hist(x_obs[:, 20], bins=100)\n",
    "plt.show()\n",
    "_ = plt.hist(x_obs[:, 28], bins=100)\n",
    "plt.show()\n",
    "_ = plt.hist(x_obs[:, 100], bins=100)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfcs0 = lfcs.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assert False\n",
    "dataset_path = os.path.join(DIR_PATH, \"dataset.pickle\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    dataset = GeneExpressionDataset()\n",
    "    dataset.populate_from_data(X=x_obs.numpy(), labels=labels.numpy())\n",
    "    dataset.lfc = lfcs\n",
    "    save_pickle(data=dataset, filename=dataset_path)\n",
    "else:\n",
    "    dataset = load_pickle(dataset_path)\n",
    "    lfcs = dataset.lfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfcs == lfcs0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h0_bis, h1_bis = demultiply(h0, h1, factor=6)\n",
    "lfc_orig = h0_bis.exp().log2() - h1_bis.exp().log2()\n",
    "lfc_gt = lfc_orig.mean(0)\n",
    "lfc_gt = lfc_gt.numpy()\n",
    "\n",
    "# is_significant_de = (np.abs(lfc_orig) >= DELTA).numpy().mean(0) >= 0.5\n",
    "is_significant_de = np.abs(lfcs[:, 0] - lfcs[:, 1]) >= DELTA\n",
    "n_genes = dataset.nb_genes\n",
    "trace1 = go.Histogram(x=lfcs[:, 1] - lfcs[:, 0])\n",
    "fig = go.Figure(data=[trace1])\n",
    "# save_fig(fig, filename=\"powsimR_properties\", do_cloud=DO_CLOUD)\n",
    "# fig.show()\n",
    "iplot(fig, filename=\"lognormal2_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = len(dataset)\n",
    "TEST_INDICES = np.random.permutation(n_examples)[:2000]\n",
    "\n",
    "x_test, y_test = dataset.X[TEST_INDICES, :], dataset.labels[TEST_INDICES, :].squeeze()\n",
    "data_path = os.path.join(DIR_PATH, \"data.npy\")\n",
    "labels_path = os.path.join(DIR_PATH, \"labels.npy\")\n",
    "means_path = os.path.join(DIR_PATH, \"means.npy\")\n",
    "\n",
    "np.save(data_path, x_test.squeeze().astype(int))\n",
    "np.save(means_path, h[TEST_INDICES].exp())\n",
    "np.savetxt(labels_path, y_test.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EARLY_STOPPING_KWARGS = {\n",
    "    \"early_stopping_metric\": \"elbo_ratio_loss\",\n",
    "    \"save_best_state_metric\": \"elbo_ratio_loss\",\n",
    "    \"patience\": 20,\n",
    "    \"threshold\": 0,\n",
    "    \"reduce_lr_on_plateau\": True,\n",
    "    \"lr_patience\": 10,\n",
    "    \"lr_factor\": 0.2,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_params = dict(\n",
    "    iaf_b=dict(n_hidden=128, n_layers=2, do_h=False, n_latent=12, t=3, dropout_rate=0.2),\n",
    "    mf_b=dict(n_hidden=128, n_layers=1, n_latent=5, dropout_rate=0.1),\n",
    "\n",
    "    \n",
    "    iaf=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4, dropout_rate=0.2),\n",
    "    iaf_res=dict(\n",
    "        n_hidden=128,\n",
    "        n_layers=1,\n",
    "        do_h=True,\n",
    "        n_latent=10,\n",
    "        t=4,\n",
    "        dropout_rate=0.2,\n",
    "        n_blocks_encoder=1,\n",
    "        res_connection_decoder=False,\n",
    "    ),\n",
    "    mf=dict(n_hidden=128, n_layers=1, n_latent=10, dropout_rate=0.2),\n",
    "    mf_skip=dict(\n",
    "        n_hidden=128,\n",
    "        n_layers=1,\n",
    "        n_latent=10,\n",
    "        dropout_rate=0.2,\n",
    "        n_blocks=1,\n",
    "        decoder_do_last_skip=True,\n",
    "    ),\n",
    "    mf_skip2=dict(\n",
    "        n_hidden=128,\n",
    "        n_layers=1,\n",
    "        n_latent=10,\n",
    "        dropout_rate=0.2,\n",
    "        n_blocks=2,\n",
    "        decoder_do_last_skip=True,\n",
    "    ),\n",
    "    iaf_at=dict(\n",
    "        n_hidden=128, n_layers=1, do_h=False, n_latent=10, t=2, dropout_rate=0.2\n",
    "    ),\n",
    "    mf_at=dict(n_hidden=128, n_layers=1, n_latent=5, dropout_rate=0.1),\n",
    "    iaf_k5=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4),\n",
    "    iaf_skip=dict(\n",
    "        n_hidden=128,\n",
    "        n_layers=1,\n",
    "        do_h=True,\n",
    "        n_latent=10,\n",
    "        t=4,\n",
    "        dropout_rate=0.2,\n",
    "        n_blocks=1,\n",
    "        decoder_do_last_skip=True,\n",
    "    ),\n",
    "    iaf_skip2=dict(\n",
    "        n_hidden=128,\n",
    "        n_layers=1,\n",
    "        do_h=True,\n",
    "        n_latent=10,\n",
    "        t=4,\n",
    "        dropout_rate=0.2,\n",
    "        n_blocks=2,\n",
    "        decoder_do_last_skip=True,\n",
    "    ),\n",
    "    mf_k5=dict(n_hidden=128, n_layers=1, n_latent=10),\n",
    ")\n",
    "train_params = dict(\n",
    "    iaf=dict(\n",
    "        ratio_loss=True,\n",
    "        test_indices=TEST_INDICES,\n",
    "        #         frequency=FREQUENCY,\n",
    "        #         early_stopping_kwargs=EARLY_STOPPING_KWARGS,\n",
    "    ),\n",
    "    base=dict(\n",
    "        ratio_loss=True,\n",
    "        test_indices=TEST_INDICES,\n",
    "        frequency=FREQUENCY,\n",
    "        early_stopping_kwargs=EARLY_STOPPING_KWARGS,\n",
    "    ),\n",
    "    iaf_b=dict(\n",
    "        ratio_loss=True,\n",
    "        test_indices=TEST_INDICES,\n",
    "        frequency=FREQUENCY,\n",
    "        early_stopping_kwargs=EARLY_STOPPING_KWARGS,\n",
    "    ),\n",
    "    mf=dict(\n",
    "        ratio_loss=True,\n",
    "        test_indices=TEST_INDICES,\n",
    "        #         frequency=FREQUENCY,\n",
    "        #         early_stopping_kwargs=EARLY_STOPPING_KWARGS,\n",
    "    ),\n",
    "    iaf_k5=dict(\n",
    "        ratio_loss=True,\n",
    "        test_indices=TEST_INDICES,\n",
    "        k_importance_weighted=5,\n",
    "        single_backward=False,\n",
    "        frequency=FREQUENCY,\n",
    "        early_stopping_kwargs=EARLY_STOPPING_KWARGS,\n",
    "    ),\n",
    "    mf_k5=dict(\n",
    "        ratio_loss=True,\n",
    "        test_indices=TEST_INDICES,\n",
    "        k_importance_weighted=5,\n",
    "        single_backward=False,\n",
    "        frequency=FREQUENCY,\n",
    "        early_stopping_kwargs=EARLY_STOPPING_KWARGS,\n",
    "    ),\n",
    ")\n",
    "train_fn_params = dict(\n",
    "    iaf=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    base=dict(n_epochs=600, lr=1e-2),\n",
    "    iaf_b=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    mf=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    iaf_k5=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    "    mf_k5=dict(n_epochs=N_EPOCHS, lr=1e-2),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute competitors scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions = all_predictions(\n",
    "    filename=os.path.join(DIR_PATH, \"other_predictions_double_check.pickle\"),\n",
    "    n_genes=n_genes,\n",
    "    n_picks=N_PICKS,\n",
    "    sizes=SIZES,\n",
    "    data_path=data_path,\n",
    "    labels_path=labels_path,\n",
    "    normalized_means=means_path,\n",
    "    delta=DELTA,\n",
    "    path_to_scripts=PATH_TO_SCRIPTS,\n",
    ")\n",
    "\n",
    "other_predictions = all_de_predictions(\n",
    "    other_predictions, significance_level=Q0, delta=DELTA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sign of LFC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.utils import plot_identity\n",
    "\n",
    "# lfc_gt = -(lfcs[:, 1] - lfcs[:, 0])\n",
    "# lfc_gt = - (h[:, 1] - lfcs0[:, 0])\n",
    "plt.scatter(lfc_gt, other_predictions[\"edger\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(lfc_gt, other_predictions[\"deseq2\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(lfc_gt, other_predictions[\"mast\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"] = -other_predictions[\"edger\"][\"lfc\"]\n",
    "other_predictions[\"mast\"][\"lfc\"] = -other_predictions[\"mast\"][\"lfc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: redo experiments, already done for MF (called 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_mf_final1_high_lr_epochs.pickle\"),\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf_final1_high_lr_epochs3.pickle\"),\n",
    "#         filename=os.path.join(DIR_PATH, \"res_mf.pickle\"),\n",
    "    mdl_class=VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    "    sizes=SIZES,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    normalized_means=h.exp(),\n",
    "    label_b=1,\n",
    ").assign(algorithm=\"MF\")\n",
    "\n",
    "# res_mf_skip = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf_skip_final1_high_lr_epochs.pickle\"),\n",
    "#     #     filename=os.path.join(DIR_PATH, \"res_mf.pickle\"),\n",
    "#     mdl_class=VAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"mf_skip\"],\n",
    "#     train_params=train_params[\"mf\"],\n",
    "#     train_fn_params=train_fn_params[\"mf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "# ).assign(algorithm=\"MF\")\n",
    "\n",
    "# res_mf_skip2 = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf_skip2_final1_high_lr_epochs.pickle\"),\n",
    "#     #     filename=os.path.join(DIR_PATH, \"res_mf.pickle\"),\n",
    "#     mdl_class=VAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"mf_skip2\"],\n",
    "#     train_params=train_params[\"mf\"],\n",
    "#     train_fn_params=train_fn_params[\"mf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "# ).assign(algorithm=\"MF\")\n",
    "\n",
    "\n",
    "\n",
    "res_iaf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_iaf_final1_high_lr_epochs.pickle\"),\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf_final1_high_lr_epochs3.pickle\"),\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "    sizes=SIZES,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    normalized_means=h.exp(),\n",
    "    label_a=0,\n",
    "    label_b=1,\n",
    ").assign(algorithm=\"IAF\")\n",
    "\n",
    "# res_iafk5 = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iafk5_final1_high_lr_epochs.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "# ).assign(algorithm=\"IAF K5\")\n",
    "\n",
    "# res_iaf_res = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf_res_at_final1_high_lr_epochs.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf_res\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "# ).assign(algorithm=\"IAF AT\")\n",
    "\n",
    "# res_iaf_at = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf__at_final1_high_lr_epochs.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf_at\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "# ).assign(algorithm=\"IAF AT\")\n",
    "\n",
    "# res_iaf_skip = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf_skip_final1_high_lr_epochs.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf_skip\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "# ).assign(algorithm=\"IAF SKIP\")\n",
    "\n",
    "# res_iaf_skip2 = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf_skip2_final1_high_lr_epochs.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf_skip2\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "# ).assign(algorithm=\"IAF SKIP\")\n",
    "# res_iaf_is = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf_is_final1_high_lr_epochs.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1,\n",
    "#     importance_sampling=True\n",
    "# ).assign(algorithm=\"IAF IS\")\n",
    "\n",
    "\n",
    "# res_mf = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf_at.pickle\"),\n",
    "#     mdl_class=VAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"mf_at\"],\n",
    "#     train_params=train_params[\"mf\"],\n",
    "#     train_fn_params=train_fn_params[\"mf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1\n",
    "# ).assign(algorithm=\"MF\")\n",
    "\n",
    "# res_iaf = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf_at.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf_at\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1\n",
    "# ).assign(algorithm=\"IAF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_iaf.loc[res_iaf.sample_size == 5, \"de_proba\"].hist()\n",
    "plt.show()\n",
    "res_iaf.loc[res_iaf.sample_size == 100, \"de_proba\"].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algos_comparison(my_df, key1, other_keys, key_values=\"error\"):\n",
    "    vals_key1 = my_df.loc[my_df[\"algorithm\"] == key1, key_values].values\n",
    "    algo1_is_better = True\n",
    "    for key2 in other_keys:\n",
    "        vals_other = my_df.loc[my_df[\"algorithm\"] == key2, key_values].values\n",
    "        try:\n",
    "            key1_better = has_lower_mean(vals_key1, vals_other)\n",
    "        except ValueError:\n",
    "            key1_better = False\n",
    "            break\n",
    "        if not key1_better:\n",
    "            algo1_is_better = False\n",
    "            break\n",
    "    return key1_better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR / Power Control and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    mdl_class, dataset, mdl_params: dict, train_params: dict, train_fn_params: dict\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param mdl_class: Class of algorithm\n",
    "    :param dataset: Dataset\n",
    "    :param mdl_params:\n",
    "    :param train_params:\n",
    "    :param train_fn_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    my_vae = mdl_class(dataset.nb_genes, n_batch=dataset.n_batches, **mdl_params)\n",
    "    my_trainer = UnsupervisedTrainer(my_vae, dataset, **train_params)\n",
    "    print(my_trainer.test_set.data_loader.sampler.indices)\n",
    "    my_trainer.train(**train_fn_params)\n",
    "    print(my_trainer.train_losses)\n",
    "    return my_vae, my_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### FDR and TPR Control"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Posterior Expected FDR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: Comparer flows avec MF pour les mÃªmes 5 cells et comparer PE FDR a FDR\n",
    "Montrer que FDR mieux estimer avec flows est super cool\n",
    "\n",
    "Dans papier, Ok d'utiliser deux decision rules. Dire que PE FDR overconservative ok\n",
    "Dire investigation futur papier\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fdr(probas):\n",
    "    sorted_genes = np.argsort(-probas)\n",
    "    sorted_pgs = probas[sorted_genes]\n",
    "    cumulative_fdr = (1.0 - sorted_pgs).cumsum() / (1.0 + np.arange(len(sorted_pgs)))\n",
    "    d = (cumulative_fdr <= 5e-2).sum() - 1\n",
    "    return cumulative_fdr, sorted_genes\n",
    "\n",
    "\n",
    "def get_fdr_gt(my_sorted_genes):\n",
    "    fdr_k = []\n",
    "    for k in range(n_genes):\n",
    "        predictions = np.zeros(n_genes)\n",
    "        predictions[my_sorted_genes[: (k + 1)]] = 1\n",
    "        fdr = ((~is_significant_de) * predictions).sum() / (k + 1)\n",
    "        fdr_k.append(fdr)\n",
    "    return np.array(fdr_k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sizes effects**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# probas_5 = res_mf.loc[\n",
    "#     lambda x: (x.experiment == 3) & (x.training == 0) & (x.sample_size == 5), \"de_proba\"\n",
    "# ].values\n",
    "# probas_100 = res_mf.loc[\n",
    "#     lambda x: (x.experiment == 3) & (x.training == 0) & (x.sample_size == 100),\n",
    "#     \"de_proba\",\n",
    "# ].values\n",
    "\n",
    "# cumulative_fdr_5, sorted_genes_5 = get_fdr(probas_5)\n",
    "# fdr_k = get_fdr_gt(sorted_genes_5)\n",
    "\n",
    "# cumulative_fdr_100, sorted_genes_100 = get_fdr(probas_100)\n",
    "# fdr_k_100 = get_fdr_gt(sorted_genes_100)\n",
    "\n",
    "# fig = go.Figure(\n",
    "#     [\n",
    "#         go.Scatter(y=cumulative_fdr_5, name=\"Posterior Expected FDR\"),\n",
    "#         go.Scatter(y=fdr_k, name=\"Ground-Truth FDR\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# iplot(fig, sharing=\"private\", filename=\"logpoisson_pe_fdr5\")\n",
    "\n",
    "# fig = go.Figure(\n",
    "#     [\n",
    "#         go.Scatter(y=cumulative_fdr_100, name=\"Posterior Expected FDR\"),\n",
    "#         go.Scatter(y=fdr_k_100, name=\"Ground-Truth FDR\"),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# fig.show()\n",
    "# # iplot(fig, sharing=\"private\", filename=\"logpoisson_pe_fdr100\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IAF vs MF**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_mf = res_mf.loc[\n",
    "    lambda x: (x.experiment == 2) & (x.training == 0) & (x.sample_size == 5), \"de_proba\"\n",
    "].values\n",
    "probas_iaf = res_iaf.loc[\n",
    "    lambda x: (x.experiment == 2) & (x.training == 0) & (x.sample_size == 5), \"de_proba\"\n",
    "].values\n",
    "\n",
    "cumulative_fdr_mf, sorted_genes_mf = get_fdr(probas_mf)\n",
    "fdr_mf = get_fdr_gt(sorted_genes_mf)\n",
    "\n",
    "cumulative_fdr_iaf, sorted_genes_iaf = get_fdr(probas_iaf)\n",
    "fdr_iaf = get_fdr_gt(sorted_genes_iaf)\n",
    "\n",
    "plt.plot(cumulative_fdr_mf, label=\"PE FDR MF\")\n",
    "plt.plot(cumulative_fdr_iaf, label=\"PE FDR IAF\")\n",
    "plt.plot(fdr_mf, label=\"True MF\")\n",
    "plt.plot(fdr_iaf, label=\"True IAF\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "all_res = pd.concat([res_mf, res_iaf], ignore_index=True)\n",
    "\n",
    "def apply_fdr_compute(my_df):\n",
    "    cum_fdr, sorted_genes = get_fdr(my_df[\"de_proba\"].values)\n",
    "    return pd.Series(dict(cum_pefdr=cum_fdr, sorted_genes=sorted_genes))\n",
    "\n",
    "\n",
    "pfdr_study_all = (\n",
    "    all_res.groupby([\"experiment\", \"training\", \"sample_size\", \"algorithm\"])\n",
    "    .apply(apply_fdr_compute)\n",
    "    .reset_index()\n",
    ")\n",
    "pfdr_study_all.loc[:, \"fdr_gt\"] = pfdr_study_all.loc[:, \"sorted_genes\"].apply(\n",
    "    get_fdr_gt\n",
    ")\n",
    "\n",
    "pfdr_study_all.loc[:, \"diff\"] = pfdr_study_all[\"cum_pefdr\"] - pfdr_study_all[\"fdr_gt\"]\n",
    "pfdr_study_all.loc[:, \"L2error\"] = pfdr_study_all[\"diff\"].apply(np.linalg.norm)\n",
    "pfdr_study_all.loc[:, \"L1error\"] = pfdr_study_all[\"diff\"].apply(np.max)\n",
    "\n",
    "my_key = \"L1error\"\n",
    "\n",
    "gped = pfdr_study_all.groupby(\"sample_size\")\n",
    "fdr_mf_better = gped.apply(\n",
    "    algos_comparison, key1=\"MF\", other_keys=[\"IAF\"], key_values=my_key\n",
    ")\n",
    "fdr_iaf_better = gped.apply(\n",
    "    algos_comparison, key1=\"IAF\", other_keys=[\"MF\"], key_values=my_key\n",
    ")\n",
    "\n",
    "res_table = (\n",
    "    pfdr_study_all.groupby([\"sample_size\", \"algorithm\"])[my_key]\n",
    "    .mean()\n",
    "    .round(3)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"err_better\"] = fdr_mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"err_better\"] = fdr_iaf_better.values\n",
    "\n",
    "res_table.loc[res_table[\"err_better\"], my_key] = res_table.loc[\n",
    "    res_table[\"err_better\"], my_key\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[:, my_key] = res_table.loc[:, my_key].apply(lambda x: \"$ {} $\".format(x))\n",
    "\n",
    "res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=[my_key]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pfdr_study_all.groupby(\"sample_size\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FDR vals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTROL_ALPHA = False\n",
    "\n",
    "\n",
    "def fdr_fnr(my_df, control_alpha=True):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    assert len(my_df) == n_genes\n",
    "    if control_alpha:\n",
    "        is_pred_de = predict_de_genes(my_df.de_proba.values, desired_fdr=Q0)\n",
    "        alpha = my_df.de_proba.values[is_pred_de].min()\n",
    "    else:\n",
    "        is_pred_de = my_df.de_proba.values >= 0.5\n",
    "        alpha = 0.5\n",
    "    true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "    n_positives = is_significant_de.sum()\n",
    "    true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "    return pd.Series(dict(fdr=true_fdr, fnr=true_fnr, alpha=alpha))\n",
    "\n",
    "\n",
    "fdr_fnr_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr, control_alpha=CONTROL_ALPHA)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "fdr_fnr_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr, control_alpha=CONTROL_ALPHA)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "# fdr_fnr_iaf_at = (\n",
    "#     res_iaf_at.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "#     .apply(fdr_fnr, control_alpha=CONTROL_ALPHA)\n",
    "#     .reset_index()\n",
    "#     .assign(algorithm=\"IAF\")\n",
    "# )\n",
    "# fdr_fnr_iaf_res = (\n",
    "#     res_iaf_res.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "#     .apply(fdr_fnr, control_alpha=CONTROL_ALPHA)\n",
    "#     .reset_index()\n",
    "#     .assign(algorithm=\"IAF\")\n",
    "# )\n",
    "# fdr_fnr_iaf_is = (\n",
    "#     res_iaf_is.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "#     .apply(fdr_fnr, control_alpha=CONTROL_ALPHA)\n",
    "#     .reset_index()\n",
    "#     .assign(algorithm=\"IAF\")\n",
    "# )\n",
    "# fdr_fnr_iafk5 = (\n",
    "#     res_iafk5.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "#     .apply(fdr_fnr, control_alpha=CONTROL_ALPHA)\n",
    "#     .reset_index()\n",
    "#     .assign(algorithm=\"IAF\")\n",
    "# )\n",
    "\n",
    "# fdr_fnr_iaf_skip = (\n",
    "#     res_iaf_skip.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "#     .apply(fdr_fnr, control_alpha=CONTROL_ALPHA)\n",
    "#     .reset_index()\n",
    "#     .assign(algorithm=\"IAF\")\n",
    "# )\n",
    "\n",
    "df = pd.concat(\n",
    "    [\n",
    "        fdr_fnr_mf,\n",
    "        fdr_fnr_iaf,\n",
    "#         fdr_fnr_iaf_res\n",
    "        #         fdr_fnr_iaf_at,\n",
    "        #         fdr_fnr_iaf_is,\n",
    "        #         fdr_fnr_iafk5,\n",
    "        #         fdr_fnr_iaf_skip.assign(algorithm=\"IAF SKIP\")\n",
    "#         fdr_fnr_iaf_skip,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fdr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Discovery Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_fdr_control\")\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fnr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Negative Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_power_control\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[lambda x: x.sample_size == 100].groupby([\"algorithm\", \"training\"])[\n",
    "    \"fdr\", \"fnr\"\n",
    "].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['deseq2', 'edger', 'mast']\n",
    "\n",
    "\n",
    "def get_fdr_fnr(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        y_pred: (n_sz, n_picks, n_genes) bool predictions\n",
    "        y_true: (n_genes) gt vals\n",
    "    \"\"\"\n",
    "    n_sz, n_picks, _ = y_pred.shape\n",
    "    fnrs = np.zeros((n_sz, n_picks))\n",
    "    fdrs = np.zeros((n_sz, n_picks))\n",
    "    for sz in range(n_sz):\n",
    "        for pick in range(n_picks):\n",
    "            y_pred_it = y_pred[sz, pick, :]\n",
    "            fnr = ((~y_true) * y_pred_it).sum() / y_pred_it.sum()\n",
    "            fdr = (y_true * (~y_pred_it)).sum() / y_true.sum()\n",
    "            fnrs[sz, pick] = fnr\n",
    "            fdrs[sz, pick] = fdr\n",
    "    fnrs[np.isnan(fnrs)] = 0.0\n",
    "    return dict(fnr=fnrs, fdr=fdrs)\n",
    "\n",
    "\n",
    "print(other_predictions[\"mast\"][\"pval\"].shape)\n",
    "print(other_predictions[\"deseq2\"][\"pval\"].shape)\n",
    "print(other_predictions[\"edger\"][\"pval\"].shape)\n",
    "\n",
    "is_de_mast = other_predictions[\"mast\"][\"is_de\"]\n",
    "is_de_deseq2 = other_predictions[\"deseq2\"][\"is_de\"]\n",
    "is_de_edger = other_predictions[\"edger\"][\"is_de\"]\n",
    "# is_de_edgerr = other_predictions[\"edger_robust\"][\"is_de\"]\n",
    "\n",
    "\n",
    "res_mast = get_fdr_fnr(is_de_mast, y_true=is_significant_de)\n",
    "res_deseq2 = get_fdr_fnr(is_de_deseq2, y_true=is_significant_de)\n",
    "res_edger = get_fdr_fnr(is_de_edger, y_true=is_significant_de)\n",
    "# res_edgerr = get_fdr_fnr(is_de_edgerr, y_true=is_significant_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_mast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf = res_mf[\n",
    "    (res_mf.experiment == 0) & (res_mf.training == 0) & (res_mf.sample_size == 100)\n",
    "]\n",
    "preds_iaf = res_iaf[\n",
    "    (res_iaf.experiment == 0) & (res_iaf.training == 0) & (res_iaf.sample_size == 100)\n",
    "]\n",
    "\n",
    "# preds_mf = preds_mf.sort_values(\"de_proba\").set_index(\"gene\")\n",
    "# preds_iaf = preds_iaf.set_index(\"gene\").reindex(index=preds_mf.index)\n",
    "# preds_iaf[]\n",
    "\n",
    "preds = pd.concat([preds_mf, preds_iaf], ignore_index=True)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importance sampling marche moins bien"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = df[df.training == 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algos_comparison(my_df, key1, other_keys, key_values=\"error\"):\n",
    "    vals_key1 = my_df.loc[my_df[\"algorithm\"] == key1, key_values].values\n",
    "    algo1_is_better = True\n",
    "    for key2 in other_keys:\n",
    "        vals_other = my_df.loc[my_df[\"algorithm\"] == key2, key_values].values\n",
    "        try:\n",
    "            key1_better = has_lower_mean(vals_key1, vals_other)\n",
    "        except ValueError:\n",
    "            key1_better = False\n",
    "            break\n",
    "        if not key1_better:\n",
    "            algo1_is_better = False\n",
    "            break\n",
    "    return key1_better\n",
    "\n",
    "\n",
    "gped = df.groupby(\"sample_size\")\n",
    "fdr_mf_better = gped.apply(\n",
    "    algos_comparison, key1=\"MF\", other_keys=[\"IAF\"], key_values=\"fdr\"\n",
    ")\n",
    "fdr_iaf_better = gped.apply(\n",
    "    algos_comparison, key1=\"IAF\", other_keys=[\"MF\"], key_values=\"fdr\"\n",
    ")\n",
    "\n",
    "fnr_mf_better = gped.apply(\n",
    "    algos_comparison, key1=\"MF\", other_keys=[\"IAF\"], key_values=\"fnr\"\n",
    ")\n",
    "fnr_iaf_better = gped.apply(\n",
    "    algos_comparison, key1=\"IAF\", other_keys=[\"MF\"], key_values=\"fnr\"\n",
    ")\n",
    "\n",
    "res_table = (\n",
    "    df.groupby([\"sample_size\", \"algorithm\"])[\"fdr\", \"fnr\"].mean().round(3).reset_index()\n",
    ")\n",
    "\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"fdr_better\"] = fdr_mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"fdr_better\"] = fdr_iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"fnr_better\"] = fnr_mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"fnr_better\"] = fnr_iaf_better.values\n",
    "\n",
    "res_table.loc[res_table[\"fdr_better\"], \"fdr\"] = res_table.loc[\n",
    "    res_table[\"fdr_better\"], \"fdr\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[res_table[\"fnr_better\"], \"fnr\"] = res_table.loc[\n",
    "    res_table[\"fnr_better\"], \"fnr\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[:, \"fdr\"] = res_table.loc[:, \"fdr\"].apply(lambda x: \"$ {} $\".format(x))\n",
    "res_table.loc[:, \"fnr\"] = res_table.loc[:, \"fnr\"].apply(lambda x: \"$ {} $\".format(x))\n",
    "\n",
    "res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(\n",
    "    res_table\n",
    "# .loc[lambda x: x[\"sample_size\"].isin([5, 20, 100])]\n",
    ".pivot(\n",
    "    index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"]\n",
    ").T\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    res_table.loc[lambda x: x[\"sample_size\"].isin([20, 50, 100])]\n",
    "    .pivot(index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"])\n",
    "    .T.to_latex(escape=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"fdr\")\n",
    "    .loc[:, [5, 20, 100]]\n",
    "    .to_latex(escape=True)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 2\n",
    "\n",
    "preds_md = res_mf.loc[\n",
    "    lambda x: (x.experiment == 2)\n",
    "    & (x.training == selected_training)\n",
    "    & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "preds_iaf = res_iaf.loc[\n",
    "    lambda x: (x.experiment == 2)\n",
    "    & (x.training == selected_training)\n",
    "    & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "# preds_iaf_res = res_iaf_res.loc[\n",
    "#     lambda x: (x.experiment == 2)\n",
    "#     & (x.training == selected_training)\n",
    "#     & (x.sample_size == 100)\n",
    "# ].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "# preds_iafk5 = res_iafk5.loc[\n",
    "#     lambda x: (x.experiment == 0)\n",
    "#     & (x.training == selected_training)\n",
    "#     & (x.sample_size == 100)\n",
    "# ].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "# preds_iaf_skip = res_iaf_skip.loc[\n",
    "#     lambda x: (x.experiment == 0)\n",
    "#     & (x.training == selected_training)\n",
    "#     & (x.sample_size == 100)\n",
    "# ].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "# preds_iaf_is = res_iaf_is.loc[\n",
    "#     lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "# ].sort_values(\"gene\")[\"de_proba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"deseq2\"][\"pval\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "preds_deseq2 = 1.0 - other_predictions[\"deseq2\"][\"pval\"][-1, 0, :] + 1e-10*np.random.randn(n_genes)\n",
    "preds_edger = 1.0 - other_predictions[\"edger\"][\"pval\"][-1, 0, :] #+ 1e-10*np.random.randn(n_genes)\n",
    "preds_mast = 1.0 - other_predictions[\"mast\"][\"pval\"][-1, 0, :] #+ 1e-10*np.random.randn(n_genes)\n",
    "\n",
    "# preds_deseq2 = 1.0 - other_predictions['deseq2']['pval'][:]\n",
    "# preds_edger = 1.0 - other_predictions['edger']['pval'][:]\n",
    "# preds_mast = 1.0 - other_predictions['mast']['pval'][:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(preds_md).mean())\n",
    "print(np.isnan(preds_iaf).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_edger).mean())\n",
    "print(np.isnan(preds_mast).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(\n",
    "    [\n",
    "        fdr_fnr_mf,\n",
    "        fdr_fnr_iaf,\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "df.loc[:, \"recall\"] = 1.0 - df.loc[:, \"fnr\"]\n",
    "recall_ranges = df.loc[lambda x: x.sample_size == 100].groupby([\"algorithm\"])[\"recall\"]\n",
    "\n",
    "# max_recalls = recall_ranges.min()\n",
    "# min_recalls = recall_ranges.max()\n",
    "# display(min_recalls, max_recalls)\n",
    "\n",
    "min_recalls = recall_ranges.mean() - 2.0*recall_ranges.std() \n",
    "max_recalls = recall_ranges.mean() + 2.0*recall_ranges.std() \n",
    "display(min_recalls, max_recalls)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "stat, pval = mannwhitneyu(\n",
    "    df.loc[lambda x: x.algorithm==\"IAF\", \"recall\"],\n",
    "    df.loc[lambda x: x.algorithm==\"MF\", \"recall\"],\n",
    "    alternative=\"greater\"\n",
    ")\n",
    "\n",
    "print(stat, pval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[:, \"precision\"] = 1.0 - df.loc[:, \"fdr\"]\n",
    "precision_ranges = df.loc[lambda x: x.sample_size == 100].groupby([\"algorithm\"])[\"precision\"]\n",
    "\n",
    "# max_recalls = recall_ranges.min()\n",
    "# min_recalls = recall_ranges.max()\n",
    "# display(min_recalls, max_recalls)\n",
    "\n",
    "min_precisions = precision_ranges.mean() - 2.0*precision_ranges.std() \n",
    "max_precisions = precision_ranges.mean() + 2.0*precision_ranges.std()\n",
    "max_precisions[max_precisions>=1.0] = 1.0\n",
    "display(min_precisions, max_precisions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import mannwhitneyu\n",
    "\n",
    "stat, pval = mannwhitneyu(\n",
    "    df.loc[lambda x: x.algorithm==\"IAF\", \"precision\"],\n",
    "    df.loc[lambda x: x.algorithm==\"MF\", \"precision\"],\n",
    "    alternative=\"greater\"\n",
    ")\n",
    "\n",
    "print(stat, pval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "prec_iaf, rec_iaf = (\n",
    "    precision_score(is_significant_de, preds_iaf >= 0.5),\n",
    "    recall_score(is_significant_de, preds_iaf >= 0.5),\n",
    ")\n",
    "prec_mf, rec_mf = (\n",
    "    precision_score(is_significant_de, preds_md >= 0.5),\n",
    "    recall_score(is_significant_de, preds_md >= 0.5),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opacity = 0.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "\n",
    "def plot_pr(fig, preds, y_true, name):\n",
    "    average_precision = average_precision_score(y_true, preds)\n",
    "    preds[np.isnan(preds)] = np.min(preds[~np.isnan(preds)])\n",
    "    precs, recs, _ = precision_recall_curve(y_true=y_true, probas_pred=preds)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=recs, y=precs, name=name + \"@AP: {0:0.2f}\".format(average_precision)\n",
    "        )\n",
    "    )\n",
    "    return\n",
    "\n",
    "\n",
    "layout = go.Layout(\n",
    "    title=\"Precision Recall Curves\",\n",
    "    xaxis=dict(title=\"Recall\"),\n",
    "    yaxis=dict(title=\"Precision\"),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "plot_pr(fig=fig, preds=preds_md, y_true=is_significant_de, name=\"MF\")\n",
    "plot_pr(fig=fig, preds=preds_iaf, y_true=is_significant_de, name=\"IAF\")\n",
    "# plot_pr(fig=fig, preds=preds_iaf_res, y_true=is_significant_de, name=\"IAF RES\")\n",
    "# plot_pr(fig=fig, preds=preds_iaf_skip, y_true=is_significant_de, name=\"IAF SKIP\")\n",
    "# plot_pr(fig=fig, preds=preds_iafk5, y_true=is_significant_de, name=\"IAFK5\")\n",
    "# plot_pr(fig=fig, preds=preds_iaf_is, y_true=is_significant_de, name='IAF IS')\n",
    "\n",
    "\n",
    "plot_pr(fig=fig, preds=preds_deseq2, y_true=is_significant_de, name=\"DESeq2\")\n",
    "plot_pr(fig=fig, preds=preds_edger, y_true=is_significant_de, name=\"EdgeR\")\n",
    "plot_pr(fig=fig, preds=preds_mast, y_true=is_significant_de, name=\"MAST\")\n",
    "\n",
    "\n",
    "# trace = go.Scatter(\n",
    "#     y=(prec_mf, prec_iaf),\n",
    "#     x=(rec_mf, rec_iaf),\n",
    "#     marker=dict(size=2 * [14], color=[\"blue\", \"red\"]),\n",
    "#     showlegend=False,\n",
    "#     mode=\"markers\"\n",
    "# )\n",
    "# fig.add_trace(trace)\n",
    "\n",
    "layouts = [\n",
    "    go.layout.Shape(\n",
    "        type=\"rect\",\n",
    "        x0=min_recalls.MF,\n",
    "        y0=min_precisions.MF,\n",
    "        x1=max_recalls.MF,\n",
    "        y1=max_precisions.MF,\n",
    "        line=dict(color=\"blue\", width=2),\n",
    "        fillcolor=\"blue\",\n",
    "        opacity=opacity\n",
    "    ),\n",
    "    go.layout.Shape(\n",
    "        type=\"rect\",\n",
    "        x0=min_recalls.IAF,\n",
    "        y0=min_precisions.IAF,\n",
    "        x1=max_recalls.IAF,\n",
    "        y1=max_precisions.IAF,\n",
    "        line=dict(color=\"red\", width=2),\n",
    "        fillcolor=\"red\",\n",
    "        opacity=opacity\n",
    "    ),\n",
    "]\n",
    "\n",
    "fig.update_layout(shapes=layouts)\n",
    "fig.update_xaxes(range=[0.5, 1.01])\n",
    "fig.update_yaxes(range=[0.5, 1.01])\n",
    "fig.show()\n",
    "iplot(fig, filename=\"lognormal_pr_curves4\", sharing=\"private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ap(my_df):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    average_precision = average_precision_score(is_significant_de, my_df.de_proba)\n",
    "    return pd.Series(dict(AP=average_precision))\n",
    "\n",
    "\n",
    "ap_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "ap_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "\n",
    "# ap_iafk5 = (\n",
    "#     res_iaf_skip.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "#     .apply(do_ap)\n",
    "#     .reset_index()\n",
    "#     .assign(algorithm=\"IAF K5\")\n",
    "# )\n",
    "\n",
    "# ap_iaf_is = (\n",
    "#     res_iaf_is.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "#     .apply(do_ap)\n",
    "#     .reset_index()\n",
    "#     .assign(algorithm=\"IAF IS\")\n",
    "# )\n",
    "\n",
    "all_ap = pd.concat(\n",
    "    [\n",
    "        ap_mf,\n",
    "        ap_iaf,\n",
    "#         ap_iafk5,\n",
    "        #     ap_iaf_is\n",
    "    ],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "px.box(all_ap, x=\"sample_size\", y=\"AP\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sz in SIZES:\n",
    "    res = has_lower_mean(\n",
    "        all_ap.loc[lambda x: (x.algorithm == \"IAF\") & (x.sample_size == sz), \"AP\"],\n",
    "        all_ap.loc[lambda x: (x.algorithm == \"MF\") & (x.sample_size == sz), \"AP\"],\n",
    "    )\n",
    "    print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ap.groupby([\"algorithm\", \"sample_size\"]).agg(dict(AP=[\"mean\", \"std\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfc_gt = -(lfcs[:, 1] - lfcs[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 0\n",
    "subsample_genes = np.sort(np.random.permutation(n_genes)[:120])\n",
    "\n",
    "lfcs_mf = (\n",
    "    res_mf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        #         err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        #         err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_ia = (\n",
    "    res_iaf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        #         err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        #         err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_iak5 = (\n",
    "    res_iafk5.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        #         err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        #         err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "all_lfcs = pd.concat([lfcs_mf, lfcs_ia, lfcs_iak5], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    all_lfcs,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    color=\"algorithm\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# iplot(fig, sharing=\"private\", filename=\"logpoisson2_diagonal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5 against 100\n",
    "\n",
    "lfcs_a = (\n",
    "    res_iaf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 5)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        #         err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        #         err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "        legend=\"5\",\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_b = (\n",
    "    res_iaf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\n",
    "        [\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"hdi64_low\", \"hdi64_high\", \"algorithm\"]\n",
    "    ]\n",
    "    .assign(\n",
    "        #         err_minus=lambda x: x.lfc_mean - x.hdi64_low,\n",
    "        #         err_pos=lambda x: x.hdi64_high - x.lfc_mean,\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes],\n",
    "        legend=\"100\",\n",
    "    )\n",
    ")\n",
    "\n",
    "all_lfcs = pd.concat([lfcs_a, lfcs_b], ignore_index=True)\n",
    "\n",
    "fig = px.scatter(\n",
    "    all_lfcs,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    color=\"legend\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "        showlegend=False,\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of LFC errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_err(diff):\n",
    "    res = 0.5 * (diff ** 2) ** (0.5)\n",
    "    res = np.nanmean(res, axis=-1)\n",
    "    return res\n",
    "\n",
    "\n",
    "def l2_err_competitor(vals: np.ndarray, other: np.ndarray = None):\n",
    "    vals[np.isnan(vals)] = 0.0\n",
    "    if other is None:\n",
    "        diff = vals\n",
    "    else:\n",
    "        diff = vals - other\n",
    "    res = compute_l2_err(diff)\n",
    "    assert res.shape == (N_SIZES, N_PICKS)\n",
    "    data = []\n",
    "    for (size_ix, size) in enumerate(SIZES):\n",
    "        for pick in range(N_PICKS):\n",
    "            data.append(\n",
    "                dict(\n",
    "                    experiment=pick,\n",
    "                    training=0,\n",
    "                    sample_size=size,\n",
    "                    error=res[size_ix, pick],\n",
    "                )\n",
    "            )\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "\n",
    "lfcs_errs_deseq2 = l2_err_competitor(\n",
    "    other_predictions[\"deseq2\"][\"lfc\"], other=lfc_gt\n",
    ").assign(algorithm=\"DESeq2\")\n",
    "lfcs_errs_edger = l2_err_competitor(\n",
    "    other_predictions[\"edger\"][\"lfc\"], other=lfc_gt\n",
    ").assign(algorithm=\"EdgeR\")\n",
    "lfcs_errs_mast = l2_err_competitor(\n",
    "    other_predictions[\"mast\"][\"lfc\"], other=lfc_gt\n",
    ").assign(algorithm=\"MAST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_l2_err(my_df):\n",
    "    diff = my_df.sort_values(\"gene\")[\"lfc_mean\"] - lfc_gt\n",
    "    error = 0.5 * (diff ** 2) ** (0.5)\n",
    "    error = np.nanmean(error)\n",
    "    return pd.Series(dict(error=error))\n",
    "\n",
    "\n",
    "lfcs_errs_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "lfcs_errs_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errs = pd.concat(\n",
    "    [lfcs_errs_mf, lfcs_errs_iaf, lfcs_errs_deseq2, lfcs_errs_edger, lfcs_errs_mast],\n",
    "    ignore_index=True,\n",
    ")\n",
    "\n",
    "px.box(all_errs, x=\"sample_size\", y=\"error\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Skip connections in the decoder are without doubt beneficial**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algos_comparison(my_df, key1, other_keys):\n",
    "    vals_key1 = my_df.loc[my_df[\"algorithm\"] == key1, \"error\"].values\n",
    "    algo1_is_better = True\n",
    "    for key2 in other_keys:\n",
    "        vals_other = my_df.loc[my_df[\"algorithm\"] == key2, \"error\"].values\n",
    "        key1_better = has_lower_mean(vals_key1, vals_other)\n",
    "        if not key1_better:\n",
    "            algo1_is_better = False\n",
    "            break\n",
    "    return key1_better\n",
    "\n",
    "\n",
    "gped = all_errs.groupby(\"sample_size\")\n",
    "mf_or_iaf_better = gped.apply(\n",
    "    algos_comparison, key1=\"MF\", other_keys=[\"DESeq2\", \"EdgeR\", \"MAST\"]\n",
    ") & gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"DESeq2\", \"EdgeR\", \"MAST\"])\n",
    "mf_better = gped.apply(\n",
    "    algos_comparison, key1=\"MF\", other_keys=[\"IAF\", \"DESeq2\", \"EdgeR\", \"MAST\"]\n",
    ")\n",
    "iaf_better = gped.apply(\n",
    "    algos_comparison, key1=\"IAF\", other_keys=[\"MF\", \"DESeq2\", \"EdgeR\", \"MAST\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table = (\n",
    "    all_errs.groupby([\"sample_size\", \"algorithm\"])\n",
    "    .error.agg(dict(err_mean=\"mean\", err_std=\"std\"))\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        displayed=lambda x: x.apply(lambda y: \"{:.3f}\".format(y.err_mean), axis=1),\n",
    "        is_better=False,\n",
    "        one_of_best=False,\n",
    "    )\n",
    ")\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"is_better\"] = mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"is_better\"] = iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"one_of_best\"] = mf_or_iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"one_of_best\"] = mf_or_iaf_better.values\n",
    "\n",
    "\n",
    "res_table.loc[lambda x: x.one_of_best, \"displayed\"] = (\n",
    "    res_table.loc[lambda x: x.one_of_best, \"displayed\"] + \"^*\"\n",
    ")\n",
    "res_table.loc[lambda x: x.is_better, \"displayed\"] = res_table.loc[\n",
    "    lambda x: x.is_better, \"displayed\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[:, \"displayed\"] = res_table.loc[:, \"displayed\"].apply(\n",
    "    lambda x: \"$ {} $\".format(x)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"displayed\").loc[\n",
    "    [\"DESeq2\", \"EdgeR\", \"MAST\", \"MF\", \"IAF\"], #[20, 50, 100]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"displayed\")\n",
    "    .loc[[\"DESeq2\", \"EdgeR\", \"MAST\", \"MF\", \"IAF\"], [5, 20, 100]]\n",
    "    .to_latex(escape=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_params.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test to see if poorer performance of models for important number of cells is linked to mixing factors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_mf, trainer_mf = train_model(\n",
    "    mdl_class=VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    ")\n",
    "\n",
    "mdl_iaf, trainer_iaf = train_model(\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainer_mf.train_losses[10:])\n",
    "plt.plot(trainer_iaf.train_losses[10:])\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampled_posterior(post, indices):\n",
    "    post.data_loader.sampler.indices = indices\n",
    "    return post\n",
    "\n",
    "\n",
    "def sample_random_indices(sz):\n",
    "    where_a = np.where(y_test == 0)[0]\n",
    "    where_b = np.where(y_test == 1)[0]\n",
    "    idx_a = np.random.choice(where_a, size=sz)\n",
    "    idx_b = np.random.choice(where_b, size=sz)\n",
    "    return idx_a, idx_b\n",
    "\n",
    "\n",
    "def compute_lfc(my_trainer, my_idx_a, my_idx_b, n_samples=1000, importance_sampling=False):\n",
    "    post_a = subsampled_posterior(my_trainer.test_set, TEST_INDICES[my_idx_a])\n",
    "    outputs_a = post_a.get_latents(n_samples=1000, other=True, device=\"cpu\")\n",
    "    scales_a, weights_a = outputs_a[\"scale\"], outputs_a[\"log_probas\"]\n",
    "    scales_a = scales_a.reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "    post_b = subsampled_posterior(my_trainer.test_set, TEST_INDICES[my_idx_b])\n",
    "    outputs_b = post_b.get_latents(n_samples=1000, other=True, device=\"cpu\")\n",
    "    scales_b, weights_b = outputs_b[\"scale\"], outputs_b[\"log_probas\"]\n",
    "    scales_b = scales_b.reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "    if importance_sampling:\n",
    "        weights_a = softmax(weights_a.reshape((-1)))\n",
    "        weights_b = softmax(weights_b.reshape((-1)))\n",
    "    else:\n",
    "        weights_a = None\n",
    "        weights_b = None\n",
    "    scales_a, scales_b = demultiply(\n",
    "        arr1=scales_a, arr2=scales_b, factor=3, weights_a=weights_a, weights_b=weights_b\n",
    "    )\n",
    "\n",
    "    lfc = np.log2(scales_a) - np.log2(scales_b)\n",
    "    return lfc\n",
    "\n",
    "\n",
    "def compute_lfc_gt(my_idx_a, my_idx_b):\n",
    "    n_cells_a = len(my_idx_a)\n",
    "    n_cells_b = len(my_idx_b)\n",
    "    h_a_gt = h[TEST_INDICES][my_idx_a].exp().log2()\n",
    "    h_b_gt = h[TEST_INDICES][my_idx_b].exp().log2()\n",
    "    my_lfc_orig = torch.zeros((n_cells_a, n_cells_b, 1000))\n",
    "    for i in range(n_cells_a):\n",
    "        for j in range(n_cells_b):\n",
    "            my_lfc_orig[i, j, :] = h_a_gt[i] - h_b_gt[j]\n",
    "    my_lfc_orig = my_lfc_orig.mean((0, 1))\n",
    "    return my_lfc_orig\n",
    "\n",
    "\n",
    "CREDIBLE_LEVELS = [5, 10, 15, 20]\n",
    "\n",
    "\n",
    "def get_coverage(lfc_pred, lfc_gt):\n",
    "    errs = []\n",
    "    for q in CREDIBLE_LEVELS:\n",
    "        hdi = compute_hdi(lfc_pred, 2*q / 100.0)\n",
    "        hdi_low = hdi[:, 0]\n",
    "        hdi_high = hdi[:, 1]\n",
    "        lfc_ground_truth_np = lfc_gt.numpy()\n",
    "        gene_is_covered = (lfc_ground_truth_np >= hdi_low) & (lfc_ground_truth_np <= hdi_high)\n",
    "        mean_cov = gene_is_covered.mean()\n",
    "        \n",
    "#         hdi_low = np.percentile(lfc_pred, q=q)\n",
    "#         hdi_high = np.percentile(lfc_pred, q=100 - q)\n",
    "#         gene_is_covered = (lfc_gt >= hdi_low) & (lfc_gt <= hdi_high)\n",
    "#         mean_cov = gene_is_covered.numpy().mean()\n",
    "\n",
    "        mean_cov = gene_is_covered.mean()\n",
    "        print(2 * q / 100.0, mean_cov)\n",
    "        errs.append(((2 * q / 100.0 - mean_cov) ** 2.0) * 0.5)\n",
    "    return np.mean(errs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# errs_iaf = []\n",
    "# errs_iaf_is = []\n",
    "# errs_mf = []\n",
    "\n",
    "sz = 25\n",
    "for _ in tqdm(range(10)):\n",
    "    idx_a, idx_b = sample_random_indices(sz)\n",
    "    lfc_mf = compute_lfc(trainer_mf, idx_a, idx_b, n_samples=300)\n",
    "    lfc_iaf = compute_lfc(trainer_iaf, idx_a, idx_b, n_samples=300, importance_sampling=False)\n",
    "#     lfc_iaf_is = compute_lfc(trainer_iaf, idx_a, idx_b, n_samples=2000, importance_sampling=True)\n",
    "    lfc_ground_truth = compute_lfc_gt(idx_a, idx_b)\n",
    "    print(\"IAF\")\n",
    "    errs_iaf.append(get_coverage(lfc_iaf, lfc_ground_truth))\n",
    "#     errs_iaf_is.append(get_coverage(lfc_iaf_is, lfc_ground_truth))\n",
    "    print(\"MF\")\n",
    "    errs_mf.append(get_coverage(lfc_mf, lfc_ground_truth))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(errs_iaf, label=\"IAF\", alpha=0.5)\n",
    "# plt.hist(errs_iaf_is, label=\"IAF IS\", alpha=0.5)\n",
    "plt.hist(errs_mf, label='MF', alpha=0.5)\n",
    "plt.legend()\n",
    "\n",
    "print(has_lower_mean(samp_a=errs_iaf, samp_b=errs_mf))\n",
    "print(np.mean(errs_iaf))\n",
    "print(np.mean(errs_mf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "new FDR control (not very convincing!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sz = 100\n",
    "\n",
    "# idx_a, idx_b = sample_random_indices(sz)\n",
    "# lfc_mf = compute_lfc(trainer_mf, idx_a, idx_b, n_samples=5000)\n",
    "# lfc_iaf = compute_lfc(trainer_iaf, idx_a, idx_b, n_samples=5000)\n",
    "\n",
    "# probas_mf = np.abs(lfc_mf >= 0.5).mean(0)\n",
    "# probas_iaf = np.abs(lfc_iaf >= 0.5).mean(0)\n",
    "\n",
    "# lfc_ground_truth = compute_lfc_gt(idx_a, idx_b).numpy()\n",
    "# is_significant_de_local = lfc_ground_truth >= 0.5\n",
    "\n",
    "# def get_fdr(probas):\n",
    "#     sorted_genes = np.argsort(-probas)\n",
    "#     sorted_pgs = probas[sorted_genes]\n",
    "#     cumulative_fdr = (1.0 - sorted_pgs).cumsum() / (1.0 + np.arange(len(sorted_pgs)))\n",
    "#     d = (cumulative_fdr <= 5e-2).sum() - 1\n",
    "#     return cumulative_fdr, sorted_genes\n",
    "\n",
    "\n",
    "# def get_fdr_gt(is_de, my_sorted_genes):\n",
    "#     fdr_k = []\n",
    "#     for k in range(n_genes):\n",
    "#         predictions = np.zeros(n_genes)\n",
    "#         predictions[my_sorted_genes[: (k + 1)]] = 1\n",
    "#         fdr = ((~is_de) * predictions).sum() / (k + 1)\n",
    "#         fdr_k.append(fdr)\n",
    "#     return np.array(fdr_k)\n",
    "\n",
    "# cumulative_fdr, sorted_genes = get_fdr(probas_mf)\n",
    "# fdr_gt = get_fdr_gt(is_significant_de_local, sorted_genes)\n",
    "\n",
    "# d = (cumulative_fdr <= 5e-2).sum() - 1\n",
    "# print(fdr_gt[d])\n",
    "\n",
    "# plt.plot(fdr_gt)\n",
    "# plt.plot(cumulative_fdr)\n",
    "\n",
    "# cumulative_fdr, sorted_genes = get_fdr(probas_iaf)\n",
    "# fdr_gt = get_fdr_gt(is_significant_de_local, sorted_genes)\n",
    "\n",
    "# d = (cumulative_fdr <= 5e-2).sum() - 1\n",
    "# print(fdr_gt[d])\n",
    "\n",
    "\n",
    "# plt.plot(fdr_gt)\n",
    "# plt.plot(cumulative_fdr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Diagonal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 50\n",
    "\n",
    "# random_genes = np.random.permutation(n_genes)[:100]\n",
    "\n",
    "random_genes = np.argsort(-np.abs(lfc_gt))[:900]\n",
    "random_genes = np.random.choice(random_genes, 100)\n",
    "idx_a, idx_b = sample_random_indices(sz)\n",
    "lfc_mf = compute_lfc(trainer_mf, idx_a, idx_b, n_samples=2000)\n",
    "lfc_iaf = compute_lfc(trainer_iaf, idx_a, idx_b, n_samples=2000)\n",
    "lfc_ground_truth = compute_lfc_gt(idx_a, idx_b).numpy()[random_genes]\n",
    "\n",
    "mean_mf = lfc_mf.mean(0)[random_genes]\n",
    "mean_iaf = lfc_iaf.mean(0)[random_genes]\n",
    "mean_mf = np.median(lfc_mf, 0)[random_genes]\n",
    "mean_iaf = np.median(lfc_iaf, 0)[random_genes]\n",
    "hdis_mf = compute_hdi(lfc_mf, credible_interval=0.95)[random_genes]\n",
    "hdis_iaf = compute_hdi(lfc_iaf, credible_interval=0.95)[random_genes]\n",
    "\n",
    "fig = go.Figure()\n",
    "trace_mf = go.Scatter(\n",
    "    x=lfc_ground_truth-0.001,\n",
    "    y=mean_mf,\n",
    "    mode=\"markers\",\n",
    "    error_y=dict(\n",
    "        type=\"data\",\n",
    "        symmetric=False,\n",
    "        array=hdis_mf[:, 1] - mean_mf,\n",
    "        arrayminus=mean_mf - hdis_mf[:, 0],\n",
    "    ),\n",
    "    name=\"MF\"\n",
    ")\n",
    "\n",
    "trace_iaf = go.Scatter(\n",
    "    x=lfc_ground_truth+0.001,\n",
    "    y=mean_iaf,\n",
    "    mode=\"markers\",\n",
    "    error_y=dict(\n",
    "        type=\"data\",\n",
    "        symmetric=False,\n",
    "        array=hdis_iaf[:, 1] - mean_iaf,\n",
    "        arrayminus=mean_iaf - hdis_iaf[:, 0],\n",
    "    ),\n",
    "    name=\"IAF\"\n",
    ")\n",
    "trace_gt = go.Scatter(\n",
    "    x=[-6, 8],\n",
    "    y=[-6, 8],\n",
    "    mode=\"lines\",\n",
    "    line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "    showlegend=False,\n",
    ")\n",
    "fig.add_traces([trace_mf, trace_iaf, trace_gt])\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iplot(fig, filename=\"diagonal_logpoisson_new_low_LFC\", sharing=\"private\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = random_genes = np.argsort(-np.abs(lfc_gt))[10]\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_traces([\n",
    "    go.Histogram(x=lfc_mf[:, idx], name=\"MF\"),\n",
    "    go.Histogram(x=lfc_iaf[:, idx], name=\"IAF\"),\n",
    "])\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "plt.hist(lfc_mf[:, idx], alpha=0.5, bins=100)\n",
    "plt.hist(lfc_iaf[:, idx], alpha=0.5, bins=100)\n",
    "plt.axvline(compute_lfc_gt(idx_a, idx_b).numpy()[idx])\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volcano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 0\n",
    "\n",
    "preds_md = (\n",
    "    res_mf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\"de_proba\"]\n",
    "    .values\n",
    ")\n",
    "\n",
    "preds_iaf = (\n",
    "    res_iaf.loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "    ]\n",
    "    .sort_values(\"gene\")[\"de_proba\"]\n",
    "    .values\n",
    ")\n",
    "\n",
    "# preds_iaf_at = res_iaf_is.loc[\n",
    "#     lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "# ].sort_values(\"gene\")[\"de_proba\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsampled_genes = np.random.permutation(n_genes)[:300]\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=go.Layout(\n",
    "        yaxis=dict(title=\"Estimated probabily of DE\"),\n",
    "        xaxis=dict(title=\"Ground-Truth LFC\"),\n",
    "    )\n",
    ")\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Scatter(\n",
    "            x=lfc_gt[subsampled_genes],\n",
    "            #             y=np.log10(preds_md + 1e-12)[subsampled_genes],\n",
    "            y=preds_md[subsampled_genes],\n",
    "            mode=\"markers\",\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=lfc_gt[subsampled_genes],\n",
    "            #             y=np.log10(preds_iaf + 1e-12)[subsampled_genes],\n",
    "            y=preds_iaf[subsampled_genes],\n",
    "            mode=\"markers\",\n",
    "        ),\n",
    "        #         go.Scatter(\n",
    "        #             x=lfc_gt[subsampled_genes],\n",
    "        #             y=np.log10(preds_iaf_at + 1e-12)[subsampled_genes],\n",
    "        #             mode=\"markers\"\n",
    "        #         ),\n",
    "        #         go.Scatter(\n",
    "        #             x=[-0.5, -0.5], y=[-6, 0.0], mode=\"lines\", line=dict(color=\"black\", width=2)\n",
    "        #         ),\n",
    "        #         go.Scatter(\n",
    "        #             x=[0.5, 0.5], y=[-6, 0.0], mode=\"lines\", line=dict(color=\"black\", width=2)\n",
    "        #         ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_md = (\n",
    "    res_mf.loc[lambda x: (x.sample_size == 100)].sort_values(\"gene\")[\"de_proba\"].values\n",
    ")\n",
    "\n",
    "preds_iaf = (\n",
    "    res_iaf.loc[lambda x: (x.sample_size == 100)].sort_values(\"gene\")[\"de_proba\"].values\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds_iaf.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "ax = sns.kdeplot(lfc_gt, preds_md, cut=1, cmap=\"Reds\", shade=True, shade_lowest=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.kdeplot(lfc_gt, preds_iaf, cut=1, cmap=\"Blues\", shade=True, shade_lowest=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Proportion of detected genes that have LFC abs above O.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def counter(my_df):\n",
    "    df = (my_df.sort_values(\"gene\")[\"de_proba\"] >= 0.5).values\n",
    "    assert df.shape == (n_genes,)\n",
    "    return (df & (~is_significant_de)).mean()\n",
    "\n",
    "preds_md = res_mf.loc[lambda x: (x.sample_size == 100)]\n",
    "preds_iaf = res_iaf.loc[lambda x: (x.sample_size == 100)]\n",
    "\n",
    "mf_de_genes_not_sig = preds_md.groupby([\"experiment\", \"training\"]).apply(counter).values\n",
    "iaf_de_genes_not_sig = preds_iaf.groupby([\"experiment\", \"training\"]).apply(counter).values\n",
    "edger_de_genes_not_sig = ((other_predictions[\"edger\"][\"pval\"][-1] <= 0.05) & (~is_significant_de)).mean(1)\n",
    "mast_de_genes_not_sig = ((other_predictions[\"mast\"][\"pval\"][-1] <= 0.05) & (~is_significant_de)).mean(1)\n",
    "deseq2_de_genes_not_sig = ((other_predictions[\"deseq2\"][\"pval\"][-1] <= 0.05) & (~is_significant_de)).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "couples = [\n",
    "    (\"MF\", mf_de_genes_not_sig), \n",
    "    (\"IAF\", iaf_de_genes_not_sig), \n",
    "    (\"edgeR\", edger_de_genes_not_sig), \n",
    "    (\"MAST\", mast_de_genes_not_sig), \n",
    "    (\"DESeq2\", deseq2_de_genes_not_sig), \n",
    "]\n",
    "\n",
    "res = []\n",
    "for key, vals in couples:\n",
    "    res.append({\"Algorithm\": key, \"Portion\": \"$ {} $\".format(round(vals.mean(), 3))})\n",
    "my_df = pd.DataFrame(res).set_index(\"Algorithm\").loc[[\"DESeq2\", \"edgeR\", \"MAST\", \"MF\", \"IAF\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deseq2_de_genes_not_sig.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "has_lower_mean(iaf_de_genes_not_sig, mf_de_genes_not_sig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(my_df.to_latex(escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DEBUG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Uncertainty"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remarks:\n",
    "\n",
    "Problem linked to the fact that when you condition on less samples, the posterior LFC is sharper\n",
    "\n",
    "I see two solutions:\n",
    "- Voting stategy when you have many samples\n",
    "- Modification of the decision rule\n",
    "- Use posterior predicted\n",
    "- dataset is too easy ==> Add complexity\n",
    "- base decision making on credible intervals as previously\n",
    "- 3 ways classification: Upregulated, downregulated, non DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi_utils import train_model\n",
    "from scvi.utils import compute_hdi\n",
    "from plotly import graph_objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_train_params = dict(\n",
    "    ratio_loss=True,\n",
    "    test_indices=TEST_INDICES,\n",
    "    frequency=1,\n",
    "    early_stopping_kwargs={\n",
    "        \"early_stopping_metric\": \"elbo_ratio_loss\",\n",
    "        \"save_best_state_metric\": \"elbo_ratio_loss\",\n",
    "        \"patience\": 20,\n",
    "        \"threshold\": 0,\n",
    "        \"reduce_lr_on_plateau\": True,\n",
    "        \"lr_patience\": 10,\n",
    "        \"lr_factor\": 0.2,\n",
    "    },\n",
    ")\n",
    "\n",
    "my_mdl_params = {\n",
    "    \"n_hidden\": 128,\n",
    "    \"n_layers\": 1,\n",
    "    \"do_h\": True,\n",
    "    \"n_latent\": 10,\n",
    "    \"t\": 4,\n",
    "    \"n_blocks\": 2,\n",
    "    \"dropout_rate\": 0.2,\n",
    "    \"decoder_do_last_skip\": True,\n",
    "}\n",
    "\n",
    "my_train_fn_params = {\"n_epochs\": 200, \"lr\": 0.01}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_mf, trainer_mf = train_model(\n",
    "    mdl_class=VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    ")\n",
    "\n",
    "mdl_iaf, trainer_iaf = train_model(\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(trainer_iaf.history[\"elbo_ratio_loss_train_set\"][5:])\n",
    "plt.plot(trainer_iaf.history[\"elbo_ratio_loss_test_set\"][5:])\n",
    "plt.plot(trainer_mf.history[\"elbo_ratio_loss_train_set\"][5:])\n",
    "plt.plot(trainer_mf.history[\"elbo_ratio_loss_test_set\"][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.test_set.get_latents(n_samples=10, other=True, device=\"cpu\")\n",
    "z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "labels = labels.squeeze().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# z_transfo = TSNE().fit_transform(z.mean(0))\n",
    "sc = plt.scatter(z_transfo[:, 0], z_transfo[:, 1], c=labels.squeeze())\n",
    "plt.show()\n",
    "sc = plt.scatter(z_transfo[:, 0], z_transfo[:, 1], c=clusters)\n",
    "plt.scatter(z_transfo[idx_a, 0], z_transfo[idx_a, 1], c=\"blue\")\n",
    "plt.scatter(z_transfo[idx_b, 0], z_transfo[idx_b, 1], c=\"red\")\n",
    "# plt.colormaps(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vanilla**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.test_set.get_latents(n_samples=10, other=True, device=\"cpu\")\n",
    "z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "\n",
    "# Based on distances in latent\n",
    "\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "\n",
    "idx_a = np.random.choice(where_a, size=3000)\n",
    "idx_b = np.random.choice(where_b, size=3000)\n",
    "\n",
    "scales_a = scales[:, idx_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, idx_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "scales_a, scales_b = demultiply(scales_a, scales_b, 3)\n",
    "\n",
    "lfc = np.log2(scales_a) - np.log2(scales_b)\n",
    "hdis = np.array([np.percentile(lfc, q=5, axis=0), np.percentile(lfc, q=95, axis=0)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdis[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_idx = 150\n",
    "plt.hist(lfc[:, gene_idx], alpha=0.2, density=True)\n",
    "plt.hist(lfc_orig[:, gene_idx], alpha=0.2, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdis[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_idx = 0\n",
    "plt.hist(lfc[:, gene_idx], alpha=0.2, density=True)\n",
    "plt.hist(lfc_orig[:, gene_idx], alpha=0.2, density=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((hdis[:, 0] <= lfc_orig.numpy()) & (lfc_orig.numpy() <= hdis[:, 1])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "((hdis[:, 0] <= lfc_orig.numpy()) & (lfc_orig.numpy() <= hdis[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdis[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdis[:5]\n",
    "lfc_orig[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_y = graph_objects.scatter.ErrorY(\n",
    "    array=hdis[:, 1] - lfc.mean(0), arrayminus=lfc.mean(0) - hdis[:, 0]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Scatter(x=lfc_gt, y=lfc.mean(0), error_y=errs_y, mode=\"markers\"),\n",
    "        go.Scatter(\n",
    "            x=[-3, 3],\n",
    "            y=[-3, 3],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(((hdis[:, 0] <= lfc_gt) & (lfc_gt <= hdis[:, 1])).mean())\n",
    "print(((hdis[:, 0] <= lfc_orig.numpy()) & (lfc_orig.numpy() <= hdis[:, 1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Adjusted**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = trainer.train_set.get_latents(\n",
    "#     n_samples=500, other=True, device=\"cpu\"\n",
    "# )\n",
    "z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=sz)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=sz)]\n",
    "scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "lfc = np.log2(scales_a) - np.log2(scales_b)\n",
    "hdis = compute_hdi(lfc, credible_interval=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hdis = np.array([np.percentile(lfc, q=25, axis=0), np.percentile(lfc, q=75, axis=0)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hdi = hdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_hdi = np.array(\n",
    "    [\n",
    "        1.0 / np.sqrt(sz) * (hdis[:, 1] - lfc.mean(0)),\n",
    "        1.0 / np.sqrt(sz) * (lfc.mean(0) - hdis[:, 0]),\n",
    "    ]\n",
    ").T\n",
    "new_hdi[:, 0] = lfc.mean(0) - new_hdi[:, 0]\n",
    "new_hdi[:, 1] = lfc.mean(0) + new_hdi[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_idx = 182\n",
    "\n",
    "de_probas = (np.abs(lfc) >= 0.5).mean(0)\n",
    "is_pred_de = predict_de_genes(de_probas, desired_fdr=Q0)\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)\n",
    "\n",
    "plt.hist(lfc[:, gene_idx])\n",
    "plt.axvline(x=lfc_gt[gene_idx], color=\"black\")\n",
    "plt.title(de_probas[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_y = graph_objects.scatter.ErrorY(\n",
    "    array=new_hdi[:, 1] - lfc.mean(0), arrayminus=lfc.mean(0) - new_hdi[:, 0]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Scatter(x=lfc_gt, y=lfc.mean(0), error_y=errs_y, mode=\"markers\"),\n",
    "        go.Scatter(\n",
    "            x=[-3, 3],\n",
    "            y=[-3, 3],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(((new_hdi[:, 0] <= lfc_gt) & (lfc_gt <= new_hdi[:, 1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Idea adjusted experiment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for sz in tqdm([5, 10, 25, 50]):\n",
    "    for exp in range(20):\n",
    "        z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "        where_a = np.where(labels == 0)[0]\n",
    "        where_b = np.where(labels == 1)[0]\n",
    "        where_a = where_a[np.random.choice(len(where_a), size=sz)]\n",
    "        where_b = where_b[np.random.choice(len(where_b), size=sz)]\n",
    "        scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "        scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "        lfc = np.log2(scales_a) - np.log2(scales_b)\n",
    "        for level in [5, 10, 15, 20]:\n",
    "            new_level = (50.0 - level) / np.sqrt(level)\n",
    "            hdis = np.array(\n",
    "                [\n",
    "                    np.percentile(lfc, q=new_level, axis=0),\n",
    "                    np.percentile(lfc, q=100 - new_level, axis=0),\n",
    "                ]\n",
    "            ).T\n",
    "            score = ((hdis[:, 0] <= lfc_gt) & (lfc_gt <= hdis[:, 1])).mean()\n",
    "            res.append(dict(level=level, sample_size=sz, experiment=exp, score=score))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sum-related technique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take log ratios of means as f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer_iaf.test_set.get_latents(n_samples=500, other=True, device=\"cpu\")\n",
    "z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=n_cells)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=n_cells)]\n",
    "scales_a = scales[:, where_a, :]  # .reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :]  # .reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "NEW_N_SAMPLES = 3000\n",
    "n_samples = scales_a.shape[0]\n",
    "\n",
    "new_scales_a = torch.zeros((NEW_N_SAMPLES, n_cells, n_genes))\n",
    "for i in range(n_cells):\n",
    "    idx_samp = np.random.choice(a=n_samples, size=NEW_N_SAMPLES)\n",
    "    new_scales_a[:, i, :] = scales_a[idx_samp, i, :]\n",
    "new_scales_a = new_scales_a.mean(1)\n",
    "\n",
    "new_scales_b = torch.zeros((NEW_N_SAMPLES, n_cells, n_genes))\n",
    "for i in range(n_cells):\n",
    "    idx_samp = np.random.choice(a=n_samples, size=NEW_N_SAMPLES)\n",
    "    new_scales_b[:, i, :] = scales_b[idx_samp, i, :]\n",
    "new_scales_b = new_scales_b.mean(1)\n",
    "\n",
    "log_scales_a = new_scales_a.log2()\n",
    "log_scales_b = new_scales_b.log2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc = (log_scales_a - log_scales_b).numpy()\n",
    "hdis = np.array([np.percentile(lfc, q=5, axis=0), np.percentile(lfc, q=95, axis=0)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_test = h[TEST_INDICES]\n",
    "# h_a_gt = h_test[where_a]\n",
    "# h_b_gt = h_test[where_b]\n",
    "\n",
    "where_a_tot = np.where(labels == 0)[0]\n",
    "where_b_tot = np.where(labels == 1)[0]\n",
    "h_a_gt = h_test[where_a_tot].exp()\n",
    "h_b_gt = h_test[where_b_tot].exp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DEf as log of means\n",
    "lfc_gt_sum = h_a_gt.mean(0).log2() - h_b_gt.mean(0).log2()\n",
    "lfc_gt_sum = lfc_gt_sum.numpy()\n",
    "print(lfc_gt_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_y = graph_objects.scatter.ErrorY(\n",
    "    array=hdis[:, 1] - lfc.mean(0), arrayminus=lfc.mean(0) - hdis[:, 0]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Scatter(x=lfc_gt_sum, y=lfc.mean(0), error_y=errs_y, mode=\"markers\"),\n",
    "        go.Scatter(\n",
    "            x=[-3, 3],\n",
    "            y=[-3, 3],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(((hdis[:, 0] <= lfc_gt_sum) & (lfc_gt_sum <= hdis[:, 1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Median**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take Median LFC as f\n",
    "\n",
    "==> Does not work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cells = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = trainer.test_set.get_latents(\n",
    "#     n_samples=500, other=True, device=\"cpu\"\n",
    "# )\n",
    "# z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=n_cells)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=n_cells)]\n",
    "scales_a = scales[:, where_a, :]  # .reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :]  # .reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "log_scales_a = scales_a.log2()\n",
    "log_scales_b = scales_b.log2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "SZ = 1200\n",
    "\n",
    "lfc_estimate = torch.zeros((n_cells, n_cells, SZ, 1000))\n",
    "for i in tqdm(range(n_cells)):\n",
    "    for j in range(n_cells):\n",
    "        idx_a = np.random.choice(500, size=SZ)\n",
    "        idx_b = np.random.choice(500, size=SZ)\n",
    "        lfc_estimate[i, j, :] = log_scales_a[idx_a, i, :] - log_scales_b[idx_b, j, :]\n",
    "#         lfc_estimate[i, j, :] = scales_a[idx_a, i, :] - scales_b[idx_b, j, :]\n",
    "\n",
    "lfc_estimate = lfc_estimate.reshape((n_cells * n_cells, SZ, 1000)).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lfc = np.mean(lfc_estimate, axis=0)\n",
    "lfc = np.median(lfc_estimate, axis=0)\n",
    "hdis = np.array([np.percentile(lfc, q=10, axis=0), np.percentile(lfc, q=90, axis=0)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc = lfc_estimate.reshape((-1, 1000))\n",
    "hdis = np.array([np.percentile(lfc, q=25, axis=0), np.percentile(lfc, q=75, axis=0)]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "errs_y = graph_objects.scatter.ErrorY(\n",
    "    array=hdis[:, 1] - lfc.mean(0), arrayminus=lfc.mean(0) - hdis[:, 0]\n",
    ")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Scatter(x=lfc_gt, y=lfc.mean(0), error_y=errs_y, mode=\"markers\"),\n",
    "        go.Scatter(\n",
    "            x=[-3, 3],\n",
    "            y=[-3, 3],\n",
    "            mode=\"lines\",\n",
    "            line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "            showlegend=False,\n",
    "        ),\n",
    "    ]\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "print(((hdis[:, 0] <= lfc_gt) & (lfc_gt <= hdis[:, 1])).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MMD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsampled_posterior(post, indices):\n",
    "    post.data_loader.sampler.indices = indices\n",
    "    return post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer_iaf.test_set.data_loader.sampler.indices = TEST_INDICES\n",
    "trainer_mf.test_set.data_loader.sampler.indices = TEST_INDICES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_post = subsampled_posterior(trainer_mf.test_set, TEST_INDICES)\n",
    "outputs = trainer_mf.test_set.get_latents(n_samples=10, other=True, device=\"cpu\")\n",
    "labels = outputs[\"label\"]\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "idx_a = np.random.choice(where_a, size=75)\n",
    "idx_b = np.random.choice(where_b, size=75)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_a = subsampled_posterior(trainer_mf.test_set, TEST_INDICES[idx_a])\n",
    "scales_a = post_a.get_latents(n_samples=1000, other=True, device=\"cpu\")[\"scale\"]\n",
    "scales_a = scales_a.reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "post_b = subsampled_posterior(trainer_mf.test_set, TEST_INDICES[idx_b])\n",
    "scales_b = post_b.get_latents(n_samples=1000, other=True, device=\"cpu\")[\"scale\"]\n",
    "scales_b = scales_b.reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "scales_a, scales_b = demultiply(scales_a, scales_b, 3)\n",
    "lfc_mf = np.log2(scales_a) - np.log2(scales_b)\n",
    "\n",
    "post_a = subsampled_posterior(trainer_iaf.test_set, TEST_INDICES[idx_a])\n",
    "scales_a = post_a.get_latents(n_samples=1000, other=True, device=\"cpu\")[\"scale\"]\n",
    "scales_a = scales_a.reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "post_b = subsampled_posterior(trainer_iaf.test_set, TEST_INDICES[idx_b])\n",
    "scales_b = post_b.get_latents(n_samples=1000, other=True, device=\"cpu\")[\"scale\"]\n",
    "scales_b = scales_b.reshape((-1, dataset.nb_genes)).numpy()\n",
    "\n",
    "scales_a, scales_b = demultiply(scales_a, scales_b, 3)\n",
    "lfc_iaf = np.log2(scales_a) - np.log2(scales_b)\n",
    "\n",
    "# lfc_orig_gt = lfc_orig.numpy()\n",
    "# lfc_orig_gt = lfc_orig_gt[:5000]\n",
    "n_cells_a = len(idx_a)\n",
    "n_cells_b = len(idx_b)\n",
    "h_a_gt = h[TEST_INDICES][idx_a].exp().log2()\n",
    "h_b_gt = h[TEST_INDICES][idx_b].exp().log2()\n",
    "lfc_orig_gt = torch.zeros((n_cells_a, n_cells_b, 1000))\n",
    "for i in range(n_cells_a):\n",
    "    for j in range(n_cells_b):\n",
    "        lfc_orig_gt[i, j, :] = h_a_gt[i] - h_b_gt[j]\n",
    "lfc_orig_gt = lfc_orig_gt.mean((0, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc_orig_gt.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MMD protocol using a RBF kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.gaussian_process.kernels import RBF\n",
    "# from sklearn.metrics.pairwise import linear_kernel\n",
    "# from tqdm import tqdm\n",
    "\n",
    "# def compute_mmd(k, X, Y):\n",
    "#     kxx = k(X, X)\n",
    "#     kxy = k(X, Y)\n",
    "#     kyy = k(Y, Y)\n",
    "#     return (kxx.mean() + kyy.mean() - 2.0*kxy.mean())**0.5\n",
    "\n",
    "# # k = RBF()\n",
    "# k = linear_kernel\n",
    "\n",
    "# mmds_iaf = []\n",
    "# for gene in tqdm(range(n_genes)):\n",
    "#     idxa = np.random.permutation(len(lfc_iaf))[:400]\n",
    "#     idxb = np.random.permutation(len(lfc_orig_gt))[:400]\n",
    "#     x = lfc_iaf[idxa, [gene]].reshape((-1, 1))\n",
    "#     y = lfc_orig_gt[idxb, [gene]].reshape((-1, 1))\n",
    "#     mmds_iaf.append(compute_mmd(k, x, y))\n",
    "\n",
    "# mmds_mf = []\n",
    "# for gene in tqdm(range(n_genes)):\n",
    "#     idxa = np.random.permutation(len(lfc_mf))[:400]\n",
    "#     idxb = np.random.permutation(len(lfc_orig_gt))[:400]\n",
    "#     x = lfc_mf[idxa, [gene]].reshape((-1, 1))\n",
    "#     y = lfc_orig_gt[idxb, [gene]].reshape((-1, 1))\n",
    "#     mmds_mf.append(compute_mmd(k, x, y))\n",
    "\n",
    "# plt.hist(mmds_iaf, label=\"IAF\", alpha=0.5, bins=100)\n",
    "# plt.hist(mmds_mf, label=\"MF\", alpha=0.5, bins=100)\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# print(has_lower_mean(mmds_iaf, mmds_mf))\n",
    "\n",
    "# has_lower_mean(mmds_mf, mmds_iaf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in np.random.choice(n_genes, size=15):\n",
    "#     plt.hist(lfc_orig[:, i], density=True, alpha=0.5, label=\"GT\")\n",
    "    plt.axvline(x=lfc_orig_gt[i])\n",
    "    plt.hist(lfc_mf[:, i], density=True, alpha=0.5, label=\"MF\")\n",
    "    plt.hist(lfc_iaf[:, i], density=True, alpha=0.5, label=\"IAF\")\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CREDIBLE_LEVELS = [5, 10, 15, 20]\n",
    "def get_coverage(lfc_pred, lfc_ground_truth):\n",
    "    errs = []\n",
    "    for q in CREDIBLE_LEVELS:\n",
    "        hdi_low = np.percentile(lfc_pred, q=q)\n",
    "        hdi_high = np.percentile(lfc_pred, q=100 - q)\n",
    "        gene_is_covered = (lfc_ground_truth >= hdi_low) & (lfc_ground_truth <= hdi_high)\n",
    "#         mean_cov = np.mean(gene_is_covered)\n",
    "        mean_cov = gene_is_covered.numpy().mean()\n",
    "        print(2*q, mean_cov)\n",
    "        errs.append(((2*q / 100.0 - mean_cov) ** 2.0) * 0.5)\n",
    "    return pd.Series(dict(calibration_score=np.mean(errs)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coverage(lfc_mf, lfc_orig_gt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_coverage(lfc_iaf, lfc_orig_gt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR inconsistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying to use a local definition for FDR "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sz = 100\n",
    "outputs = trainer.test_set.get_latents(n_samples=2, other=True, device=\"cpu\")\n",
    "z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "idx_a = np.random.choice(where_a, size=sz)\n",
    "idx_b = np.random.choice(where_b, size=sz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IAF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = trainer.test_set.get_latents(n_samples=500, other=True, device=\"cpu\")\n",
    "z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "\n",
    "scales_a = scales[:, idx_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, idx_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_a, scales_b = demultiply(scales_a, scales_b, 3)\n",
    "lfc = np.log2(scales_a) - np.log2(scales_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MF\n",
    "outputs = trainer_mf.test_set.get_latents(n_samples=500, other=True, device=\"cpu\")\n",
    "z, labels, scales = outputs[\"z\"], outputs[\"label\"], outputs[\"scale\"]\n",
    "\n",
    "scales_a = scales[:, idx_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, idx_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_a, scales_b = demultiply(scales_a, scales_b, 3)\n",
    "lfc_mf = np.log2(scales_a) - np.log2(scales_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_mf = (np.abs(lfc_mf) >= 0.5).mean(0)\n",
    "probas_iaf = (np.abs(lfc) >= 0.5).mean(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GT\n",
    "h_a = h[TEST_INDICES][idx_a]\n",
    "h_b = h[TEST_INDICES][idx_b]\n",
    "lfcs_gt_loc = torch.zeros((sz, sz, 1000))\n",
    "for i in range(sz):\n",
    "    for j in range(sz):\n",
    "        lfcs_gt_loc[i, j, :] = h_b[i] - h_a[j]\n",
    "lfcs_gt_loc = lfcs_gt_loc.mean(dim=(0, 1)).numpy()\n",
    "# is_sig_gt_loc = np.abs(lfcs_gt_loc) >= 0.5\n",
    "is_sig_gt_loc = is_significant_de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fdr(probas):\n",
    "    sorted_genes = np.argsort(-probas)\n",
    "    sorted_pgs = probas[sorted_genes]\n",
    "    cumulative_fdr = (1.0 - sorted_pgs).cumsum() / (1.0 + np.arange(len(sorted_pgs)))\n",
    "    d = (cumulative_fdr <= 5e-2).sum() - 1\n",
    "    return cumulative_fdr, sorted_genes\n",
    "\n",
    "\n",
    "def get_fdr_gt(my_sorted_genes, is_sig_gt_loc):\n",
    "    fdr_k = []\n",
    "    for k in range(n_genes):\n",
    "        predictions = np.zeros(n_genes)\n",
    "        predictions[my_sorted_genes[: (k + 1)]] = 1\n",
    "        fdr = ((~is_sig_gt_loc) * predictions).sum() / (k + 1)\n",
    "        fdr_k.append(fdr)\n",
    "    return np.array(fdr_k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_fdr, sorted_genes = get_fdr(probas_iaf)\n",
    "fdr_gt = get_fdr_gt(sorted_genes, is_sig_gt_loc)\n",
    "plt.plot(cumulative_fdr, label=\"PRED\")\n",
    "plt.plot(fdr_gt, label=\"GT\")\n",
    "plt.legend()\n",
    "d = (cumulative_fdr <= 5e-2).sum() - 1\n",
    "print(fdr_gt[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_fdr, sorted_genes = get_fdr(probas_mf)\n",
    "fdr_gt = get_fdr_gt(sorted_genes, is_sig_gt_loc)\n",
    "plt.plot(cumulative_fdr, label=\"PRED\")\n",
    "plt.plot(fdr_gt, label=\"GT\")\n",
    "plt.legend()\n",
    "d = (cumulative_fdr <= 5e-2).sum() - 1\n",
    "print(fdr_gt[d])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_fdr, sorted_genes = get_fdr(probas_iaf)\n",
    "fdr_gt = get_fdr_gt(sorted_genes, is_sig_gt_loc)\n",
    "plt.plot(cumulative_fdr, label=\"PRED\")\n",
    "plt.plot(fdr_gt, label=\"GT\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scales_a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "# z, labels, scales = trainer.test_set.get_latents(\n",
    "#     n_samples=50, other=True, device=\"cpu\"\n",
    "# )\n",
    "\n",
    "# labels = labels.squeeze()\n",
    "# where_a = np.where(labels == 0)[0]\n",
    "# where_b = np.where(labels == 1)[0]\n",
    "# where_a = where_a[np.random.choice(len(where_a), size=100)]\n",
    "# where_b = where_b[np.random.choice(len(where_b), size=100)]\n",
    "# scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "# lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "\n",
    "# de_probas = (np.abs(lfc) >= 0.5).mean(0)\n",
    "# is_pred_de = predict_de_genes(de_probas, desired_fdr=Q0)\n",
    "# alpha = is_pred_de[is_pred_de].min()\n",
    "\n",
    "# true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "# n_positives = is_significant_de.sum()\n",
    "# true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "# print(true_fdr, true_fnr)\n",
    "\n",
    "# plt.hist(lfc[:, gene_idx])\n",
    "# plt.axvline(x=true_lfc, color=\"black\")\n",
    "# plt.title(de_probas[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probas_thresh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, labels, scales = trainer.test_set.get_latents(\n",
    "    n_samples=500, other=True, device=\"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 2\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=100)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=100)]\n",
    "scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "\n",
    "de_probas = (np.abs(lfc) >= 0.5).mean(0)\n",
    "de_probas_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "\n",
    "# is_pred_de = predict_de_genes(de_probas, desired_fdr=Q0)\n",
    "is_pred_de = de_probas >= 0.5\n",
    "# probas_thresh = -np.sort(-de_probas)[215]\n",
    "# is_pred_de = de_probas >= probas_thresh\n",
    "\n",
    "alpha = is_pred_de[is_pred_de].min()\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)\n",
    "\n",
    "plt.hist(lfc[:, gene_idx])\n",
    "plt.axvline(x=true_lfc, color=\"black\")\n",
    "plt.title(de_probas[gene_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RÃ©sultats pour decision >= 0.5\n",
    "\n",
    "**100 cells**\n",
    "0.23706896551724138 0.019390581717451522\n",
    "\n",
    "0.2886178861788618 0.030470914127423823\n",
    "\n",
    "\n",
    "\n",
    "**5 cellules**\n",
    "0.42448979591836733 0.2188365650969529\n",
    "\n",
    "0.12435233160621761 0.06371191135734072\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision making based on credible intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = np.percentile(lfc, q=[2.5, 97.5], axis=0)\n",
    "\n",
    "is_pred_de = (np.abs(low) >= 0.5) & (np.abs(high) >= 0.5) & (low * high >= 0.0)\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "# n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Option 1\n",
    "# z, labels, scales = trainer.test_set.get_latents(\n",
    "#     n_samples=50, other=True, device=\"cpu\"\n",
    "# )\n",
    "\n",
    "# labels = labels.squeeze()\n",
    "# where_a = np.where(labels == 0)[0]\n",
    "# where_b = np.where(labels == 1)[0]\n",
    "# where_a = where_a[np.random.choice(len(where_a), size=5)]\n",
    "# where_b = where_b[np.random.choice(len(where_b), size=5)]\n",
    "# scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "# lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "# de_probas_small = (np.abs(lfc) >= 0.5).mean(0)\n",
    "\n",
    "\n",
    "# is_pred_de = predict_de_genes(de_probas_small, desired_fdr=Q0)\n",
    "# alpha = is_pred_de[is_pred_de].min()\n",
    "# true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "# n_positives = is_significant_de.sum()\n",
    "# true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "# print(true_fdr, true_fnr)\n",
    "\n",
    "# plt.hist(lfc[:, gene_idx])\n",
    "# plt.axvline(x=true_lfc, color=\"black\")\n",
    "# plt.title(de_probas_small[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "# z, labels, scales = trainer.test_set.get_latents(\n",
    "#     n_samples=1000, other=True, device=\"cpu\"\n",
    "# )\n",
    "\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=5)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=5)]\n",
    "scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "de_probas_small = (np.abs(lfc) >= 0.5).mean(0)\n",
    "de_probas_small_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "# is_pred_de_small = predict_de_genes(de_probas_small, desired_fdr=Q0)\n",
    "is_pred_de_small = de_probas_small >= 0.5\n",
    "alpha = is_pred_de_small[is_pred_de_small].min()\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de_small).sum() / is_pred_de_small.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de_small)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)\n",
    "\n",
    "plt.hist(lfc[:, gene_idx])\n",
    "plt.axvline(x=true_lfc, color=\"black\")\n",
    "plt.title(de_probas_small[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "low, high = np.percentile(lfc, q=[2.5, 97.5], axis=0)\n",
    "\n",
    "is_pred_de = (np.abs(low) >= 0.5) & (np.abs(high) >= 0.5) & (low * high >= 0.0)\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Option 1\n",
    "z, labels, scales = trainer.test_set.get_latents(\n",
    "    n_samples=2000, other=True, device=\"cpu\"\n",
    ")\n",
    "\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=1)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=1)]\n",
    "scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=10)\n",
    "lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "de_probas_small = (np.abs(lfc) >= 0.5).mean(0)\n",
    "de_probas_small_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "is_pred_de_small = predict_de_genes(de_probas_small, desired_fdr=Q0)\n",
    "# is_pred_de_small = de_probas_small >= 0.5\n",
    "alpha = is_pred_de_small[is_pred_de_small].min()\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de_small).sum() / is_pred_de_small.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de_small)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)\n",
    "\n",
    "plt.hist(lfc[:, gene_idx])\n",
    "plt.axvline(x=true_lfc, color=\"black\")\n",
    "plt.title(de_probas_small[gene_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(is_pred_de.sum())\n",
    "print(is_pred_de_small.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.title(\"Empirical distribution of predicted probabilities of being DE\")\n",
    "\n",
    "plt.hist(de_probas_small, alpha=0.25, label=\"5 cells\")\n",
    "plt.hist(de_probas, alpha=0.25, label=\"100 cells\")\n",
    "\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(de_probas_std, alpha=0.25)\n",
    "plt.hist(de_probas_small_std, alpha=0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###Â Voting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_votes = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "z, labels, scales = trainer.test_set.get_latents(\n",
    "    n_samples=2000, other=True, device=\"cpu\"\n",
    ")\n",
    "\n",
    "labels = labels.squeeze()\n",
    "where_a = np.where(labels == 0)[0]\n",
    "where_b = np.where(labels == 1)[0]\n",
    "where_a = where_a[np.random.choice(len(where_a), size=100)]\n",
    "where_b = where_b[np.random.choice(len(where_b), size=100)]\n",
    "scales_a_all = scales[:, where_a, :].numpy()\n",
    "scales_b_all = scales[:, where_b, :].numpy()\n",
    "\n",
    "all_votes = np.zeros((n_votes, n_genes))\n",
    "for vote in tqdm(range(n_votes)):\n",
    "    where_a = np.random.choice(100, size=1)\n",
    "    where_b = np.random.choice(100, size=1)\n",
    "    scales_a = scales_a_all[:, where_a, :].reshape((-1, n_genes))\n",
    "    scales_b = scales_b_all[:, where_b, :].reshape((-1, n_genes))\n",
    "\n",
    "    scales_ab, scales_bb = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "    lfc = np.log2(scales_ab) - np.log2(scales_bb)\n",
    "\n",
    "    de_probas = (np.abs(lfc) >= 0.5).mean(0)\n",
    "    de_probas_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "\n",
    "    is_pred_de = predict_de_genes(de_probas, desired_fdr=Q0)\n",
    "\n",
    "    all_votes[vote, :] = is_pred_de\n",
    "\n",
    "#     de_probas_small_std = (np.abs(lfc) >= 0.5).std(0)\n",
    "#     alpha = is_pred_de[is_pred_de].min()\n",
    "\n",
    "#     true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "#     n_positives = is_significant_de.sum()\n",
    "#     true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "#     print(true_fdr, true_fnr)\n",
    "\n",
    "# plt.hist(lfc[:, gene_idx])\n",
    "# plt.axvline(x=true_lfc, color=\"black\")\n",
    "# plt.title(de_probas[gene_idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "is_pred_de_vote = all_votes.mean(0) >= 0.5\n",
    "\n",
    "true_fdr = ((1.0 - is_significant_de) * is_pred_de_vote).sum() / is_pred_de_vote.sum()\n",
    "n_positives = is_significant_de.sum()\n",
    "true_fnr = (is_significant_de * (1.0 - is_pred_de_vote)).sum() / n_positives\n",
    "print(true_fdr, true_fnr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(all_votes.mean(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Credible intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_iaf.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fnr_fdr(my_df):\n",
    "    my_is_pred_de = my_df.is_pred_de\n",
    "    true_fdr = ((1.0 - is_significant_de) * my_is_pred_de).sum() / my_is_pred_de.sum()\n",
    "    n_positives = is_significant_de.sum()\n",
    "    true_fnr = (is_significant_de * (1.0 - my_is_pred_de)).sum() / n_positives\n",
    "    return pd.Series(dict(fdr=true_fdr, fnr=true_fnr))\n",
    "\n",
    "\n",
    "(\n",
    "    res_iaf.assign(\n",
    "        is_pred_de=lambda x: (x.hdi64_low.abs() >= 0.5)\n",
    "        & (x.hdi64_high.abs() >= 0.5)\n",
    "        & (x.hdi64_low * x.hdi64_high >= 0.0)\n",
    "    )\n",
    "    .groupby([\"training\", \"algorithm\", \"sample_size\", \"experiment\"])\n",
    "    .apply(fnr_fdr)\n",
    "    .reset_index()\n",
    "    .groupby([\"sample_size\"])\n",
    "    .agg(dict(fdr=[\"mean\", \"std\"], fnr=[\"mean\", \"std\"]))\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Environment (conda_scvi)",
   "language": "python",
   "name": "conda_scvi"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "758.976px",
    "left": "40.7569px",
    "top": "109.722px",
    "width": "271.997px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
