{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt\n",
      "Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from dca.api import dca\n",
    "import anndata\n",
    "\n",
    "from benchmarking import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression data\n",
    "data_path = \"/home/ubuntu/single-cell-scVI/data/10xPBMCs/\"\n",
    "expression_train = np.load(data_path + \"de/data_train.npy\")\n",
    "expression_test = np.load(data_path + \"de/data_test.npy\")\n",
    "\n",
    "# qc metrics\n",
    "r_train = np.load(data_path + \"design_train.npy\")\n",
    "r_test = np.load(data_path + \"design_test.npy\")\n",
    "\n",
    "# labels\n",
    "c_train = np.loadtxt(data_path + \"label_train\")\n",
    "c_test = np.loadtxt(data_path + \"label_test\")\n",
    "\n",
    "# batch info\n",
    "b_train = np.loadtxt(data_path + \"b_train\")\n",
    "b_test = np.loadtxt(data_path + \"b_test\")\n",
    "\n",
    "# corrupted data\n",
    "X_zero, i, j, ix = \\\n",
    "        np.load(data_path + \"imputation/X_zero.npy\"), np.load(data_path + \"imputation/i.npy\"),\\\n",
    "        np.load(data_path + \"imputation/j.npy\"), np.load(data_path + \"imputation/ix.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCA: Successfully preprocessed 3346 genes and 9029 cells.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 3346)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           214208      count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 10)           650         enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 10)           30          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 10)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           704         center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (None, 3346)         0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 3346)         0           output[0][0]                     \n",
      "                                                                 dispersion[0][0]                 \n",
      "                                                                 pi[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 868,446\n",
      "Trainable params: 868,170\n",
      "Non-trainable params: 276\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8126 samples, validate on 903 samples\n",
      "Epoch 1/300\n",
      "8126/8126 [==============================] - 9s 1ms/step - loss: 0.4408 - val_loss: 0.4013\n",
      "Epoch 2/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.4029 - val_loss: 0.3954\n",
      "Epoch 3/300\n",
      "8126/8126 [==============================] - 8s 999us/step - loss: 0.3996 - val_loss: 0.3936\n",
      "Epoch 4/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3981 - val_loss: 0.3935\n",
      "Epoch 5/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3973 - val_loss: 0.3924\n",
      "Epoch 6/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3967 - val_loss: 0.3919\n",
      "Epoch 7/300\n",
      "8126/8126 [==============================] - 8s 1000us/step - loss: 0.3963 - val_loss: 0.3921\n",
      "Epoch 8/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3961 - val_loss: 0.3917\n",
      "Epoch 9/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3957 - val_loss: 0.3922\n",
      "Epoch 10/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3954 - val_loss: 0.3911\n",
      "Epoch 11/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3953 - val_loss: 0.3917\n",
      "Epoch 12/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3952 - val_loss: 0.3913\n",
      "Epoch 13/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3950 - val_loss: 0.3914\n",
      "Epoch 14/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3949 - val_loss: 0.3919\n",
      "Epoch 15/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3947 - val_loss: 0.3918\n",
      "Epoch 16/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3946 - val_loss: 0.3910\n",
      "Epoch 17/300\n",
      "8126/8126 [==============================] - 8s 999us/step - loss: 0.3946 - val_loss: 0.3910\n",
      "Epoch 18/300\n",
      "8126/8126 [==============================] - 8s 999us/step - loss: 0.3944 - val_loss: 0.3910\n",
      "Epoch 19/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3948 - val_loss: 0.3920\n",
      "Epoch 20/300\n",
      "8126/8126 [==============================] - 8s 1000us/step - loss: 0.3946 - val_loss: 0.3915\n",
      "Epoch 21/300\n",
      "8126/8126 [==============================] - 8s 996us/step - loss: 0.3942 - val_loss: 0.3908\n",
      "Epoch 22/300\n",
      "8126/8126 [==============================] - 8s 999us/step - loss: 0.3942 - val_loss: 0.3911\n",
      "Epoch 23/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3942 - val_loss: 0.3912\n",
      "Epoch 24/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3941 - val_loss: 0.3913\n",
      "Epoch 25/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3941 - val_loss: 0.3912\n",
      "Epoch 26/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3942 - val_loss: 0.3918\n",
      "Epoch 27/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3941 - val_loss: 0.3913\n",
      "Epoch 28/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3941 - val_loss: 0.3914\n",
      "Epoch 29/300\n",
      "8126/8126 [==============================] - 8s 997us/step - loss: 0.3943 - val_loss: 0.3914\n",
      "Epoch 30/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3940 - val_loss: 0.3916\n",
      "Epoch 31/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3940 - val_loss: 0.3916\n",
      "\n",
      "Epoch 00031: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 32/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3935 - val_loss: 0.3909\n",
      "Epoch 33/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3932 - val_loss: 0.3908\n",
      "Epoch 34/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3931 - val_loss: 0.3909\n",
      "Epoch 35/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3932 - val_loss: 0.3909\n",
      "Epoch 36/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3931 - val_loss: 0.3909\n",
      "Epoch 00036: early stopping\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "train = anndata.AnnData(expression_train)\n",
    "res = dca(train, verbose=True, mode=\"latent\", hidden_size=(64, 10, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.374523, 0.6921303361231197, 0.5549110822129342]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LATENT 10\n",
    "latent = train.obsm['X_dca']\n",
    "cluster_scores(latent, len(np.unique(c_train)), c_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.37623665, 0.7574129556811758, 0.6677622883585378]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# LATENT 32 \n",
    "latent = train.obsm['X_dca']\n",
    "cluster_scores(latent, len(np.unique(c_train)), c_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCA: Successfully preprocessed 3346 genes and 9029 cells.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 3346)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           214208      count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (None, 3346)         0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 3346)         0           output[0][0]                     \n",
      "                                                                 dispersion[0][0]                 \n",
      "                                                                 pi[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 871,350\n",
      "Trainable params: 871,030\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8126 samples, validate on 903 samples\n",
      "Epoch 1/300\n",
      "8126/8126 [==============================] - 13s 2ms/step - loss: 0.4158 - val_loss: 0.3808\n",
      "Epoch 2/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3815 - val_loss: 0.3748\n",
      "Epoch 3/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3782 - val_loss: 0.3733\n",
      "Epoch 4/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3767 - val_loss: 0.3729\n",
      "Epoch 5/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3758 - val_loss: 0.3720\n",
      "Epoch 6/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3752 - val_loss: 0.3718\n",
      "Epoch 7/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3746 - val_loss: 0.3718\n",
      "Epoch 8/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3743 - val_loss: 0.3715\n",
      "Epoch 9/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3739 - val_loss: 0.3714\n",
      "Epoch 10/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3735 - val_loss: 0.3710\n",
      "Epoch 11/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3733 - val_loss: 0.3714\n",
      "Epoch 12/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3730 - val_loss: 0.3718\n",
      "Epoch 13/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3728 - val_loss: 0.3708\n",
      "Epoch 14/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3726 - val_loss: 0.3719\n",
      "Epoch 15/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3724 - val_loss: 0.3711\n",
      "Epoch 16/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3723 - val_loss: 0.3711\n",
      "Epoch 17/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3722 - val_loss: 0.3709\n",
      "Epoch 18/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3720 - val_loss: 0.3713\n",
      "Epoch 19/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3722 - val_loss: 0.3724\n",
      "Epoch 20/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3721 - val_loss: 0.3724\n",
      "Epoch 21/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3717 - val_loss: 0.3716\n",
      "Epoch 22/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3717 - val_loss: 0.3706\n",
      "Epoch 23/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3716 - val_loss: 0.3731\n",
      "Epoch 24/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3715 - val_loss: 0.3716\n",
      "Epoch 25/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3715 - val_loss: 0.3710\n",
      "Epoch 26/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3715 - val_loss: 0.3720\n",
      "Epoch 27/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3714 - val_loss: 0.3719\n",
      "Epoch 28/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3715 - val_loss: 0.3717\n",
      "Epoch 29/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3716 - val_loss: 0.3716\n",
      "Epoch 30/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3713 - val_loss: 0.3717\n",
      "Epoch 31/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3713 - val_loss: 0.3714\n",
      "Epoch 32/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3713 - val_loss: 0.3725\n",
      "\n",
      "Epoch 00032: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 33/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3706 - val_loss: 0.3712\n",
      "Epoch 34/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3704 - val_loss: 0.3714\n",
      "Epoch 35/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3705 - val_loss: 0.3713\n",
      "Epoch 36/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3703 - val_loss: 0.3713\n",
      "Epoch 37/300\n",
      "8126/8126 [==============================] - 12s 2ms/step - loss: 0.3703 - val_loss: 0.3714\n",
      "Epoch 00037: early stopping\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "train = anndata.AnnData(X_zero)\n",
    "res = dca(train, verbose=True, mode=\"denoise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8247501850128174"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_index = i[ix], j[ix]\n",
    "x, y = train.X[all_index], expression_train[all_index]\n",
    "np.median(np.abs(x - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### denoise for DE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCA: Successfully preprocessed 3346 genes and 9029 cells.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 3346)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           214208      count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (None, 3346)         0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 3346)         217490      dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 3346)         0           output[0][0]                     \n",
      "                                                                 dispersion[0][0]                 \n",
      "                                                                 pi[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 871,350\n",
      "Trainable params: 871,030\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 8126 samples, validate on 903 samples\n",
      "Epoch 1/300\n",
      "8126/8126 [==============================] - 13s 2ms/step - loss: 0.4402 - val_loss: 0.4016\n",
      "Epoch 2/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.4017 - val_loss: 0.3947\n",
      "Epoch 3/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3982 - val_loss: 0.3931\n",
      "Epoch 4/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3966 - val_loss: 0.3925\n",
      "Epoch 5/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3956 - val_loss: 0.3912\n",
      "Epoch 6/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3949 - val_loss: 0.3910\n",
      "Epoch 7/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3942 - val_loss: 0.3905\n",
      "Epoch 8/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3939 - val_loss: 0.3907\n",
      "Epoch 9/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3934 - val_loss: 0.3904\n",
      "Epoch 10/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3929 - val_loss: 0.3900\n",
      "Epoch 11/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3927 - val_loss: 0.3906\n",
      "Epoch 12/300\n",
      "8126/8126 [==============================] - 12s 1ms/step - loss: 0.3925 - val_loss: 0.3897\n",
      "Epoch 13/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3898 - val_loss: 0.3900\n",
      "Epoch 29/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3899 - val_loss: 0.3899\n",
      "Epoch 30/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3896 - val_loss: 0.3897\n",
      "Epoch 31/300\n",
      "8126/8126 [==============================] - 8s 1ms/step - loss: 0.3897 - val_loss: 0.3901\n",
      "Epoch 00031: early stopping\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "train = anndata.AnnData(expression_train)\n",
    "res = dca(train, verbose=True, mode=\"denoise\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(data_path + \"de/dca_data_train\", train.X.astype(np.int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
