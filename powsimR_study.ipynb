{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objs as go\n",
    "import plotly.express as px\n",
    "import plotly as py\n",
    "import pandas as pd\n",
    "from chart_studio.plotly import plot, iplot\n",
    "\n",
    "# from plotly.offline import init_notebook_mode, iplot\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "from scvi.dataset import PowSimSynthetic, LatentLogPoissonDataset\n",
    "from scvi.models import VAE, IAVAE\n",
    "from scvi.inference import UnsupervisedTrainer\n",
    "from scvi.utils import demultiply, make_dir_if_necessary, predict_de_genes, save_fig, load_pickle, save_pickle, has_lower_mean\n",
    "from scvi_utils import estimate_de_proba, estimate_lfc_density, estimate_lfc_mean, multi_train_estimates\n",
    "from R_interop import all_predictions, all_de_predictions\n",
    "\n",
    "\n",
    "N_EPOCHS = 100\n",
    "DELTA = 0.5\n",
    "SIZES = [5, 10, 20, 30, 50, 100]\n",
    "SIZE = 100\n",
    "N_SIZES = len(SIZES)\n",
    "DO_CLOUD = True\n",
    "Q0 = 5e-2\n",
    "N_TRAININGS = 5\n",
    "N_PICKS = 10\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "PATH_TO_SCRIPTS = \"/home/ubuntu/conquer_comparison/scripts\"\n",
    "DIR_PATH = 'lfc_estimates/powsimr2'\n",
    "make_dir_if_necessary(DIR_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chart_studio.plotly as py\n",
    "py.sign_in(\"pierreboyeau\", \"2wvdnWZ2Qut1zD07ADVy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = os.path.join(DIR_PATH, \"dataset.pickle\")\n",
    "if not os.path.exists(dataset_path):\n",
    "    dataset = PowSimSynthetic(\n",
    "        cluster_to_samples=[7500, 7500],\n",
    "        de_p=0.5,\n",
    "        n_genes=1500,\n",
    "        mode=\"NB\"\n",
    "    )\n",
    "    save_pickle(dataset, filename=dataset_path)\n",
    "else:\n",
    "    dataset = load_pickle(filename=dataset_path)\n",
    "\n",
    "is_significant_de = np.abs(dataset.lfc[:, 1] - dataset.lfc[:, 0]) >= DELTA\n",
    "n_genes = dataset.nb_genes\n",
    "trace1 = go.Histogram(x=dataset.lfc[:, 1] - dataset.lfc[:, 0])\n",
    "fig = go.Figure(data=[trace1])\n",
    "# save_fig(fig, filename=\"powsimR_properties\", do_cloud=DO_CLOUD)\n",
    "# fig.show()\n",
    "iplot(fig, filename=\"powsimR_properties\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_examples = len(dataset)\n",
    "TEST_INDICES = np.random.permutation(n_examples)[:2000]\n",
    "\n",
    "x_test, y_test = dataset.X[TEST_INDICES, :], dataset.labels[TEST_INDICES, :].squeeze()\n",
    "data_path = os.path.join(DIR_PATH, 'data.npy')\n",
    "labels_path = os.path.join(DIR_PATH, 'labels.npy')\n",
    "\n",
    "np.save(\n",
    "    data_path,\n",
    "    x_test.squeeze().astype(int)\n",
    ")\n",
    "np.savetxt(\n",
    "    labels_path,\n",
    "    y_test.squeeze()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.where(y_test == 1)[0][:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mdl_params = dict(\n",
    "    iaf=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4, dropout_rate=0.3),\n",
    "    iaf_b=dict(n_hidden=128, n_layers=2, do_h=False, n_latent=10, t=3, dropout_rate=0.3),\n",
    "    mf=dict(n_hidden=128, n_layers=1, n_latent=10, dropout_rate=0.3),\n",
    "    iaf_k5=dict(n_hidden=128, n_layers=1, do_h=True, n_latent=10, t=4),\n",
    "    mf_k5=dict(n_hidden=128, n_layers=1, n_latent=10),\n",
    ")\n",
    "train_params = dict(\n",
    "    iaf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    iaf_b=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    mf=dict(ratio_loss=True, test_indices=TEST_INDICES),\n",
    "    iaf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5, single_backward=True),\n",
    "    mf_k5=dict(ratio_loss=True, test_indices=TEST_INDICES, k_importance_weighted=5, single_backward=True)\n",
    ")\n",
    "train_fn_params = dict(\n",
    "    iaf=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    iaf_b=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    mf=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    iaf_k5=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    "    mf_k5=dict(n_epochs=N_EPOCHS, lr=1e-3),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compute competitors scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions = all_predictions(\n",
    "    filename=os.path.join(DIR_PATH, \"other_predictions_final.pickle\"),\n",
    "    n_genes=n_genes, \n",
    "    n_picks=N_PICKS, \n",
    "    sizes=SIZES, \n",
    "    data_path=data_path, \n",
    "    labels_path=labels_path,\n",
    "    path_to_scripts=PATH_TO_SCRIPTS\n",
    ")\n",
    "\n",
    "other_predictions = all_de_predictions(\n",
    "    other_predictions, significance_level=Q0, delta=DELTA\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check sign of LFC "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scvi.utils import plot_identity\n",
    "\n",
    "lfc_gt = -(dataset.lfc[:, 1] - dataset.lfc[:, 0])\n",
    "plt.scatter(lfc_gt, -other_predictions[\"edger\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "plt.scatter(lfc_gt, other_predictions[\"deseq2\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "plt.scatter(lfc_gt, -other_predictions[\"mast\"][\"lfc\"][-1, -1, :])\n",
    "plot_identity()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "other_predictions[\"edger\"][\"lfc\"] = -other_predictions[\"edger\"][\"lfc\"]\n",
    "other_predictions[\"mast\"][\"lfc\"] = -other_predictions[\"mast\"][\"lfc\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir(DIR_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# res_mf = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf1.pickle\"),\n",
    "#     mdl_class=VAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"mf\"],\n",
    "#     train_params=train_params[\"mf\"],\n",
    "#     train_fn_params=train_fn_params[\"mf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1\n",
    "# ).assign(algorithm=\"MF\")\n",
    "\n",
    "# res_iaf = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_iaf1.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf\"],\n",
    "#     train_params=train_params[\"iaf\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf\"],\n",
    "#     sizes=SIZES,\n",
    "#     n_trainings=N_TRAININGS,\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_samples=500,\n",
    "#     label_a=0,\n",
    "#     label_b=1\n",
    "# ).assign(algorithm=\"IAF\")\n",
    "\n",
    "res_mf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_mf.pickle\"),\n",
    "    mdl_class=VAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"mf\"],\n",
    "    train_params=train_params[\"mf\"],\n",
    "    train_fn_params=train_fn_params[\"mf\"],\n",
    "    sizes=SIZES,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    label_b=1\n",
    ").assign(algorithm=\"MF\")\n",
    "\n",
    "res_iaf = multi_train_estimates(\n",
    "    filename=os.path.join(DIR_PATH, \"res_iaf.pickle\"),\n",
    "    mdl_class=IAVAE,\n",
    "    dataset=dataset,\n",
    "    mdl_params=mdl_params[\"iaf\"],\n",
    "    train_params=train_params[\"iaf\"],\n",
    "    train_fn_params=train_fn_params[\"iaf\"],\n",
    "    sizes=SIZES,\n",
    "    n_trainings=N_TRAININGS,\n",
    "    n_picks=N_PICKS,\n",
    "    n_samples=500,\n",
    "    label_a=0,\n",
    "    label_b=1\n",
    ").assign(algorithm=\"IAF\")\n",
    "\n",
    "# res_mfk5 = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_mf_k5_final1.pickle\"),\n",
    "#     mdl_class=VAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"mf_k5\"],\n",
    "#     train_params=train_params[\"mf_k5\"],\n",
    "#     train_fn_params=train_fn_params[\"mf_k5\"],\n",
    "#     sizes=[SIZE],\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_trainings=N_TRAININGS\n",
    "# ).assign(algorithm=\"MF K5\")\n",
    "\n",
    "# res_iak5 = multi_train_estimates(\n",
    "#     filename=os.path.join(DIR_PATH, \"res_ia_k5_final1.pickle\"),\n",
    "#     mdl_class=IAVAE,\n",
    "#     dataset=dataset,\n",
    "#     mdl_params=mdl_params[\"iaf_k5\"],\n",
    "#     train_params=train_params[\"iaf_k5\"],\n",
    "#     train_fn_params=train_fn_params[\"iaf_k5\"],\n",
    "#     sizes=[SIZE],\n",
    "#     n_picks=N_PICKS,\n",
    "#     n_trainings=1\n",
    "# ).assign(algorithm=\"IAF K5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FDR / Power Control and PR Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(\n",
    "    mdl_class, dataset, mdl_params: dict, train_params: dict, train_fn_params: dict\n",
    "):\n",
    "    \"\"\"\n",
    "\n",
    "    :param mdl_class: Class of algorithm\n",
    "    :param dataset: Dataset\n",
    "    :param mdl_params:\n",
    "    :param train_params:\n",
    "    :param train_fn_params:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    my_vae = mdl_class(dataset.nb_genes, n_batch=dataset.n_batches, **mdl_params)\n",
    "    my_trainer = UnsupervisedTrainer(my_vae, dataset, **train_params)\n",
    "    print(my_trainer.test_set.data_loader.sampler.indices)\n",
    "    my_trainer.train(**train_fn_params)\n",
    "    print(my_trainer.train_losses)\n",
    "    return my_vae, my_trainer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### FDR and TPR Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fdr_fnr(my_df):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    assert len(my_df) == n_genes\n",
    "    is_pred_de = predict_de_genes(my_df.de_proba.values, desired_fdr=Q0)\n",
    "    \n",
    "    alpha = my_df.de_proba.values[is_pred_de].min()\n",
    "#     alpha = 0.8\n",
    "#     is_pred_de = my_df.de_proba.values >= 0.4\n",
    "    true_fdr = ((1.0 - is_significant_de) * is_pred_de).sum() / is_pred_de.sum()\n",
    "    n_positives = is_significant_de.sum()\n",
    "    true_fnr = (is_significant_de * (1.0 - is_pred_de)).sum() / n_positives\n",
    "    return pd.Series(dict(fdr=true_fdr, fnr=true_fnr, alpha=alpha))\n",
    "\n",
    "\n",
    "fdr_fnr_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "fdr_fnr_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(fdr_fnr)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "\n",
    "df = pd.concat([fdr_fnr_mf, fdr_fnr_iaf], ignore_index=True)\n",
    "\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fdr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Discovery Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_fdr_control\")\n",
    "\n",
    "fig = px.box(\n",
    "    df,\n",
    "    x=\"sample_size\",\n",
    "    y=\"fnr\",\n",
    "    color=\"algorithm\",\n",
    "    title=\"Control on False Negative Rate\",\n",
    ")\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"powsimr_power_control\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alpha comparisons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_fnr_iaf.groupby(\"sample_size\").alpha.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fdr_fnr_mf.groupby(\"sample_size\").alpha.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ['deseq2', 'edger', 'mast']\n",
    "\n",
    "def get_fdr_fnr(y_pred, y_true):\n",
    "    \"\"\"\n",
    "        y_pred: (n_sz, n_picks, n_genes) bool predictions\n",
    "        y_true: (n_genes) gt vals\n",
    "    \"\"\"\n",
    "    n_sz, n_picks, _ = y_pred.shape\n",
    "    fnrs = np.zeros((n_sz, n_picks))\n",
    "    fdrs = np.zeros((n_sz, n_picks))\n",
    "    for sz in range(n_sz):\n",
    "        for pick in range(n_picks):\n",
    "            y_pred_it = y_pred[sz, pick, :]\n",
    "            fnr = ((~y_true) * y_pred_it).sum() / y_pred_it.sum()\n",
    "            fdr = (y_true * (~y_pred_it)).sum() / y_true.sum()\n",
    "            fnrs[sz, pick] = fnr\n",
    "            fdrs[sz, pick] = fdr\n",
    "    fnrs[np.isnan(fnrs)] = 0.0\n",
    "    return dict(fnr=fnrs, fdr=fdrs)\n",
    "\n",
    "print(other_predictions[\"mast\"]['pval'].shape)\n",
    "print(other_predictions[\"deseq2\"]['pval'].shape)\n",
    "print(other_predictions[\"edger\"]['pval'].shape)\n",
    "\n",
    "is_de_mast = other_predictions[\"mast\"][\"is_de\"]\n",
    "is_de_deseq2 = other_predictions[\"deseq2\"][\"is_de\"]\n",
    "is_de_edger = other_predictions[\"edger\"][\"is_de\"]\n",
    "# is_de_edgerr = other_predictions[\"edger_robust\"][\"is_de\"]\n",
    "\n",
    "\n",
    "res_mast = get_fdr_fnr(is_de_mast, y_true=is_significant_de)\n",
    "res_deseq2 = get_fdr_fnr(is_de_deseq2, y_true=is_significant_de)\n",
    "res_edger = get_fdr_fnr(is_de_edger, y_true=is_significant_de)\n",
    "# res_edgerr = get_fdr_fnr(is_de_edgerr, y_true=is_significant_de)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf = res_mf[(res_mf.experiment == 0) & (res_mf.training == 0) & (res_mf.sample_size == 100)]\n",
    "preds_iaf = res_iaf[(res_iaf.experiment == 0) & (res_iaf.training == 0) & (res_iaf.sample_size == 100)]\n",
    "\n",
    "# preds_mf = preds_mf.sort_values(\"de_proba\").set_index(\"gene\")\n",
    "# preds_iaf = preds_iaf.set_index(\"gene\").reindex(index=preds_mf.index)\n",
    "# preds_iaf[]\n",
    "\n",
    "preds = pd.concat([preds_mf, preds_iaf], ignore_index=True)\n",
    "preds.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_mf = preds_mf.assign(\n",
    "    de_proba_iaf=preds_iaf.de_proba,\n",
    "    gene_mean=dataset.X.mean(0),\n",
    "    is_de=is_significant_de.astype(float),\n",
    ")\n",
    "\n",
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [preds_mf[\"de_proba\"], preds_mf[\"de_proba_iaf\"]],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.figure_factory as ff\n",
    "\n",
    "fig = ff.create_distplot(\n",
    "    [preds_mf[\"de_proba\"], preds_mf[\"de_proba_iaf\"]],\n",
    "    [\"de_proba\", \"de_proba_iaf\"],\n",
    "    bin_size=5e-2,\n",
    ")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algos_comparison(my_df, key1, other_keys, key_values=\"error\"):\n",
    "    vals_key1 = my_df.loc[my_df[\"algorithm\"]==key1, key_values].values\n",
    "    algo1_is_better = True\n",
    "    for key2 in other_keys:\n",
    "        vals_other = my_df.loc[my_df[\"algorithm\"] == key2, key_values].values\n",
    "        try:\n",
    "            key1_better = has_lower_mean(vals_key1, vals_other)\n",
    "        except ValueError:\n",
    "            key1_better = False\n",
    "            break\n",
    "        if not key1_better:\n",
    "            algo1_is_better = False\n",
    "            break\n",
    "    return key1_better\n",
    "\n",
    "\n",
    "gped = df.groupby(\"sample_size\")\n",
    "fdr_mf_better = gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"IAF\"], key_values=\"fdr\")\n",
    "fdr_iaf_better = gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"MF\"], key_values=\"fdr\")\n",
    "\n",
    "fnr_mf_better = gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"IAF\"], key_values=\"fnr\")\n",
    "fnr_iaf_better = gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"MF\"], key_values=\"fnr\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table = df.groupby([\"sample_size\", \"algorithm\"])[\"fdr\", \"fnr\"].mean().round(3).reset_index()\n",
    "\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"fdr_better\"] = fdr_mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"fdr_better\"] = fdr_iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"fnr_better\"] = fnr_mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"fnr_better\"] = fnr_iaf_better.values\n",
    "\n",
    "res_table.loc[res_table[\"fdr_better\"], \"fdr\"] = res_table.loc[\n",
    "    res_table[\"fdr_better\"], \"fdr\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[res_table[\"fnr_better\"], \"fnr\"] = res_table.loc[\n",
    "    res_table[\"fnr_better\"], \"fnr\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[:, \"fdr\"] = res_table.loc[:, \"fdr\"].apply(lambda x: \"$ {} $\".format(x))\n",
    "res_table.loc[:, \"fnr\"] = res_table.loc[:, \"fnr\"].apply(lambda x: \"$ {} $\".format(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.pivot(\n",
    "    index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.loc[lambda x: x[\"sample_size\"].isin([5, 20, 100])].pivot(\n",
    "    index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"]\n",
    ").T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    res_table.loc[lambda x: x[\"sample_size\"].isin([5, 20, 100])]\n",
    "    .pivot(index=\"algorithm\", columns=\"sample_size\", values=[\"fdr\", \"fnr\"])\n",
    "    .T\n",
    "    .to_latex(escape=False)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"fdr\").loc[\n",
    "    :, [5, 20, 100]\n",
    "].to_latex(escape=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"fnr\").loc[:, [5, 20, 100]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PR Curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PR Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 0\n",
    "\n",
    "preds_md = res_mf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "preds_iaf = res_iaf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "preds_deseq2 = 1.0 - other_predictions['deseq2']['pval'][-1, 0, :]\n",
    "preds_edger = 1.0 - other_predictions['edger']['pval'][-1, 0, :]\n",
    "# preds_edgerr = 1.0 - other_predictions['edger_robust']['pval'][-1, 0, :]\n",
    "preds_mast = 1.0 - other_predictions['mast']['pval'][-1, 0, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isnan(preds_md).mean())\n",
    "print(np.isnan(preds_iaf).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_deseq2).mean())\n",
    "print(np.isnan(preds_edger).mean())\n",
    "print(np.isnan(preds_mast).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_deseq2[np.isnan(preds_deseq2)] = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "def plot_pr(fig, preds, y_true, name):\n",
    "    average_precision = average_precision_score(y_true, preds)\n",
    "    preds[np.isnan(preds)] = np.min(preds[~np.isnan(preds)])\n",
    "    precs, recs, _ = precision_recall_curve(y_true=y_true, probas_pred=preds)\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x=recs,\n",
    "            y=precs,\n",
    "            name=name+'@AP: {0:0.2f}'.format(average_precision)\n",
    "        )\n",
    "    )\n",
    "    return\n",
    "layout = go.Layout(\n",
    "    title='Precision Recall Curves',\n",
    "    xaxis=dict(title='Recall'),\n",
    "    yaxis=dict(title='Precision'),\n",
    "    width=800,\n",
    "    height=600,\n",
    ")\n",
    "fig = go.Figure(layout=layout)\n",
    "plot_pr(fig=fig, preds=preds_md, y_true=is_significant_de, name='MF')\n",
    "plot_pr(fig=fig, preds=preds_iaf, y_true=is_significant_de, name='IAF')\n",
    "plot_pr(fig=fig, preds=preds_deseq2, y_true=is_significant_de, name='DESeq2')\n",
    "plot_pr(fig=fig, preds=preds_edger, y_true=is_significant_de, name='EdgeR')\n",
    "# plot_pr(fig=fig, preds=preds_edgerr, y_true=is_significant_de, name='EdgeR Robust')\n",
    "plot_pr(fig=fig, preds=preds_mast, y_true=is_significant_de, name='MAST')\n",
    "\n",
    "fig.show()\n",
    "iplot(fig, filename=\"powsimr_pr_curves2\", sharing=\"private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MAP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_ap(my_df):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    average_precision = average_precision_score(is_significant_de, my_df.de_proba)\n",
    "    return pd.Series(dict(AP=average_precision))\n",
    "\n",
    "\n",
    "ap_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"MF\")\n",
    ")\n",
    "ap_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\"])\n",
    "    .apply(do_ap)\n",
    "    .reset_index()\n",
    "    .assign(algorithm=\"IAF\")\n",
    ")\n",
    "\n",
    "all_ap = pd.concat([ap_mf, ap_iaf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(all_ap, x=\"sample_size\", y=\"AP\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_ap.groupby([\"algorithm\", \"sample_size\"]).agg(dict(AP=[\"mean\", \"std\"]))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagonal Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfc_gt = -(dataset.lfc[:, 1] - dataset.lfc[:, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subsample_genes = np.sort(np.random.permutation(n_genes)[:150])\n",
    "\n",
    "lfcs_mf = (\n",
    "    res_mf\n",
    "    .loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")\n",
    "    [[\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"algorithm\"]]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes]\n",
    "    )\n",
    ")\n",
    "\n",
    "lfcs_ia = (\n",
    "    res_iaf\n",
    "    .loc[\n",
    "        lambda x: (x.experiment == 0)\n",
    "        & (x.training == selected_training)\n",
    "        & (x.sample_size == 100)\n",
    "        & (x.gene.isin(subsample_genes))\n",
    "    ]\n",
    "    .sort_values(\"gene\")\n",
    "    [[\"lfc_mean\", \"hdi99_low\", \"hdi99_high\", \"algorithm\"]]\n",
    "    .assign(\n",
    "        err_minus=lambda x: x.lfc_mean - x.hdi99_low,\n",
    "        err_pos=lambda x: x.hdi99_high - x.lfc_mean,\n",
    "        lfc_gt=lfc_gt[subsample_genes]\n",
    "    )\n",
    ")\n",
    "\n",
    "\n",
    "all_lfcs = pd.concat([lfcs_mf, lfcs_ia], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = px.scatter(\n",
    "    all_lfcs,\n",
    "    x=\"lfc_gt\",\n",
    "    y=\"lfc_mean\",\n",
    "    color=\"algorithm\",\n",
    "    error_y=\"err_pos\",\n",
    "    error_y_minus=\"err_minus\",\n",
    ")\n",
    "\n",
    "fig.add_trace(\n",
    "    go.Scatter(\n",
    "        x=[-3, 3],\n",
    "        y=[-3, 3],\n",
    "        mode=\"lines\",\n",
    "        line=dict(color=\"black\", width=4, dash=\"dash\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frac_inside(my_df):\n",
    "    confidence = 64\n",
    "    is_in_hdi64 = (lfc_gt <= my_df[\"hdi{}_high\".format(confidence)]) & (\n",
    "        lfc_gt >= my_df[\"hdi{}_low\".format(confidence)]\n",
    "    )\n",
    "    confidence = 99\n",
    "    is_in_hdi99 = (lfc_gt <= my_df[\"hdi{}_high\".format(confidence)]) & (\n",
    "        lfc_gt >= my_df[\"hdi{}_low\".format(confidence)]\n",
    "    )\n",
    "    return pd.Series([\n",
    "#         (0.64 - is_in_hdi64.mean())**2, \n",
    "#         (0.99 - is_in_hdi99.mean())**2\n",
    "        is_in_hdi64.mean(), \n",
    "        is_in_hdi99.mean()\n",
    "    ], index=[\"error64\", \"error99\"])\n",
    "\n",
    "\n",
    "errs_mf = (\n",
    "    res_mf.groupby(by=[\"experiment\", \"sample_size\", \"training\"])\n",
    "    .apply(frac_inside)\n",
    "    .reset_index()\n",
    ").assign(algorithm=\"Mean Field\")\n",
    "\n",
    "errs_iaf = (\n",
    "    res_iaf.groupby(by=[\"experiment\", \"sample_size\", \"training\"])\n",
    "    .apply(frac_inside)\n",
    "    .reset_index()\n",
    ").assign(algorithm=\"IAF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(errs_mf)\n",
    "display(errs_iaf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scoring(my_df):\n",
    "    err_scores = my_df.loc[:, \"error64\"] + my_df.loc[:, \"error99\"]\n",
    "    err_scores_mf = err_scores[my_df[\"algorithm\"] == \"Mean Field\"].values\n",
    "    err_scores_iaf = err_scores[my_df[\"algorithm\"] == \"IAF\"].values\n",
    "    \n",
    "    disp_mf = str(err_scores_mf.mean().round(3))\n",
    "    disp_iaf = str(err_scores_iaf.mean().round(3))\n",
    "#     print(err_scores_mf)\n",
    "    if has_lower_mean(err_scores_mf, err_scores_iaf):\n",
    "        disp_mf = \"\\mathbf{{ {} }}\".format(disp_mf)\n",
    "    if has_lower_mean(err_scores_iaf, err_scores_mf):\n",
    "        disp_iaf = \"\\mathbf{{ {} }}\".format(disp_iaf)\n",
    "    \n",
    "    return pd.Series(\n",
    "        dict(\n",
    "            IAF=disp_mf, \n",
    "            MF=disp_iaf\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    \n",
    "errs = pd.concat([errs_mf, errs_iaf], ignore_index=True)\n",
    "errs.groupby(\"sample_size\").apply(scoring)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Study of LFC errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_l2_err(diff):\n",
    "    res = 0.5 * (diff ** 2) ** (0.5)\n",
    "    res = np.nanmean(res, axis=-1)\n",
    "    return res\n",
    "\n",
    "def l2_err_competitor(vals: np.ndarray, other: np.ndarray = None):\n",
    "    vals[np.isnan(vals)] = 0.0\n",
    "    if other is None:\n",
    "        diff = vals\n",
    "    else:\n",
    "        diff = vals - other\n",
    "    res = compute_l2_err(diff)\n",
    "    assert res.shape == (N_SIZES, N_PICKS)\n",
    "    data = []\n",
    "    for (size_ix, size) in enumerate(SIZES):\n",
    "        for pick in range(N_PICKS):\n",
    "            data.append(dict(experiment=pick, training=0, sample_size=size, error=res[size_ix, pick]))\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "lfcs_errs_deseq2 = l2_err_competitor(other_predictions[\"deseq2\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"DESeq2\")\n",
    "lfcs_errs_edger = l2_err_competitor(other_predictions[\"edger\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"EdgeR\")\n",
    "lfcs_errs_mast = l2_err_competitor(other_predictions[\"mast\"][\"lfc\"], other=lfc_gt).assign(algorithm=\"MAST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pd_l2_err(my_df):\n",
    "    diff = my_df.sort_values(\"gene\")[\"lfc_mean\"] - lfc_gt\n",
    "    error = 0.5 * (diff ** 2) ** (0.5)\n",
    "    error = np.nanmean(error)\n",
    "    return pd.Series(dict(error=error))\n",
    "\n",
    "lfcs_errs_mf = (\n",
    "    res_mf\n",
    "    .groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "lfcs_errs_iaf = (\n",
    "    res_iaf\n",
    "    .groupby([\"experiment\", \"sample_size\", \"training\", \"algorithm\"])\n",
    "    .apply(pd_l2_err)\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_errs = pd.concat([\n",
    "    lfcs_errs_mf,\n",
    "    lfcs_errs_iaf,\n",
    "    lfcs_errs_deseq2,\n",
    "    lfcs_errs_edger,\n",
    "    lfcs_errs_mast,\n",
    "], ignore_index=True)\n",
    "\n",
    "px.box(all_errs, x=\"sample_size\", y=\"error\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def algos_comparison(my_df, key1, other_keys):\n",
    "    vals_key1 = my_df.loc[my_df[\"algorithm\"]==key1, \"error\"].values\n",
    "    algo1_is_better = True\n",
    "    for key2 in other_keys:\n",
    "        vals_other = my_df.loc[my_df[\"algorithm\"] == key2, \"error\"].values\n",
    "        key1_better = has_lower_mean(vals_key1, vals_other)\n",
    "        if not key1_better:\n",
    "            algo1_is_better = False\n",
    "            break\n",
    "    return key1_better\n",
    "\n",
    "gped = all_errs.groupby(\"sample_size\")\n",
    "mf_or_iaf_better = (\n",
    "    gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"DESeq2\", \"EdgeR\", \"MAST\"]) &\n",
    "    gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"DESeq2\", \"EdgeR\", \"MAST\"])\n",
    ")\n",
    "mf_better = gped.apply(algos_comparison, key1=\"MF\", other_keys=[\"IAF\", \"DESeq2\", \"EdgeR\", \"MAST\"])\n",
    "iaf_better = gped.apply(algos_comparison, key1=\"IAF\", other_keys=[\"MF\", \"DESeq2\", \"EdgeR\", \"MAST\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table = (\n",
    "    all_errs.groupby([\"sample_size\", \"algorithm\"])\n",
    "    .error.agg(dict(err_mean=\"mean\", err_std=\"std\"))\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        displayed=lambda x: x.apply(\n",
    "            lambda y: \"{:.3f} \\pm {:.3f}\".format(y.err_mean, y.err_std), axis=1\n",
    "        ),\n",
    "        is_better=False,\n",
    "        one_of_best=False,\n",
    "    )\n",
    ")\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"is_better\"] = mf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"is_better\"] = iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"IAF\", \"one_of_best\"] = mf_or_iaf_better.values\n",
    "res_table.loc[res_table[\"algorithm\"] == \"MF\", \"one_of_best\"] = mf_or_iaf_better.values\n",
    "\n",
    "\n",
    "res_table.loc[lambda x: x.one_of_best, \"displayed\"] = (\n",
    "    res_table.loc[lambda x: x.one_of_best, \"displayed\"] + \"^*\"\n",
    ")\n",
    "res_table.loc[lambda x: x.is_better, \"displayed\"] = res_table.loc[\n",
    "    lambda x: x.is_better, \"displayed\"\n",
    "].apply(lambda x: \"\\mathbf{{ {} }}\".format(x))\n",
    "\n",
    "res_table.loc[:, \"displayed\"] = res_table.loc[:, \"displayed\"].apply(lambda x: \"$ {} $\".format(x)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_table.pivot(index=\"algorithm\", columns=\"sample_size\", values=\"displayed\").loc[\n",
    "    [\"DESeq2\", \"EdgeR\", \"MAST\", \"MF\", \"IAF\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_mf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((res_mf.hdi64_high - res_mf.hdi64_low).mean())\n",
    "print((res_iaf.hdi64_high - res_iaf.hdi64_low).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coverage(my_df, low_key=\"hdi64_low\", high_key=\"hdi64_high\"):\n",
    "    my_df = my_df.sort_values(\"gene\")\n",
    "    assert len(my_df) == n_genes\n",
    "    gene_is_covered = (lfc_gt >= my_df[low_key]) & (lfc_gt <= my_df[high_key])\n",
    "#     mean_cov = (gene_is_covered / (my_df[high_key] - my_df[low_key])).mean()\n",
    "    mean_cov = (gene_is_covered).mean()\n",
    "    return pd.Series(dict(mean_cov=mean_cov))\n",
    "    \n",
    "\n",
    "coverage_mf = (\n",
    "    res_mf.groupby([\"experiment\", \"training\", \"sample_size\", \"algorithm\"])\n",
    "    .apply(get_coverage, low_key=\"hdi64_low\", high_key=\"hdi64_high\")\n",
    "    .reset_index()\n",
    "#     .groupby(\"sample_size\")\n",
    "#     .agg(dict(mean_cov=[\"mean\", \"std\"]))\n",
    ")\n",
    "coverage_iaf = (\n",
    "    res_iaf.groupby([\"experiment\", \"training\", \"sample_size\", \"algorithm\"])\n",
    "    .apply(get_coverage, low_key=\"hdi64_low\", high_key=\"hdi64_high\")\n",
    "    .reset_index()\n",
    "#     .groupby(\"sample_size\")\n",
    "#     .agg(dict(mean_cov=[\"mean\", \"std\"]))\n",
    ")\n",
    "\n",
    "all_coverages = pd.concat([coverage_mf, coverage_iaf], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(all_coverages, x=\"sample_size\", y=\"mean_cov\", color=\"algorithm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Volcano"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_training = 0\n",
    "\n",
    "preds_md = res_mf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "preds_iaf = res_iaf.loc[\n",
    "    lambda x: (x.experiment == 0) & (x.training == selected_training) & (x.sample_size == 100)\n",
    "].sort_values(\"gene\")[\"de_proba\"]\n",
    "\n",
    "\n",
    "\n",
    "fig = go.Figure(\n",
    "    layout=go.Layout(\n",
    "        yaxis=dict(title=\"Estimated probabily of DE\"),\n",
    "        xaxis=dict(title=\"Ground-Truth LFC\"),\n",
    "    )\n",
    ")\n",
    "fig.add_traces(\n",
    "    [\n",
    "        go.Scatter(x=lfc_gt, y=np.log10(preds_md), mode=\"markers\", name=\"MF\"),\n",
    "        go.Scatter(x=lfc_gt, y=np.log10(preds_iaf), mode=\"markers\", name=\"IAF\"),\n",
    "#         go.Scatter(x=lfc_gt, y=np.log10(preds_mast), mode=\"markers\"),\n",
    "#         go.Scatter(x=lfc_gt, y=np.log10(preds_edger), mode=\"markers\"),\n",
    "        go.Scatter(\n",
    "            x=[-0.5, -0.5], y=[-6, 0.0], mode=\"lines\", line=dict(color=\"black\", width=2)\n",
    "        ),\n",
    "        go.Scatter(\n",
    "            x=[0.5, 0.5], y=[-6, 0.0], mode=\"lines\", line=dict(color=\"black\", width=2)\n",
    "        ),\n",
    "        #         go.Scatter(\n",
    "        #             x=[alpha, alpha], y=[-6, 0.0], mode=\"lines\", line=dict(color=\"black\", width=2)\n",
    "        #         ),\n",
    "    ]\n",
    ")\n",
    "\n",
    "fig.show()\n",
    "# iplot(fig, filename=\"symsim_volcano\", sharing=\"private\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Debug"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trace1 = go.Histogram(x=dataset.X.mean(0))\n",
    "# fig = go.Figure(data=[trace1])\n",
    "# fig.show()\n",
    "\n",
    "# dataset.X\n",
    "\n",
    "# # from torch.autograd import set_detect_anomaly\n",
    "# # set_detect_anomaly(True)\n",
    "\n",
    "# iw_vae = IAVAE(\n",
    "#     dataset.nb_genes, n_batch=dataset.n_batches, n_hidden=32, n_layers=1, do_h=True, n_latent=10, t=4\n",
    "# )\n",
    "# iw_trainer = UnsupervisedTrainer(\n",
    "#     iw_vae, dataset, ratio_loss=True, k_importance_weighted=5, single_backward=False\n",
    "# )\n",
    "# iw_trainer.train(n_epochs=50, lr=1e-3)\n",
    "# iw_trainer.train_losses\n",
    "\n",
    "iw_vae = VAE(\n",
    "    dataset.nb_genes, n_batch=dataset.n_batches, n_hidden=128, n_layers=1, n_latent=10\n",
    ")\n",
    "iw_trainer = UnsupervisedTrainer(\n",
    "    iw_vae, dataset, ratio_loss=True, k_importance_weighted=5, single_backward=False\n",
    ")\n",
    "iw_trainer.train(n_epochs=50, lr=1e-3)\n",
    "iw_trainer.train_losses\n",
    "\n",
    "\n",
    "vae = VAE(\n",
    "    dataset.nb_genes, n_batch=dataset.n_batches, n_hidden=128, n_layers=1, n_latent=10\n",
    ")\n",
    "trainer = UnsupervisedTrainer(\n",
    "    vae, dataset, ratio_loss=True, #k_importance_weighted=5, single_backward=False\n",
    ")\n",
    "trainer.train(n_epochs=50, lr=1e-3)\n",
    "trainer.train_losses\n",
    "\n",
    "\n",
    "\n",
    "# iw_trainer.train_losses\n",
    "\n",
    "# iavae = IAVAE(\n",
    "#     dataset.nb_genes,\n",
    "#     n_batch=dataset.n_batches,\n",
    "#     n_hidden=128,\n",
    "#     n_layers=1,\n",
    "#     do_h=True,\n",
    "#     n_latent=5,\n",
    "#     t=4,\n",
    "# )\n",
    "# ia_trainer = UnsupervisedTrainer(iavae, dataset, ratio_loss=True)\n",
    "# ia_trainer.train(n_epochs=100, lr=1e-3)\n",
    "\n",
    "# ia_trainer.train_losses\n",
    "\n",
    "# plt.plot(trainer.train_losses[5:], label=\"MF\")\n",
    "# plt.plot(iw_trainer.train_losses[5:], label=\"IW\")\n",
    "# plt.plot(ia_trainer.train_losses[5:], label=\"IAF\")\n",
    "# # plt.yscale(\"log\")\n",
    "# plt.legend()\n",
    "\n",
    "# ia_trainer.test_set.marginal_ll(n_mc_samples=100, ratio_loss=True)\n",
    "\n",
    "\n",
    "\n",
    "# evidence_mf = trainer.test_set.marginal_ll(n_mc_samples=100, ratio_loss=True)\n",
    "# # evidence_iw = iw_trainer.test_set.marginal_ll(n_mc_samples=100, ratio_loss=True)\n",
    "# evidence_ia = ia_trainer.test_set.marginal_ll(n_mc_samples=100, ratio_loss=True)\n",
    "# print(evidence_mf, evidence_iw, evidence_ia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(iw_trainer.test_set.marginal_ll(n_mc_samples=500, ratio_loss=True))\n",
    "print(trainer.test_set.marginal_ll(n_mc_samples=500, ratio_loss=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_iw = iw_trainer.test_set\n",
    "test_mf = trainer.test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_lfc(post):\n",
    "    z_iaf, labels_iaf, scales_iaf = post.get_latents(n_samples=100, other='scales', device=\"cpu\")\n",
    "    where_a = np.where(labels_iaf == 0)[0][:200]\n",
    "    where_b = np.where(labels_iaf == 1)[0][:200]\n",
    "\n",
    "    scales_a = scales_iaf[:, where_a, :]\n",
    "    scales_b = scales_iaf[:, where_b, :]\n",
    "\n",
    "    lfc = np.log2(scales_a) - np.log2(scales_b)\n",
    "    lfc = lfc.reshape((-1, n_genes))\n",
    "    lfc = np.array(lfc)\n",
    "    return (np.abs(lfc) >= DELTA).mean(0)\n",
    "\n",
    "de_probas_iw = get_lfc(test_iw)\n",
    "de_probas_mf = get_lfc(test_mf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = go.Figure()\n",
    "plot_pr(fig=fig, preds=de_probas_iw, y_true=is_significant_de, name='IW IAF')\n",
    "plot_pr(fig=fig, preds=de_probas_mf, y_true=is_significant_de, name='MF')\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4395.42928125 4369.418739583333 4391.585421875"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the better"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using very quick experiments (I am waiting for the autotune module), it looks like:\n",
    "\n",
    "MF\n",
    "- 1 layer 16 941.0715670955882\n",
    "- 1 layer 64 935.0418129595588\n",
    "- 1 layer 128 925.3853147977941\n",
    "- 3 layers 32 972.2561259191176\n",
    "\n",
    "IAF\n",
    "- 1 layer 16 950.423221507353\n",
    "- 1 layer 64 928.0418129595588\n",
    "- 1 layer 128 917.7115165441177\n",
    "- 3 layers 32 928.8929044117647\n",
    "- t=4 1 layer 128 915.8975482536765\n",
    "- t=5 1 layer 128 920.9016911764705"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing IAF posteriors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post = ia_trainer.train_set\n",
    "# train_indices = post.data_loader.sampler.indices\n",
    "# train_samples = np.random.permutation(train_indices)[:2000]\n",
    "# post = ia_trainer.create_posterior(\n",
    "#     model=iavae, gene_dataset=dataset, indices=train_samples\n",
    "# )\n",
    "# z_ia, labels_ia, scales_ia = post.get_latents(n_samples=500, other=True, device=\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # post = trainer.train_set\n",
    "# # train_indices = post.data_loader.sampler.indices\n",
    "# # train_samples = np.random.permutation(train_indices)[:2000]\n",
    "# post = trainer.create_posterior(model=vae, gene_dataset=dataset, indices=train_samples)\n",
    "# z, labels, scales = post.get_latents(n_samples=500, other=True, device=\"cpu\")\n",
    "print((labels == labels_ia).float().mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for idx in [5, 10, 100, 1000, 2, 30, 542]:\n",
    "#     trace1 = go.Scatter(x=z[:, idx, 0], y=z[:, idx, 1], mode=\"markers\")\n",
    "#     trace2 = go.Scatter(x=z_ia[:, idx, 0], y=z_ia[:, idx, 1], mode=\"markers\")\n",
    "#     fig = go.Figure([trace1, trace2])\n",
    "#     fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from plotly.subplots import make_subplots\n",
    "# from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "# fig = make_subplots(rows=1, cols=2)\n",
    "\n",
    "# for my_z in []:\n",
    "#     z_mean = my_z.mean(0)\n",
    "#     z_tsne = TSNE().fit_transform(z_mean)\n",
    "#     #     z_tnse = z_mean\n",
    "\n",
    "#     fig.add_trace(\n",
    "#         go.Scatter(\n",
    "#             x=z_tnse[:, 0],\n",
    "#             y=z_tnse[:, 1],\n",
    "#             marker=dict(color=my_lbl.squeeze(), colorscale=\"viridis\"),\n",
    "#             mode=\"markers\",\n",
    "#         ),\n",
    "#         row=1,\n",
    "#         col=1,\n",
    "#     )\n",
    "\n",
    "# fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels = labels.squeeze()\n",
    "# where_a = np.where(labels == 0)[0]\n",
    "# where_b = np.where(labels == 1)[0]\n",
    "# where_a = where_a[np.random.choice(len(where_a), size=size)]\n",
    "# where_b = where_b[np.random.choice(len(where_b), size=size)]\n",
    "# scales_a = scales[:, where_a, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_b = scales[:, where_b, :].reshape((-1, dataset.nb_genes)).numpy()\n",
    "# scales_a, scales_b = demultiply(arr1=scales_a, arr2=scales_b, factor=3)\n",
    "# lfc = np.log2(scales_a) - np.log2(scales_b)\n",
    "\n",
    "# pgs = (np.abs(lfc) >= DELTA).mean(axis=0)\n",
    "# sorted_genes = np.argsort(-pgs)\n",
    "# sorted_pgs = pgs[sorted_genes]\n",
    "# cumulative_fdr = (1.0 - sorted_pgs).cumsum() / (1.0 + np.arange(len(sorted_pgs)))\n",
    "# d = (cumulative_fdr <= Q0).sum() - 1\n",
    "# pred_de_genes = sorted_genes[:d]\n",
    "# is_pred_de = np.zeros_like(cumulative_fdr)\n",
    "# is_pred_de[pred_de_genes] = True\n",
    "# true_fdr = ((~is_significant_de) * is_pred_de).sum() / len(pred_de_genes)\n",
    "\n",
    "# true_fdr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# y_preds_1d = is_pred_de.reshape((-1, dataset.nb_genes))\n",
    "# n_exps = len(y_preds_1d)\n",
    "# mat = confusion_matrix(is_significant_de, is_pred_de)\n",
    "# mat"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "272px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
