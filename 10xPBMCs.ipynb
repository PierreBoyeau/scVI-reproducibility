{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# loading modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "from model import scVI_final as scVI\n",
    "from benchmarking import *\n",
    "from helper import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_batch_scVI = True\n",
    "learning_rate = 0.0004\n",
    "epsilon = 0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expression data\n",
    "data_path = \"/home/ubuntu/single-cell-scVI/data/10xPBMCs/\"\n",
    "expression_train = np.load(data_path + \"de/data_train.npy\")\n",
    "expression_test = np.load(data_path + \"de/data_test.npy\")\n",
    "\n",
    "# qc metrics\n",
    "r_train = np.load(data_path + \"design_train.npy\")\n",
    "r_test = np.load(data_path + \"design_test.npy\")\n",
    "\n",
    "# labels\n",
    "c_train = np.loadtxt(data_path + \"label_train\")\n",
    "c_test = np.loadtxt(data_path + \"label_test\")\n",
    "\n",
    "# batch info\n",
    "b_train = np.loadtxt(data_path + \"b_train\")\n",
    "b_test = np.loadtxt(data_path + \"b_test\")\n",
    "\n",
    "# corrupted data\n",
    "X_zero, i, j, ix = \\\n",
    "        np.load(data_path + \"imputation/X_zero.npy\"), np.load(data_path + \"imputation/i.npy\"),\\\n",
    "        np.load(data_path + \"imputation/j.npy\"), np.load(data_path + \"imputation/ix.npy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running scVI on 3346 genes\n",
      "Got 2batches in the data\n",
      "Will apply a MMD penalty\n",
      "Will work on mode list for incorporating library size\n",
      "Will work on mode gene-batch for modeling inverse dispersion param\n",
      "Will apply zero inflation\n",
      "1 hidden layers at 128 each for a final 10 latent space\n"
     ]
    }
   ],
   "source": [
    "tf.reset_default_graph()\n",
    "expression = tf.placeholder(tf.float32, (None, expression_train.shape[1]), name='x')\n",
    "kl_scalar = tf.placeholder(tf.float32, (), name='kl_scalar')\n",
    "batch = tf.placeholder(tf.int32, [None], name=\"batch_ind\")\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate, epsilon=epsilon)\n",
    "training_phase = tf.placeholder(tf.bool, (), name='training_phase')\n",
    "mmd_scalar = tf.placeholder(tf.float32, [], name='l')\n",
    "\n",
    "# setting up priors\n",
    "l_mean, l_var = [], []\n",
    "log_library_size = np.log(np.sum(expression_train, axis=1))\n",
    "for i in np.unique(b_train):\n",
    "    l_mean.append([np.mean(log_library_size[b_train == i])])\n",
    "    l_var.append([np.var(log_library_size[b_train == i])])\n",
    "library_mean = tf.constant(l_mean)\n",
    "library_var = tf.constant(l_var)\n",
    "\n",
    "# loading model\n",
    "if not run_batch_scVI:\n",
    "    mean = np.mean(l_mean)\n",
    "    var = np.mean(l_var)\n",
    "\n",
    "    model = scVI.scVIModel(expression=expression, kl_scale=kl_scalar, \\\n",
    "                          optimize_algo=optimizer, phase=training_phase, \\\n",
    "                           library_size_mean=mean, library_size_var=var)\n",
    "    \n",
    "else:\n",
    "    model = scVI.scVIModel(expression=expression, kl_scale=kl_scalar, batch_ind=batch, mmd_scale=mmd_scalar, \\\n",
    "                       num_batches=2, apply_mmd=True, optimize_algo=optimizer, phase=training_phase, \\\n",
    "                           dispersion=\"gene-batch\", library_size_mean=library_mean, library_size_var=library_var)\n",
    "    \n",
    "# Session creation\n",
    "sess = tf.Session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "result = train_model(model, (expression_train, expression_test), sess, 120, batch=(b_train, b_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((9029, 3346), (3010, 3346))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expression_train.shape, expression_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import FactorAnalysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_generative_model(N):\n",
    "    z_train = np.random.normal(size=(N, 10))\n",
    "    l_train = np.mean(l_mean) + np.sqrt(np.mean(l_var)) * np.random.normal(size=(N, 1))\n",
    "    b_train = np.ones((N))\n",
    "    dic_z = {model.z: z_train, model.library:l_train, training_phase:False, kl_scalar:1., batch:b_train}\n",
    "    rate, dropout = sess.run((model.px_rate, model.px_dropout), feed_dict=dic_z)\n",
    "    dispersion = np.tile(sess.run((tf.exp(model.px_r))[0]), (rate.shape[0], 1))\n",
    "    p = rate / (rate + dispersion)\n",
    "    r = dispersion \n",
    "    dropout = 1. / (1. + np.exp(-dropout))\n",
    "    l_train = np.random.gamma(r, p / (1-p))\n",
    "    X_train = np.random.poisson(l_train)\n",
    "    X_train *= np.random.binomial(1, 1-dropout)\n",
    "    return X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1490.6595"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval_likelihood(model, sample_generative_model(3010), sess, batch=np.ones((3010)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "359.9556228982517"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = FactorAnalysis(n_components=10, noise_variance_init=10000 * np.ones(expression_train.shape[1]))\n",
    "fa.fit(np.log(1 + expression_train))\n",
    "test_set = np.log(1 + expression_test)\n",
    "fa.score(test_set) - np.mean(np.sum(test_set, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1176.7996554912788"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fa = FactorAnalysis(n_components=10)\n",
    "fa.fit(np.log(1 + sample_generative_model(9029)))\n",
    "test_set = np.log(1 + sample_generative_model(3010))\n",
    "fa.score(test_set) - np.mean(np.sum(test_set, axis=-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.38777304, 0.79069378213836394, 0.69595041584939288]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "latent = eval_latent(model, expression_train, sess, batch=b_train)\n",
    "cluster_scores(latent, len(np.unique(c_train)), c_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1361.0693"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = train_model(model, (expression_test, expression_test), sess, 100, \\\n",
    "                  step=model.test_step, batch=(b_test, b_test))\n",
    "eval_likelihood(model, expression_test, sess, batch=b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(9029, 3346)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.090907849371433258"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess.run(tf.global_variables_initializer())\n",
    "res = train_model(model, (expression_train, expression_test), sess, 120, batch=(b_train, b_test))\n",
    "eval_imputed_data(model, (X_zero, i, j, ix), expression_train, sess, batch=b_train)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
