{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n",
      "/home/ubuntu/dca/lib/python3.5/importlib/_bootstrap.py:222: RuntimeWarning: numpy.dtype size changed, may indicate binary incompatibility. Expected 96, got 88\n",
      "  return f(*args, **kwds)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating grammar tables from /usr/lib/python3.5/lib2to3/Grammar.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating grammar tables from /usr/lib/python3.5/lib2to3/PatternGrammar.txt\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "from dca.api import dca\n",
    "import anndata\n",
    "\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/single-cell-scVI/data/10x1M/data_small.hdf5\"\n",
    "f = h5py.File(data_path)\n",
    "expression_train = f[\"data_train\"][:10000]\n",
    "expression_test = f[\"data_test\"][:10000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"/home/ubuntu/single-cell-scVI/data/10x1M/\"\n",
    "X_zero, i, j, ix = \\\n",
    "        np.load(data_path + \"imputation/X_zero.npy\"), np.load(data_path + \"imputation/i.npy\"),\\\n",
    "        np.load(data_path + \"imputation/j.npy\"), np.load(data_path + \"imputation/ix.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DCA: Successfully preprocessed 720 genes and 10000 cells.\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "count (InputLayer)              (None, 720)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "enc0 (Dense)                    (None, 64)           46144       count[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 64)           192         enc0[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "enc0_act (Activation)           (None, 64)           0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "center (Dense)                  (None, 32)           2080        enc0_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32)           96          center[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "center_act (Activation)         (None, 32)           0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dec1 (Dense)                    (None, 64)           2112        center_act[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 64)           192         dec1[0][0]                       \n",
      "__________________________________________________________________________________________________\n",
      "dec1_act (Activation)           (None, 64)           0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "mean (Dense)                    (None, 720)          46800       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "size_factors (InputLayer)       (None, 1)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "output (Lambda)                 (None, 720)          0           mean[0][0]                       \n",
      "                                                                 size_factors[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dispersion (Dense)              (None, 720)          46800       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pi (Dense)                      (None, 720)          46800       dec1_act[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "slice (SliceLayer)              (None, 720)          0           output[0][0]                     \n",
      "                                                                 dispersion[0][0]                 \n",
      "                                                                 pi[0][0]                         \n",
      "==================================================================================================\n",
      "Total params: 191,216\n",
      "Trainable params: 190,896\n",
      "Non-trainable params: 320\n",
      "__________________________________________________________________________________________________\n",
      "Train on 9000 samples, validate on 1000 samples\n",
      "Epoch 1/300\n",
      "9000/9000 [==============================] - 4s 409us/step - loss: 1.8487 - val_loss: 1.6693\n",
      "Epoch 2/300\n",
      "9000/9000 [==============================] - 3s 296us/step - loss: 1.6580 - val_loss: 1.6278\n",
      "Epoch 3/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.6380 - val_loss: 1.6158\n",
      "Epoch 4/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.6293 - val_loss: 1.6097\n",
      "Epoch 5/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6232 - val_loss: 1.6048\n",
      "Epoch 6/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6201 - val_loss: 1.6039\n",
      "Epoch 7/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6171 - val_loss: 1.6021\n",
      "Epoch 8/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.6144 - val_loss: 1.5987\n",
      "Epoch 9/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.6123 - val_loss: 1.5971\n",
      "Epoch 10/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.6103 - val_loss: 1.5991\n",
      "Epoch 11/300\n",
      "9000/9000 [==============================] - 3s 296us/step - loss: 1.6097 - val_loss: 1.5970\n",
      "Epoch 12/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6079 - val_loss: 1.5954\n",
      "Epoch 13/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6067 - val_loss: 1.5959\n",
      "Epoch 14/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.6055 - val_loss: 1.5954\n",
      "Epoch 15/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.6037 - val_loss: 1.5936\n",
      "Epoch 16/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6033 - val_loss: 1.5916\n",
      "Epoch 17/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.6030 - val_loss: 1.5969\n",
      "Epoch 18/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.6017 - val_loss: 1.5924\n",
      "Epoch 19/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6015 - val_loss: 1.5912\n",
      "Epoch 20/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.6002 - val_loss: 1.5928\n",
      "Epoch 21/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5996 - val_loss: 1.5908\n",
      "Epoch 22/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5994 - val_loss: 1.5908\n",
      "Epoch 23/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5982 - val_loss: 1.5880\n",
      "Epoch 24/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5976 - val_loss: 1.5875\n",
      "Epoch 25/300\n",
      "9000/9000 [==============================] - 3s 294us/step - loss: 1.5975 - val_loss: 1.5923\n",
      "Epoch 26/300\n",
      "9000/9000 [==============================] - 3s 294us/step - loss: 1.5961 - val_loss: 1.5856\n",
      "Epoch 27/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5961 - val_loss: 1.5867\n",
      "Epoch 28/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5951 - val_loss: 1.5860\n",
      "Epoch 29/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5945 - val_loss: 1.5878\n",
      "Epoch 30/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5940 - val_loss: 1.5866\n",
      "Epoch 31/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.5949 - val_loss: 1.5895\n",
      "Epoch 32/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5939 - val_loss: 1.5860\n",
      "Epoch 33/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5932 - val_loss: 1.5854\n",
      "Epoch 34/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5927 - val_loss: 1.5864\n",
      "Epoch 35/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.5925 - val_loss: 1.5864\n",
      "Epoch 36/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5921 - val_loss: 1.5850\n",
      "Epoch 37/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5918 - val_loss: 1.5852\n",
      "Epoch 38/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5918 - val_loss: 1.5879\n",
      "Epoch 39/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5911 - val_loss: 1.5841\n",
      "Epoch 40/300\n",
      "9000/9000 [==============================] - 3s 294us/step - loss: 1.5907 - val_loss: 1.5849\n",
      "Epoch 41/300\n",
      "9000/9000 [==============================] - 3s 294us/step - loss: 1.5901 - val_loss: 1.5867\n",
      "Epoch 42/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5903 - val_loss: 1.5842\n",
      "Epoch 43/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5898 - val_loss: 1.5876\n",
      "Epoch 44/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5898 - val_loss: 1.5839\n",
      "Epoch 45/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5894 - val_loss: 1.5853\n",
      "Epoch 46/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5892 - val_loss: 1.5822\n",
      "Epoch 47/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5889 - val_loss: 1.5823\n",
      "Epoch 48/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5886 - val_loss: 1.5836\n",
      "Epoch 49/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5882 - val_loss: 1.5819\n",
      "Epoch 50/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5878 - val_loss: 1.5835\n",
      "Epoch 51/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5875 - val_loss: 1.5809\n",
      "Epoch 52/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.5874 - val_loss: 1.5801\n",
      "Epoch 53/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5874 - val_loss: 1.5839\n",
      "Epoch 54/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5870 - val_loss: 1.5827\n",
      "Epoch 55/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5873 - val_loss: 1.5817\n",
      "Epoch 56/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5871 - val_loss: 1.5799\n",
      "Epoch 57/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.5868 - val_loss: 1.5855\n",
      "Epoch 58/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5865 - val_loss: 1.5808\n",
      "Epoch 59/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5868 - val_loss: 1.5822\n",
      "Epoch 60/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5857 - val_loss: 1.5795\n",
      "Epoch 61/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5858 - val_loss: 1.5819\n",
      "Epoch 62/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5854 - val_loss: 1.5803\n",
      "Epoch 63/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5856 - val_loss: 1.5798\n",
      "Epoch 64/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5861 - val_loss: 1.5888\n",
      "Epoch 65/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5855 - val_loss: 1.5808\n",
      "Epoch 66/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5856 - val_loss: 1.5799\n",
      "Epoch 67/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5848 - val_loss: 1.5835\n",
      "Epoch 68/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5850 - val_loss: 1.5802\n",
      "Epoch 69/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5850 - val_loss: 1.5798\n",
      "Epoch 70/300\n",
      "9000/9000 [==============================] - 3s 288us/step - loss: 1.5846 - val_loss: 1.5792\n",
      "Epoch 71/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5839 - val_loss: 1.5795\n",
      "Epoch 72/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5844 - val_loss: 1.5799\n",
      "Epoch 73/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5850 - val_loss: 1.5813\n",
      "Epoch 74/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5847 - val_loss: 1.5793\n",
      "Epoch 75/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5844 - val_loss: 1.5807\n",
      "Epoch 76/300\n",
      "9000/9000 [==============================] - 3s 288us/step - loss: 1.5840 - val_loss: 1.5801\n",
      "Epoch 77/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5835 - val_loss: 1.5810\n",
      "Epoch 78/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5836 - val_loss: 1.5785\n",
      "Epoch 79/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5828 - val_loss: 1.5780\n",
      "Epoch 80/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5833 - val_loss: 1.5779\n",
      "Epoch 81/300\n",
      "9000/9000 [==============================] - 3s 296us/step - loss: 1.5831 - val_loss: 1.5816\n",
      "Epoch 82/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.5833 - val_loss: 1.5786\n",
      "Epoch 83/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.5829 - val_loss: 1.5783\n",
      "Epoch 84/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.5828 - val_loss: 1.5788\n",
      "Epoch 85/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.5829 - val_loss: 1.5807\n",
      "Epoch 86/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.5826 - val_loss: 1.5775\n",
      "Epoch 87/300\n",
      "9000/9000 [==============================] - 3s 297us/step - loss: 1.5824 - val_loss: 1.5771\n",
      "Epoch 88/300\n",
      "9000/9000 [==============================] - 3s 297us/step - loss: 1.5823 - val_loss: 1.5784\n",
      "Epoch 89/300\n",
      "9000/9000 [==============================] - 3s 299us/step - loss: 1.5831 - val_loss: 1.5768\n",
      "Epoch 90/300\n",
      "9000/9000 [==============================] - 3s 301us/step - loss: 1.5823 - val_loss: 1.5779\n",
      "Epoch 91/300\n",
      "9000/9000 [==============================] - 3s 302us/step - loss: 1.5822 - val_loss: 1.5782\n",
      "Epoch 92/300\n",
      "9000/9000 [==============================] - 3s 304us/step - loss: 1.5821 - val_loss: 1.5770\n",
      "Epoch 93/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5817 - val_loss: 1.5822\n",
      "Epoch 94/300\n",
      "9000/9000 [==============================] - 3s 299us/step - loss: 1.5818 - val_loss: 1.5769\n",
      "Epoch 95/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5813 - val_loss: 1.5769\n",
      "Epoch 96/300\n",
      "9000/9000 [==============================] - 3s 297us/step - loss: 1.5812 - val_loss: 1.5766\n",
      "Epoch 97/300\n",
      "9000/9000 [==============================] - 3s 301us/step - loss: 1.5819 - val_loss: 1.5783\n",
      "Epoch 98/300\n",
      "9000/9000 [==============================] - 3s 301us/step - loss: 1.5810 - val_loss: 1.5762\n",
      "Epoch 99/300\n",
      "9000/9000 [==============================] - 3s 302us/step - loss: 1.5813 - val_loss: 1.5767\n",
      "Epoch 100/300\n",
      "9000/9000 [==============================] - 3s 306us/step - loss: 1.5813 - val_loss: 1.5788\n",
      "Epoch 101/300\n",
      "9000/9000 [==============================] - 3s 296us/step - loss: 1.5811 - val_loss: 1.5773\n",
      "Epoch 102/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5811 - val_loss: 1.5787\n",
      "Epoch 103/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.5809 - val_loss: 1.5764\n",
      "Epoch 104/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5810 - val_loss: 1.5840\n",
      "Epoch 105/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5810 - val_loss: 1.5759\n",
      "Epoch 106/300\n",
      "9000/9000 [==============================] - 3s 288us/step - loss: 1.5807 - val_loss: 1.5763\n",
      "Epoch 107/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5813 - val_loss: 1.5760\n",
      "Epoch 108/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5804 - val_loss: 1.5772\n",
      "Epoch 109/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5804 - val_loss: 1.5777\n",
      "Epoch 110/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5804 - val_loss: 1.5825\n",
      "Epoch 111/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5805 - val_loss: 1.5755\n",
      "Epoch 112/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5803 - val_loss: 1.5760\n",
      "Epoch 113/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5801 - val_loss: 1.5763\n",
      "Epoch 114/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5797 - val_loss: 1.5749\n",
      "Epoch 115/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5801 - val_loss: 1.5755\n",
      "Epoch 116/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5799 - val_loss: 1.5769\n",
      "Epoch 117/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5802 - val_loss: 1.5757\n",
      "Epoch 118/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5799 - val_loss: 1.5757\n",
      "Epoch 119/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5798 - val_loss: 1.5754\n",
      "Epoch 120/300\n",
      "9000/9000 [==============================] - 3s 288us/step - loss: 1.5801 - val_loss: 1.5770\n",
      "Epoch 121/300\n",
      "9000/9000 [==============================] - 3s 288us/step - loss: 1.5802 - val_loss: 1.5765\n",
      "Epoch 122/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5797 - val_loss: 1.5747\n",
      "Epoch 123/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5798 - val_loss: 1.5747\n",
      "Epoch 124/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5801 - val_loss: 1.5802\n",
      "Epoch 125/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5796 - val_loss: 1.5752\n",
      "Epoch 126/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5790 - val_loss: 1.5749\n",
      "Epoch 127/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5792 - val_loss: 1.5747\n",
      "Epoch 128/300\n",
      "9000/9000 [==============================] - 3s 288us/step - loss: 1.5793 - val_loss: 1.5751\n",
      "Epoch 129/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5788 - val_loss: 1.5759\n",
      "Epoch 130/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5790 - val_loss: 1.5755\n",
      "Epoch 131/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5792 - val_loss: 1.5758\n",
      "Epoch 132/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5787 - val_loss: 1.5747\n",
      "\n",
      "Epoch 00132: ReduceLROnPlateau reducing learning rate to 0.00010000000474974513.\n",
      "Epoch 133/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5747 - val_loss: 1.5702\n",
      "Epoch 134/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5741 - val_loss: 1.5701\n",
      "Epoch 135/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5741 - val_loss: 1.5700\n",
      "Epoch 136/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5741 - val_loss: 1.5699\n",
      "Epoch 137/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5737 - val_loss: 1.5698\n",
      "Epoch 138/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5740 - val_loss: 1.5696\n",
      "Epoch 139/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5735 - val_loss: 1.5698\n",
      "Epoch 140/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5737 - val_loss: 1.5697\n",
      "Epoch 141/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5732 - val_loss: 1.5698\n",
      "Epoch 142/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5736 - val_loss: 1.5696\n",
      "Epoch 143/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5735 - val_loss: 1.5699\n",
      "Epoch 144/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5735 - val_loss: 1.5696\n",
      "Epoch 145/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5736 - val_loss: 1.5695\n",
      "Epoch 146/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5735 - val_loss: 1.5697\n",
      "Epoch 147/300\n",
      "9000/9000 [==============================] - 3s 295us/step - loss: 1.5731 - val_loss: 1.5696\n",
      "Epoch 148/300\n",
      "9000/9000 [==============================] - 3s 296us/step - loss: 1.5734 - val_loss: 1.5696\n",
      "Epoch 149/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5737 - val_loss: 1.5697\n",
      "Epoch 150/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5735 - val_loss: 1.5696\n",
      "Epoch 151/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5735 - val_loss: 1.5699\n",
      "Epoch 152/300\n",
      "9000/9000 [==============================] - 3s 297us/step - loss: 1.5732 - val_loss: 1.5696\n",
      "Epoch 153/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5732 - val_loss: 1.5697\n",
      "Epoch 154/300\n",
      "9000/9000 [==============================] - 3s 301us/step - loss: 1.5734 - val_loss: 1.5695\n",
      "Epoch 155/300\n",
      "9000/9000 [==============================] - 3s 299us/step - loss: 1.5731 - val_loss: 1.5697\n",
      "\n",
      "Epoch 00155: ReduceLROnPlateau reducing learning rate to 1.0000000474974514e-05.\n",
      "Epoch 156/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5728 - val_loss: 1.5695\n",
      "Epoch 157/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5726 - val_loss: 1.5695\n",
      "Epoch 158/300\n",
      "9000/9000 [==============================] - 3s 296us/step - loss: 1.5729 - val_loss: 1.5695\n",
      "Epoch 159/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5725 - val_loss: 1.5694\n",
      "Epoch 160/300\n",
      "9000/9000 [==============================] - 3s 297us/step - loss: 1.5729 - val_loss: 1.5694\n",
      "Epoch 161/300\n",
      "9000/9000 [==============================] - 3s 298us/step - loss: 1.5725 - val_loss: 1.5694\n",
      "Epoch 162/300\n",
      "9000/9000 [==============================] - 3s 297us/step - loss: 1.5729 - val_loss: 1.5693\n",
      "Epoch 163/300\n",
      "9000/9000 [==============================] - 3s 301us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 164/300\n",
      "9000/9000 [==============================] - 3s 300us/step - loss: 1.5725 - val_loss: 1.5694\n",
      "Epoch 165/300\n",
      "9000/9000 [==============================] - 3s 303us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 166/300\n",
      "9000/9000 [==============================] - 3s 297us/step - loss: 1.5729 - val_loss: 1.5695\n",
      "Epoch 167/300\n",
      "9000/9000 [==============================] - 3s 302us/step - loss: 1.5729 - val_loss: 1.5695\n",
      "Epoch 168/300\n",
      "9000/9000 [==============================] - 3s 303us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 169/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5723 - val_loss: 1.5693\n",
      "Epoch 170/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5728 - val_loss: 1.5694\n",
      "Epoch 171/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5725 - val_loss: 1.5693\n",
      "Epoch 172/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5729 - val_loss: 1.5694\n",
      "\n",
      "Epoch 00172: ReduceLROnPlateau reducing learning rate to 1.0000000656873453e-06.\n",
      "Epoch 173/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5727 - val_loss: 1.5695\n",
      "Epoch 174/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5729 - val_loss: 1.5694\n",
      "Epoch 175/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5723 - val_loss: 1.5694\n",
      "Epoch 176/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5728 - val_loss: 1.5694\n",
      "Epoch 177/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5726 - val_loss: 1.5693\n",
      "Epoch 178/300\n",
      "9000/9000 [==============================] - 3s 288us/step - loss: 1.5728 - val_loss: 1.5694\n",
      "Epoch 179/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 180/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 181/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5729 - val_loss: 1.5693\n",
      "Epoch 182/300\n",
      "9000/9000 [==============================] - 3s 290us/step - loss: 1.5731 - val_loss: 1.5695\n",
      "\n",
      "Epoch 00182: ReduceLROnPlateau reducing learning rate to 1.0000001111620805e-07.\n",
      "Epoch 183/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5728 - val_loss: 1.5694\n",
      "Epoch 184/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5727 - val_loss: 1.5693\n",
      "Epoch 185/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5725 - val_loss: 1.5694\n",
      "Epoch 186/300\n",
      "9000/9000 [==============================] - 3s 289us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 187/300\n",
      "9000/9000 [==============================] - 3s 293us/step - loss: 1.5732 - val_loss: 1.5694\n",
      "Epoch 188/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5730 - val_loss: 1.5694\n",
      "Epoch 189/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5723 - val_loss: 1.5693\n",
      "Epoch 190/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 191/300\n",
      "9000/9000 [==============================] - 3s 292us/step - loss: 1.5726 - val_loss: 1.5694\n",
      "Epoch 192/300\n",
      "9000/9000 [==============================] - 3s 291us/step - loss: 1.5727 - val_loss: 1.5694\n",
      "\n",
      "Epoch 00192: ReduceLROnPlateau reducing learning rate to 1.000000082740371e-08.\n",
      "Epoch 00192: early stopping\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n"
     ]
    }
   ],
   "source": [
    "train = anndata.AnnData(X_zero)\n",
    "res = dca(train, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0506168603897095"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_index = i[ix], j[ix]\n",
    "x, y = train.X[all_index], expression_train[all_index]\n",
    "np.median(np.abs(x - y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### time assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4000  cells\n",
      "Running DCA\n",
      "DCA: Successfully preprocessed 720 genes and 4000 cells.\n",
      "Calculating low dimensional representations...\n",
      "Calculating reconstructions...\n",
      "CPU times: user 4min 58s, sys: 11.1 s, total: 5min 9s\n",
      "Wall time: 1min 49s\n",
      "10000  cells\n",
      "Running DCA\n",
      "DCA: Successfully preprocessed 720 genes and 10000 cells.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/dca/api.py\u001b[0m in \u001b[0;36mdca\u001b[0;34m(adata, mode, ae_type, normalize_per_cell, scale, log1p, hidden_size, hidden_dropout, batchnorm, activation, init, network_kwds, epochs, reduce_lr, early_stop, batch_size, optimizer, random_state, threads, verbose, training_kwds, return_model, return_info, copy)\u001b[0m\n\u001b[1;32m    188\u001b[0m     }\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDCA_split\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtraining_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/dca/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(adata, network, output_dir, optimizer, learning_rate, train_on_full, aetype, epochs, reduce_lr, output_subset, early_stop, batch_size, clip_grad, save_weights, tensorboard, verbose, threads, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                      \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                      **kwargs)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;31m# https://github.com/tensorflow/tensorflow/issues/3388\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# K.clear_session()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000  cells\n",
      "Running DCA\n",
      "DCA: Successfully preprocessed 720 genes and 15000 cells.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/dca/api.py\u001b[0m in \u001b[0;36mdca\u001b[0;34m(adata, mode, ae_type, normalize_per_cell, scale, log1p, hidden_size, hidden_dropout, batchnorm, activation, init, network_kwds, epochs, reduce_lr, early_stop, batch_size, optimizer, random_state, threads, verbose, training_kwds, return_model, return_info, copy)\u001b[0m\n\u001b[1;32m    188\u001b[0m     }\n\u001b[1;32m    189\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 190\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDCA_split\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mtraining_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    191\u001b[0m     \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/dca/train.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(adata, network, output_dir, optimizer, learning_rate, train_on_full, aetype, epochs, reduce_lr, output_subset, early_stop, batch_size, clip_grad, save_weights, tensorboard, verbose, threads, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m                      \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m                      \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m                      **kwargs)\n\u001b[0m\u001b[1;32m     92\u001b[0m     \u001b[0;31m# https://github.com/tensorflow/tensorflow/issues/3388\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m     \u001b[0;31m# K.clear_session()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1449\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_with_new_api\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1450\u001b[0m           return tf_session.TF_SessionRunCallable(\n\u001b[0;32m-> 1451\u001b[0;31m               self._session._session, self._handle, args, status, None)\n\u001b[0m\u001b[1;32m   1452\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1453\u001b[0m           return tf_session.TF_DeprecatedSessionRunCallable(\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30000  cells\n",
      "Running DCA\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-4d7ea4343648>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Running DCA\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manndata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAnnData\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"data_train\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mn_cells\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_line_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'time'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'res = dca(train)'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/dca/lib/python3.5/site-packages/h5py/_hl/dataset.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0mmspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_simple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m         \u001b[0mfspace\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mselection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 496\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfspace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdxpl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dxpl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    497\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m         \u001b[0;31m# Patch up the output for NumPy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_cells_list = [4000, 10000, 15000, 30000, 50000, 100000]\n",
    "for n_cells in n_cells_list:\n",
    "    print(n_cells, \" cells\")\n",
    "    \n",
    "    print(\"Running DCA\")\n",
    "    train = anndata.AnnData(f[\"data_train\"][:n_cells])\n",
    "    %time res = dca(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
